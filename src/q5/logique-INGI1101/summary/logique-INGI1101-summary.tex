\documentclass[fr]{../../../eplsummary}

\usepackage[minted]{../../../eplcode}
\usepackage{float}
\usepackage{colonequals}
\usepackage[french,ruled,vlined]{algorithm2e}
\usepackage{csquotes}
\usepackage{footnote}
\usepackage{centernot}

\graphicspath{{img/}}

\newcommand{\prop}{\langle \textnormal{proposition} \rangle}
\newcommand{\form}{\langle \textnormal{formule} \rangle}
\newcommand{\true}{\textnormal{true}}
\newcommand{\false}{\textnormal{false}}
\newcommand{\EP}{\mathcal{E}_P}
\newcommand{\val}[1]{\mathrm{val}_{#1}}
\newcommand{\VAL}[1]{\mathrm{VAL}_{#1}}
\newcommand{\tauto}{{\vDash} \>}
\newcommand{\contra}{{\nvDash} \>}
\providecommand{\lxor}{\oplus}
\newcommand{\logcons}{\Rrightarrow}
\newcommand{\logeq}{\mathrel{\Lleftarrow {} \mspace{-12mu} {} \Rrightarrow}}
\newcommand{\nimplies}{\centernot\implies}

\SetKwComment{Comment}{$\triangleright$\ }{}

% TODO rewrite bnf grammar with syntax package

\hypertitle{Logique et structures discrètes}{5}{INGI}{1101}
{Gilles Peiffer}
{Peter Van Roy}

\part{Logique}
\section{Contexte: la méthode scientifique}
La logique est la science du raisonnement.
Il existe 3 types de raisonnement:
\begin{itemize}
	\item la déduction\footnote{Qu'on étudiera dans ce cours.};
	\item l'induction;
	\item l'abduction.
\end{itemize}
Tout raisonnement fait par un humain
fait partie de l'une de ces trois catégories.
Prenons l'exemple de la méthode scientifique.
L'humain commence en général par émettre une théorie
sur comment fonctionne le monde,
et ce en se basant sur un modèle théorique du monde réel.
Ensuite, à partir de cette théorie,
il est possible de \emph{déduire} les résultats
que devrait avoir une certaine expérience,
si la théorie est juste.
On effectue alors l'expérience,
et on \emph{induit} à partir des résultats
une règle que semble suivre la nature.
Finalement, observant cette règle naturelle,
on la compare avec la théorie proposée,
et si les deux ne sont pas en accord,
on se pose la question \foreignquote{french}{Pourquoi la nature suit-elle cette règle?}.
La réponse à cette question est une abduction.
\begin{figure}[H]
	\includegraphics[width=\textwidth]{scientific_method}
	\caption{Un exemple d'utilisation de la méthode scientifique
	pour les lois de Maxwell.
	Notons que ce schéma n'est pas entièrement complet:
	il faudrait séparer la partie \foreignquote{french}{Expérience} en deux;
	d'une part tout ce qui est prédiction,
	et d'autre part la mise en place.
	La théorie permet alors de faire les deux.}
	\label{fig:sci_meth}
\end{figure}
\bigbreak
On remarque donc que sur ces trois raisonnements,
uniquement la déduction est un raisonnement sûr,
alors que l'induction et l'abduction ne le sont pas.
\begin{myexem}[Sac de billes]
Un exemple illustrant cette différence
serait un sac rempli de billes colorées.
On suppose que la théorie nous dit
que toutes les billes sont blanches.
On \emph{déduit} donc avec parfaite sûreté
que si on pioche une bille du sac,
elle sera blanche.

Comparons cela avec le cas suivant:
on est en train de piocher des billes du sac,
et on remarque que toutes les billes que l'on pioche sont blanches.
On peut alors \emph{induire} que toutes les billes du sac sont blanches.
Ce raisonnement n'est pas parfaitement sûr,
contrairement à la déduction.

Finalement, supposons que l'on trouve un bille blanche à coté du sac.
On pourrait expliquer cela par le fait que les billes viennent du sac.
Il s'agit d'un raisonnement abductif,
qui n'est d'ailleurs pas sûr.
\end{myexem}

\section{La logique propositionnelle}
La logique propositionnelle
est la plus simple des formes de logique
(on a également la logique des prédicats\footnote{Plus tard dans le cours\dots},
la logique du second ordre, la logique temporelle, la logique modale,\dots).
Elle permet de formaliser les connexions logiques entre des propositions.
\bigbreak
\begin{mydef}[Proposition première]
	Les \emph{propositions premières} sont les \foreignquote{french}{briques}
	avec lesquelles nous construisons les propositions plus compliquées.
	Elles peuvent soit être vraies (true) ou fausses (false).
	Un exemple pourrait être la phrase
	\foreignquote{french}{La logique est compliquée.}\footnote{En l'occurrence, cette proposition est fausse\dots}.
	On les dénote avec des lettres majuscules ($A$, $B$,\dots).
\end{mydef}
\begin{mydef}[Proposition logique]
	Une \emph{proposition logique} est alors soit une proposition première,
	soit une combinaison de propositions logiques
	connectées par des connecteurs logiques.
\end{mydef}
L'avantage de cette notation est qu'elle permet de raisonner formellement
sur les propositions logiques.
On peut définir:
\begin{enumerate}
	\item une \emph{syntaxe} (définie par une grammaire)
	qui définit ce qui est une proposition logique et ce qui ne l'est pas;
	\item une \emph{sémantique}
	qui donne un sens à chaque proposition logique;
	\item une \emph{théorie de preuve} permettant,
	en sachant qu'une proposition est vraie,
	de trouver d'autres propositions vraies.
\end{enumerate}
\subsection{Syntaxe}
\label{sec:syntax}
La logique propositionnelle est un \emph{langage formel}.
Ce langage peut être défini à l'aide d'une grammaire sur un \emph{alphabet}.
Cet alphabet est l'ensemble des symboles qui composent une proposition logique:
\begin{itemize}
	\item les lettres majuscules pour les propositions premières: $A$, $B$,\dots;
	\item $\true$ et $\false$ pour les propositions
	resp. toujours vraie et toujours fausse;
	\item les caractères de ponctuation: $($ et $)$;
	\item les connecteurs logiques:
	\begin{itemize}
		\item la conjonction (\textsc{and}): $\land$;
		\item la disjonction (\textsc{or}): $\lor$;
		\item la négation (\textsc{not}): $\lnot$;
		\item l'implication: $\to$ ou $\implies$;
		\item l'équivalence: $\leftrightarrow$ ou $\iff$.
	\end{itemize}
	Leur précédence est la suivante:
	\begin{table}[H]
		\[
		\begin{array}{cc}
			\hline
			\textnormal{Connecteur} & \textnormal{Précédence}\\
			\lnot & 1\\
			\land & 2\\
			\lor & 3\\
			\to & 4\\
			\leftrightarrow & 5\\
			\hline
		\end{array}
		\]
	\end{table}
\end{itemize}
La grammaire suivante permet de donner les règles
que doivent respecter les phrases propositionnelles:
\[
\renewcommand{\arraystretch}{1.5}
\begin{array}{rrl}
	\langle \textnormal{identificateur} \rangle & \coloneqq & A \mid B \mid C \mid \dots\\
	\prop &\coloneqq& \true\\
	&\mid& \false\\
	&\mid& \langle \textnormal{identificateur} \rangle\\
	&\mid& \big(\prop\big)\\
	&\mid& \lnot \prop\\
	&\mid& \prop \land \prop\\
	&\mid& \prop \lor \prop\\
	&\mid& \prop \to \prop\\
	&\mid& \prop \leftrightarrow \prop
\end{array}
\]
On remarque qu'uniquement les séquences de symboles
qui respectent cette grammaire
sont des phrases propositionnelles.
\paragraph{Métalangage}
Le métalangage est un deuxième langage utilisé pour parler d'un premier,
dans notre cas, la logique propositionnelle.
Notre métalangage est le français,
auquel on ajoute les notations mathématiques.
Ce concept est important pour distinguer raisonnement formel et informel.
\subsection{Tables de vérité}
La syntaxe développée à la \sectionref{syntax}
permet d'écrire les propositions en logique formelle,
mais ne leur donne pas de sens (vrai ou faux).
Afin de donner un sens aux propositions,
il faut commencer par déterminer
si les propositions premières sont vraies ou fausses.
Pour cela, il y a deux approches:
les \emph{tables de vérité}
et les \emph{interprétations}.
Les propositions logiques sont construites
à partir de propositions plus simples.
Une table de vérité est une façon de partir
des valeurs de différentes propositions
pour déterminer la valeur d'une autre proposition.
Voici les tables de vérité des différents connecteurs logiques
(le connecteur XOR ($p \lxor q$)
est une façon plus simple
d'écrire $\lnot(p \land q) \land (p \lor q)$):
\begin{table}[H]
	\[
	\begin{array}{cc|c|c|c|c|c|c}
		p & q & p \land q & p \lor q & \lnot p & p \to q & p \leftrightarrow q & p \lxor q\\
		\hline
		\true & \true & \true & \true & \multirow{2}{*}{\false} & \true & \true & \false\\
		\true & \false & \false & \true & & \false & \false & \true \\
		\false & \true & \false & \true & \multirow{2}{*}{\true} & \true & \false & \true\\
		\false & \false & \false & \false & & \true & \true & \false
	\end{array}
	\]
\end{table}
\subsection{Interprétations}
On peut également définir si une proposition est vraie ou fausse
en utilisant une \emph{interprétation}.
Soit $\EP$ l'ensemble des propositions premières.
Alors, une interprétation $I$ définit
une fonction de valuation $\val{I} \colon \EP \to \{\true,\false\}$,
qui permet de savoir si ces propositions premières sont vraies ou fausses\footnote{La notation $f \colon \mathcal{A} \to \mathcal{B}$
signifie ici que $f$ est une fonction
depuis l'ensemble $\mathcal{A}$ vers l'ensemble $\mathcal{B}$.}.
On pourrait par exemple écrire
\begin{align*}
	\left\{
	\begin{array}{rcl}
		\val{I_1}(P) & = & \true\,,\\
		\val{I_1}(Q) & = & \false
	\end{array}
	\right.
	\quad \textnormal{et} \quad
	\left\{
	\begin{array}{rcl}
		\val{I_2}(P) & = & \true\,,\\
		\val{I_2}(Q) & = & \true\,.
	\end{array}
	\right.
\end{align*}
Il y a donc différentes interprétations possibles,
et chacune d'elles risque de donner un résultat différent.
\bigbreak
À partir de ces fonctions de valuation premières,
on peut définir d'autres fonctions de valuation:
$\VAL{I}$ se définit par rapport à $\val{I}$.
On a ainsi
\begin{align*}
	\VAL{I}(P) &= \val{I}(P)\,,\quad \forall P \in \EP\,,\\
	\VAL{I}(\lnot P) &=
	\left\{
	\begin{array}{l}
		\false \textnormal{ si } \VAL{I}(P) = \true\,,\\
		\true \textnormal{ sinon}\,,
	\end{array}
	\right.\\
	\VAL{I}(P \land Q) &=
	\left\{
	\begin{array}{l}
		\true \textnormal{ si } \VAL{I}(P) = \true \textnormal{ et } \VAL{I}(Q) = \true\,,\\
		\false \textnormal{ sinon}\,,
	\end{array}
	\right.\\
	\VAL{I}(P \lor Q) &=
	\left\{
	\begin{array}{l}
		\true \textnormal{ si } \VAL{I}(P) = \true \textnormal{ ou } \VAL{I}(Q) = \true\,,\\
		\false \textnormal{ sinon}\,.
	\end{array}
	\right.
\end{align*}
\subsection{Les modèles logiques}
Une interprétation est un modèle $M$ pour une certaine théorie si
les propositions premières sont telles
que tous les axiomes de la théorie soient vrais.
\begin{myexem}
	Prenons le cas suivant:
	\begin{align*}
		&L\textnormal{: \foreignquote{french}{Le cours de logique est facile.},}\\
		&E\textnormal{: \foreignquote{french}{L'étudiant a bien étudié.},}\\
		&R\textnormal{: \foreignquote{french}{L'étudiant a réussi l'examen de logique.},}
	\end{align*}
	\[
	\big(L \land E\big) \to R\,.\tag*{Axiome~(1)}
	\]
	Un modèle possible des études supérieures serait donc
	\[
	(L,E,R) = (\true,\true,\true)\,.
	\]
\end{myexem}
On peut donc dire que la théorie est un ensemble de formules
et que le modèle est une interprétation qui satisfait ces formules.
\bigbreak
Soit une formule $p$ aléatoire.
\begin{mydef}[Tautologie]
	Si $p$ est vraie dans toutes les interprétations possibles,
	alors on dit que $p$ est une tautologie ($\tauto p$).
	On a par exemple $p \equiv \big(P \lor \lnot P\big)$.
\end{mydef}
\begin{mydef}[Contradiction]
	Si $p$ est fausse dans toutes les interprétations possibles,
	alors on dit que $p$ est une contradiction ($\contra p$).
	On a par exemple $p \equiv \big(P \land \lnot P\big)$.
\end{mydef}
\begin{mydef}[Contingence]
	Si $p$ n'est ni une tautologie,
	ni une contradiction,
	on dit que $p$ est contingente.
	Par exemple, $p \equiv \big(P \land \lnot Q\big)$.
\end{mydef}
\subsubsection{Conséquence logique}
On note \foreignquote{french}{$q$ est conséquence logique de $p$}
par $p \logcons q$\footnote{Le Prof. Van Roy
utilise les notations $p \logcons q$ et $p \logeq q$
pour la conséquence et l'équivalence logique.
Cependant, dans la littérature,
on utilise $p \implies q$ et $p \iff q$ pour celles-ci.
Comme le Prof. Van Roy utilise déjà celles-ci
pour la conséquence et l'équivalence matérielles
(qui dans la littérature sont souvent représentées
par $p \to q$ et $p \leftrightarrow q$),
on reprendra donc sa notation,
bien qu'elle soit non standard.}.
Soit $p \vDash q$.
Cela veut dire que si $M$ est modèle de $p$, alors $M$ est aussi modèle de $q$.
On a donc $\tauto (p \to q)$,
car $p \to q$ est toujours vrai.
On peut donc bien écrire $p \logcons q$.
\begin{myexem}
	Soit $p \equiv \big(P \land Q\big)$ et $q \equiv \big(P\big)$.
	On a alors que $p \vDash q$.
\end{myexem}
La conséquence logique $p \logcons q$ fait partie du métalangage,
et n'est donc pas une proposition logique.
\subsubsection{Équivalence logique}
Par un raisonnement similaire,
si on a $p \vDash q$ et $q \vDash p$,
on a également $\tauto (p \to q)$ et $\tauto (q \to p)$.
On peut donc écrire $p \logeq q$.
L'équivalence logique n'est pas non plus une proposition logique
mais fait partie du métalangage.

\section{Preuves en logique propositionnelle}
\begin{mydef}[Preuve]
	Une \emph{preuve} est un raisonnement déductif
	qui démontre si une proposition est vraie ou fausse.
	On distingue les preuves \emph{formelles} et \emph{informelles}.
	\begin{itemize}
		\item Une \emph{preuve formelle}
		est un raisonnement en langage naturel,
		parfois augmenté avec des notations mathématiques.
		\item Une \emph{preuve informelle}
		est un raisonnement mathématique
		qui formalise le raisonnement déductif.
	\end{itemize}
\end{mydef}
\begin{myrem}
	\foreignquote{french}{On peut prouver $q$ à partir de $p$} se note $p \vdash q$.
\end{myrem}
\subsection{Preuve avec table de vérité}
La preuve formelle la plus simple est une table de vérité.
On fait un tableau avec $2^n$ lignes,
où $n$ est le nombre de propositions premières,
et pour chaque ligne,
on prouve une équivalence.
Cette méthode de preuve a l'inconvénient d'être très longue,
et en pratique elle n'est pas réellement utile
au-delà d'une centaine de propositions premières.
On appelle ces programmes (qui ont des heuristiques supplémentaires)
des \textsc{sat} solvers.
Elle n'est également pas valable pour la logique des prédicats.
\subsection{Preuve transformationnelle}
Une preuve transformationnelle est une séquence de transformations
\[
p_1 \logeq p_2 \logeq \cdots \logeq p_n
\]
dans laquelle on a toujours $p_i \logeq p_{i+1}$.
Une preuve transformationnelle est aussi un objet mathématique en métalangage.
On utilise pour ceci des \foreignquote{french}{lois} de transformation,
c'est-à-dire des équivalences connues.
\begin{table}[H]
\renewcommand\arraystretch{1.5}
\centering
\begin{tabular}{rcl@{\qquad}l}
	\hline
	$p$ & $\logeq$ & $p \lor p$ & Idempotence de $\lor$ \\
	$p$ & $\logeq$ & $p \land p$ & Idempotence de $\land$ \\
	$p \lor q$ & $\logeq$ & $q \lor p$ & Commutativité de $\lor$ \\
	$p \land q$ & $\logeq$ & $q \land p$ & Commutativité de $\land$ \\
	$(p \lor q) \lor v$ & $\logeq$ & $p \lor (q \lor v)$ & Associativité de $\lor$ \\
	$(p \land q) \land v$ & $\logeq$ & $p \land (q \land v)$ & Associativité de $\land$ \\
	$\lnot \lnot p$ & $\logeq$ & $p$ & Double négation \\
	$p \to q$ & $\logeq$ & $\lnot p \lor q$ & Implication \\
	$\lnot(p \land q)$ & $\logeq$ & $\lnot p \lor \lnot q$ & 1\ieme{} loi de De Morgan \\
	$\lnot(p \lor q)$ & $\logeq$ & $\lnot p \land \lnot q$ & 2\ieme{} loi de De Morgan \\
	$p \leftrightarrow q$ & $\logeq$ & $(p \to q) \land (q \to p)$ & Équivalence \\
	$(p \land q) \lor r$ & $\logeq$ & $(p \lor r) \land (q \lor r)$ & Distributivité de $\lor$ \\
	$(p \lor q) \land r$ & $\logeq$ & $(p \land r) \lor (q \land r)$ & Distributivité de $\land$ \\
	\hline
\end{tabular}
\caption{Lois de transformation.}
\label{tab:trans}
\end{table}
Pour réellement utiliser ces lois,
on rajoute deux lois supplémentaires:
\begin{itemize}
	\item La \emph{transitivité de l'équivalence}.

	Si $p \logeq q$ et $q \logeq r$,
	alors $p \logeq r$.
	\item Le \emph{principe de substitution}.

	On peut remplacer une sous-formule
	par une autre sous-formule équivalente
	à l'intérieur d'une formule.
\end{itemize}

\subsection{Preuve déductive}
Une preuve déductive est un objet mathématique
qui formalise une séquence de pas de raisonnement simples.
Chaque pas doit être justifié
avec le nom de la règle ou la loi qui est utilisée.
Les pas utilisent trois techniques de raisonnement différentes:
les équivalences logiques,
les règles d’inférence et
les schémas de preuve.
\subsubsection{Équivalences logiques}
Les équivalences logiques sont les mêmes
que celles données dans la \tabref{trans}.
\subsubsection{Règles d'inférence}
Contrairement aux preuves transformationnelles, les règles d'inférence
ont une direction:
elles commencent par les prémisses et terminent par une conclusion.
On utilise un raisonnement informel pour justifier les règles.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{cl@{\qquad\qquad}cl}
	\hline
	$\begin{array}{c}
	p \\
	q \\
	\hline
	p \land q
	\end{array}$ & Conjonction & $\begin{array}{c}
	p \\
	\lnot p \\
	\hline
	q
	\end{array}$ & Contradiction \\
	&&&\\
	$\begin{array}{c}
	p \land q \\
	\hline
	p
	\end{array}$ & Simplification & $\begin{array}{c}
	p \\
	\hline
	p \lor q
	\end{array}$ & Addition \\
	&&&\\
	$\begin{array}{c}
	p \leftrightarrow q\\
	q \leftrightarrow r\\
	\hline
	p \leftrightarrow r
	\end{array}$ & Transitivité de l'équivalence & $\begin{array}{c}
	p \lor q\\
	\lnot p \\
	\hline
	q
	\end{array}$ & Syllogisme disjoint \\
	&&&\\
	$\begin{array}{c}
	p \to q \\
	p \\
	\hline
	q
	\end{array}$ & Modus ponens & $\begin{array}{c}
	p \to q \\
	\lnot q \\
	\hline
	\lnot p
	\end{array}$ & Modus tollens \\
	&&&\\
	$\begin{array}{c}
	p \leftrightarrow q \\
	\hline
	q \leftrightarrow p
	\end{array}$ & Loi d'équivalence & $\begin{array}{c}
	\lnot \lnot p \\
	\hline
	p
	\end{array}$ & Double négation \\
	\hline
\end{tabular}
\caption{Ensemble des règles d'inférence utilisées.}
\label{tab:inference}
\end{table}

\subsubsection{Schémas de preuve}
En plus des équivalences logiques et des règles d'inférence,
on a deux schémas de preuve
qui formalisent des techniques de raisonnement plus abstraites:
le théorème de déduction et la démonstration par l'absurde.
En général, on met ces preuves dans des schémas comme celui en \figuref{proof}.
\begin{figure}[H]
\centering
\begin{tabular}{ccc}
	& Formule & Justification \\
	\hline
	1. & $s$ & $\cdots$ \\
	2. & $\cdots$ & $\cdots$ \\
	\multirow{3}{*}{$\vdots$} & \multirow{3}{*}{$\vdots$} & \multirow{3}{*}{$\vdots$} \\
	&&\\
	&&\\
	$n$. & $t$ & $\cdots$\\
	\hline
\end{tabular}
\caption{Le squelette d'une preuve en logique propositionnelle.}
\label{fig:proof}
\end{figure}
\paragraph{Théorème de déduction (preuve conditionnelle)}
Supposons $s$ vraie.
C'est notre hypothèse.
Elle s'ajoute donc aux prémisses utilisées dans la preuve.
Ensuite, on fait une preuve de $t$.
On peut construire une preuve (objet mathématique) de $t$ en commençant de $s$.
On note ceci $s \vdash t$.
\[
\begin{array}{lcr}
	p,\dots,r,s & \vdash & t \\
	\hline
	p,\dots,r & \vdash & s \to t
\end{array}
\]
On a évacué l'hypothèse.
\paragraph{Démonstration par l'absurde (preuve par contradiction)}
Également appelée \emph{reductio ad absurdum}.
On suppose nos prémisses $p,\dots,q$ sans contradiction.
On rajoute $r$ aux prémisses.
Si on peut prouver $s$ et $\lnot s$,
cela signifie que $r$ rend les prémisses inconsistantes.
\[
\begin{array}{lcr}
	p,\dots,q,r & \vdash & s \\
	p,\dots,q,r & \vdash & \lnot s\\
	\hline
	p,\dots,q & \vdash & \lnot r
\end{array}
\]
\subsection{Algorithme de preuve}
Les preuves manuelles de la section précédente
ont un grand défaut:
elles sont extrêmement lentes.
Afin de diminuer le temps nécessaire pour prouver une équivalence logique,
on peut définir un algorithme de preuve,
qui est une automatisation d'une démonstration par l'absurde
qui est basée sur une seule règle,
la \emph{résolution}.

\subsubsection{Transformation en forme normale conjonctive}

Toute formule logique peut être transformée en une formule équivalente,
qui est une conjonction de disjonctions.
C'est la forme normale conjonctive (\textsc{fnc}).
On peut également définir une forme similaire,
la \textsc{fnd} (forme normale disjonctive),
qui est donc une disjonction de conjonctions.

\paragraph{Terminologie}
Afin de faciliter la discussion autour des formes normales,
on introduit la terminologie suivante:
\begin{itemize}
	\item Un \emph{littéral} $L$ est soit une proposition première,
	soit la négation d'une proposition première.
	\item Une \emph{clause} $C$, en \textsc{fnc},
	est une disjonction de littéraux.
	On écrit
	\begin{align*}
	C &\equiv \bigvee\limits_{i = 1}^{n} L_i\\
	&\equiv L_1 \lor L_2 \lor \dots \lor L_n\,.
	\end{align*}
	En \textsc{fnc}, on a souvent des conjonctions de clauses,
	c'est-à-dire des formules de la forme
	\begin{align*}
	\bigwedge\limits_{i=1}^{m} C_i &\equiv \bigwedge\limits_{i = 1}^{m} \bigvee\limits_{j = 1}^{n_i} L_{ij}\\
	&\equiv \big(L_{11} \lor \dots \lor L_{1n_1}\big) \land \dots \land \big(L_{m1} \lor \dots \lor L_{mn_m}\big) \,.
	\end{align*}
\end{itemize}

\paragraph{Algorithme de normalisation}
Pour mettre sous \textsc{fnc} une formule logique quelconque,
on utilise l'algorithme suivant:
\begin{enumerate}
	\item éliminer les implications et équivalences matérielles
	($\to$ et $\leftrightarrow$)
	en les remplaçant par des formules équivalentes;
	\item déplacer les négations vers l'intérieur
	(jusqu'aux propositions premières)
	en utilisant les formules de De Morgan;
	\item déplacer les disjonctions ($\lor$) vers l'intérieur
	en utilisant les lois distributives;
	\item simplifier en éliminant les formes $(P \lor \lnot P)$
	dans chaque disjonction.
\end{enumerate}

\subsubsection{Résolution}
La règle de résolution est l'unique règle d'inférence utilisée
dans les algorithmes de preuve.
Elle s'écrit comme suit:
\[
\begin{array}{l@{}c@{}r}
	p_1 & {} \lor {} & q\\
	p_2 & {} \lor {} & \lnot q\\
	\hline
	p_1 & {} \lor {} & p_2
\end{array}
\]
\begin{myrem}[La résolution préserve les modèles]\leavevmode
\begin{proof}
	Soient
	\begin{align*}
		p &\equiv \bigwedge\limits_{i=1}^{2} C_i\,,\\
		C_1 &\equiv p_1 \lor q\,,\\
		C_2 &\equiv p_2 \lor \lnot q\,,\\
		r &\equiv p_1 \lor p_2\\
		&\equiv \left(C_1 - \{P\}\right) \lor \left(C_2 - \{\lnot P\}\right)\,.
	\end{align*}
	$r$ est une nouvelle disjonction à partir de deux autres disjonctions.
	Prouvons que $r$ est toujours vraie ($\{C_1, C_2\} \vDash r$).
	On utilise pour cela la sémantique,
	une preuve en métalangage qui n'est donc pas formalisée.

	Si
	\[
	\left\{
	\begin{array}{c}
		\VAL{I}(C_1) = \true\,,\\
		\VAL{I}(C_2) = \true\,,
	\end{array}
	\right.
	\]
	alors $I$ est modèle de $\{C_1, C_2\}$.

	On voit que c'est le cas,
	car on a toujours soit $\VAL{I}(q) = \true$
	et donc $\VAL{I}(p_2) = \true$,
	ou bien $\VAL{I}(q) = \false$
	et donc $\VAL{I}(p_1) = \true$.
	$r$ est donc toujours vraie.
\end{proof}
\end{myrem}

\subsubsection{Pseudocode}

Prenons des axiomes $C_i$ en \textsc{fnc}:
\begin{align*}
C_i &= \bigvee\limits_{i=1}^{n} L_i\,,\\
L_i &= P \textnormal{ ou } \lnot P\,,
\end{align*}
et un théorème à prouver, $C$.
On veut donc démontrer
\[
\{C_1, \dots,C_n\} \vdash C\,.
\]
On sait que
\[
\{C_1, \dots, C_n\} \vDash C \iff \{C_1, \dots, C_n, \lnot C\} \vDash \false\,.
\]
Il suffit donc de démontrer que $\mathcal{S} = \{C_1, \dots, C_n, \lnot C\}$
est inconsistant.
Pour faire cela,
l'algorithme de preuve combine les éléments de $\mathcal{S}$
grâce à la résolution,
jusqu'à ce qu'on arrive à une contradiction,
ou qu'il n'y ait plus de possibilité d'utiliser la résolution.
Dans le premier cas,
nous avons prouvé $C$.
Dans le second,
$C$ est improuvable.

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{$C_1,\dots,C_n$ les clauses, et $C$ le théorème à prouver.}
\KwResult{Si oui ou non $C_1, \dots, C_n \vdash C$.}
\Begin{
	$\mathcal{S} \gets \{C_1, \dots, C_n, \lnot C\}$\;
	\While{$\false \notin \mathcal{S}$ et
	$\exists C_i, C_j \in \mathcal{S}$ telles que $C_i, C_j$
	résolvables et non résolues}{
		choisir $C_1, C_2 \in \mathcal{S}$
		telles que $\exists P \in C_1, \lnot P \in C_2$\;
		calculer $r \gets C_1 \setminus \{P\} \lor C_2 \setminus \{ \lnot P\}$\;
		calculer $\mathcal{S} \gets \mathcal{S} \cup \{r\}$\;
	}
	\eIf{$\false \in \mathcal{S}$}{
		\Return \true \Comment*[r]{$C$ prouvé}
	}{
		\Return \false \Comment*[r]{$C$ improuvable}
	}
}
\caption{Algorithme de preuve\label{algo:proof}}
\end{algorithm}

En plus de prouver l'existence éventuelle d'une preuve,
l'algorithme de preuve nous donne également cette preuve.

\begin{myprop}[Propriétés de l'algorithme de preuve en logique propositionnelle]
	Pour toute théorie $\mathcal{B} = \{C_1, \dots, C_n\}$ et théorème $C$,
	l'algorithme de preuve est
	\begin{itemize}
		\item \textbf{Décidable (\emph{decidable}).}
		L'algorithme se termine
		après un nombre fini d'étapes $\forall \mathcal{B}, C$.
		\item \textbf{Adéquat (\emph{sound}).}
		Si $\mathcal{B} \vdash C$,
		alors $\mathcal{B} \vDash C$.
		\item \textbf{Complet (\emph{complete}).}
		Si $\mathcal{B} \vDash C$,
		alors $\mathcal{B} \vdash C$.
	\end{itemize}

	Le temps d'exécution
	est exponentiel en le nombre de propositions premières
	et de clauses\footnote{Bien que ce temps d'exécution
	puisse être fortement amélioré
	par l'utilisation de diverses heuristiques,
	comme celles utilisées dans les \textsc{sat} solvers.}.
\end{myprop}

\section{La logique des prédicats}
Étudions maintenant une logique plus expressive,
la logique des prédicats ou logique de premier ordre.
Les principales différences avec la logique propositionnelle sont:
\begin{itemize}
	\item Les propositions premières sont remplacées
	par des prédicats.
	\item On introduit les quantificateurs existentiel et universel.
	\item Il est possible d'exprimer
	des relations entre les éléments d'un ensemble de façon concise,
	même lorsque cet ensemble est de taille infinie.
\end{itemize}

\subsection{Quantificateurs}
Afin de noter clairement
le fait de dire \foreignquote{french}{pour tout $x$} et
\foreignquote{french}{il existe un $x$ tel que},
on introduit la notion de quantificateur,
respectivement $\forall x$ et $\exists x$ dans ce cas-ci.
La \emph{portée} d'un quantificateur est la partie d'une phrase logique
dans laquelle il est d'application\footnote{En pratique, les quantificateurs sont suivis d'une variable, puis souvent d'un point.
Cette notation est due à Russell, Whitehead et Church et signifie que la portée de ce quantificateur va du point jusqu'à la fin de la phrase logique.}.
Afin de résoudre des conflits potentiels,
on fait appel à une nouvelle opération: le renommage des noms de variable.

\subsection{Syntaxe}
\subsubsection{Symboles}
Une formule en logique des prédicats peut utiliser différents symboles.
\begin{mydef}[Arité d'un prédicat]
	On appelle \foreignquote{french}{\emph{arité}} d'un prédicat
	(ou d'une fonction) le nombre d'arguments de celui-ci.
\end{mydef}
Les symboles utilisés sont montrés dans la \tabref{symbols}.
\begin{table}[H]
	\centering
	\begin{tabular}{ccc}
		\hline
		\multirow{5}{*}{Symboles logiques} & Quantificateurs & $\exists$ et $\forall$ \\
		 & Connecteurs logiques & $\land$, $\lor$, $\lnot$, $\to$ et $\leftrightarrow$ \\
		 & Caractères de ponctuation & $($ et $)$ \\
		 & \multirow{2}{*}{Variables} & $x$, $y$, $z$,\ldots \\
		 & & $\true$ et $\false$ \\
		\multirow{3}{*}{Symboles non logiques} & Symboles de prédicat & $P$, $Q$, $R$,\ldots (avec arité $\ge 0$) \\
		 & Symboles de fonction & $f$, $g$, $h$,\ldots (avec arité $\ge 0$) \\
		 & Symboles de constante & $a$, $b$, $c$,\ldots (si arité $= 0$) \\
		\hline
	\end{tabular}
	\caption{Symboles utilisés en logique des prédicats.}
	\label{tab:symbols}
\end{table}

\subsubsection{Règles de grammaire}
Les règles de grammaire en logique des prédicats sont les suivantes:
\[
\renewcommand{\arraystretch}{1.5}
\begin{array}{rrl}
	\form &\coloneqq & \langle \textnormal{formule atomique} \rangle\\
	&\mid& \lnot \form\\
	&\mid& \form \langle \textnormal{connecteur} \rangle \form\\
	&\mid& \forall \langle \textnormal{var} \rangle.\ \form\\
	&\mid& \exists \langle \textnormal{var} \rangle.\ \form\\
	\langle \textnormal{formule atomique} \rangle &\coloneqq& \true \mid \false\\
	&\mid& \langle \textnormal{prédicat} \rangle \big(\langle \textnormal{terme} \ast \rangle\big)\\
	\langle \textnormal{terme} \rangle &\coloneqq& \langle \textnormal{constante} \rangle\\
	&\mid& \langle \textnormal{var} \rangle\\
	&\mid& \langle \textnormal{fonction} \rangle \big(\langle \textnormal{terme} \ast \rangle\big)\\
	\langle \textnormal{connecteur binaire} \rangle &\coloneqq& \land \mid \lor \mid \to \mid \leftrightarrow\\
\end{array}
\]

\subsection{Sémantique}
En logique des prédicats nous gardons
les notions de modèle et d'interprétation
déjà définies pour la logique propositionnelle.
Comme pour la logique propositionnelle,
une interprétation a une valeur dans l'ensemble $\{\true, \false\}$.
\subsubsection{Interprétation}
\begin{mydef}[Interprétation en logique des prédicats]
	Une \emph{interprétation} $I$ en logique des prédicats
	est une paire $I = (\mathcal{D}_I, \val{I})$,
	où $\mathcal{D}_I$ est un ensemble appelé
	\foreignquote{french}{\emph{domaine de discours}}
	et $\val{I}$ est une \emph{fonction de valuation}
	qui renvoie un élément de $\mathcal{D}_I$ pour chaque symbole.
	Pour chaque symbole $s$, on a alors
	\begin{itemize}
		\item si $s$ est un symbole de prédicat,
		$\val{I}(s) = P_I \colon \mathcal{D}_I^n \to \{\true, \false\}$, avec $n$ le nombre d'arguments de $P_I$;
		\item si $s$ est un symbole de fonction,
		$\val{I}(s) = f_I \colon \mathcal{D}_I^n \to \mathcal{D}_I$, avec $n$ le nombre d'arguments de $f_I$;
		\item si $s$ est une constante (fonction sans arguments),
		$\val{I}(s) = c \in \mathcal{D}_I$;
		\item si $s$ est une variable,
		$\val{I}(s) = x_I \in \mathcal{D}_I$.
	\end{itemize}
	\begin{myrem}
		On remarque deux choses:
		\begin{itemize}
			\item Chaque fonction et chaque prédicat
			correspondent à une vraie fonction ou à un vrai prédicat,
			alors que chaque constante et chaque variable
			correspondent à une constante
			dans le domaine de discours, $\mathcal{D}_I$.
			\item L'interprétation est un élément de $\mathcal{D}_I$
			autant pour les constantes que pour les variables,
			mais les variables peuvent s'utiliser dans les quantificateurs
			alors que les constantes non.
		\end{itemize}
	\end{myrem}
\end{mydef}

Ayant défini la fonction $\val{I}$ sur tous les symboles,
on peut maintenant définir une fonction $\VAL{I}$
sur les termes et les formules:
\[
\VAL{I} \colon \mathcal{T} \cup \mathcal{P} \to \mathcal{D}_I \cup \{\true, \false\}\,,
\]
où $\mathcal{T}$ est l'ensemble des termes et
$\mathcal{P}$ est l'ensemble des formules.
Nous avons donc:
\begin{itemize}
	\item si $t$ est un terme, $\VAL{I} \colon t \mapsto \VAL{I}(t)$;
	\item si $p$ est un terme, $\VAL{I} \colon p \mapsto \VAL{I}(p)$.
\end{itemize}
On peut définir $\VAL{I}$ à partir de $\val{I}$ comme suit:
\begin{itemize}
	\item $\VAL{I}\big(P(t_1, \ldots, t_m)\big) = \big(\val{I}(P)\big)\big(\VAL{I}(t_1), \ldots, \VAL{I}(t_m)\big)$.
	\item $\VAL{I}(\forall x.\ p) = \true$
	si pour chaque $d \in \mathcal{D}_I$,
	$\VAL{\{x \leftarrow d\} \circ I}(p) = \true$,
	$\false$ sinon.
	En clair, $d$ est l'interprétation de $x$ dans la formule $p$.
	Si $p$ est vraie pour tout $d$ dans le domaine de discours,
	alors le quantificateur universel est vrai aussi.
	\item $\VAL{I}(\exists x.\ p) = \true$
	s'il existe un élément $d \in \mathcal{D}_I$
	tel que $\VAL{\{x \leftarrow d\} \circ I}(p) = \true$,
	$\false$ sinon.
	\item $\VAL{I}(p \land q) = \true$
	si $\VAL{I}(p) = \VAL{I}(q) = \true$,
	$\false$ sinon.
	\item $\VAL{I}(p \lor q) = \true$
	si $\VAL{I}(p) = \true$ ou $\VAL{I}(q) = \true$,
	$\false$ sinon.
	\item $\VAL{I}(\lnot p) = \true$
	si $\VAL{I}(p) = \false$,
	$\false$ sinon.
	\item $\VAL{I}(p \to q) = \true$
	si $\VAL{I}(\lnot p \lor q) = \true$,
	$\false$ sinon.
	\item $\VAL{I}(p \leftrightarrow q) = \true$
	si $\VAL{I}\big((p \to q) \land (q \to p)\big) = \true$,
	$\false$ sinon.
	\item $\VAL{I}(c) = \val{I}(c)$
	si $c$ est un symbole de constante.
	\item $\VAL{I}(x) = \val{I}(x)$
	si $x$ est un symbole de variable.
	\item $\VAL{I}\big(f(t_1, \ldots, t_n)\big) = \big(\val{I}(f)\big)\big(\VAL{I}(t_1), \ldots, \VAL{I}(t_n)\big)$
	si $f$ est un symbole de fonction.
	\item $\VAL{I}(\true) = \true$.
	\item $\VAL{I}(\false) = \false$.
\end{itemize}
Il est donc possible de décomposer la formule $p$ en ses symboles de base
de sorte à pouvoir calculer $\VAL{I}(p)$ à partir de $\val{I}$.
\begin{mydef}[Modèle en logique des prédicats]
	Un \emph{modèle} d'un ensemble de formules
	est une interprétation qui rend toutes les formules vraies.
	Formellement,
	si on a un ensemble de formules $\mathcal{B} = \{p_1, \ldots, p_n\}$,
	une interprétation $I$ pour $\mathcal{B}$ est modèle si et seulement si
	\[
	\VAL{I}(p_i) = \true\,, \quad \forall p_i \in \mathcal{B}\,.
	\]
\end{mydef}

\section{Preuves en logique des prédicats}
Afin de traiter les quantificateurs en logique des prédicats,
nous utilisons l'approche suivante:
\begin{enumerate}
	\item enlever les quantificateurs pour avoir des variables libres;
	\item raisonner sur l'intérieur;
	\item remettre les quantificateurs.
\end{enumerate}
Afin de faire cela, il faut rajouter quatre règles
par rapport aux preuves formelles en logique propositionnelle\footnote{Les autres règles de la logique propositionnelle
restent valables en logique des prédicats.}:
\begin{itemize}
	\item élimination du quantificateur universel;
	\item introduction du quantificateur universel;
	\item élimination du quantificateur existentiel;
	\item introduction du quantificateur existentiel;
\end{itemize}

\subsection{Règles en logique des prédicats}
Avant de donner les règles de déduction pour les quantificateurs,
définissons une manipulation symbolique importante
qui est utilisée dans ces règles: la \emph{substitution}.

\begin{myrem}
	Il est impossible de prouver en logique des prédicats
	que les règles sont exactes.
	Chaque règle est justifiée par un raisonnement en métalangage,
	basée sur le modèle de la formule.
\end{myrem}

\subsubsection{Substitution}
La \emph{substitution} en logique des prédicats
consiste à prendre une formule et remplacer une partie par une autre.
\begin{mynota}[Substitution]
	Soit $p$ une formule quelconque; la notation $p[x/t]$ signifie
	que l'on remplace chaque occurrence libre de $x$ par $t$.
	\begin{mydef}[Occurrence libre]
		Une \emph{occurrence libre} d'une variable
		est une occurrence
		qui n'est pas dans la portée d'un quantificateur.
		Afin d'éviter la capture de variable,
		il faut faire un renommage des variables.
	\end{mydef}
\end{mynota}

\subsubsection{Élimination du quantificateur universel}
Comme $\forall x.\ P(x)$ veut dire
\foreignquote{french}{pour tout $x_I$ appartenant au domaine de discours,
$P_I(x_I)$ est vrai}, on peut remplacer sans contrainte:
\begin{align*}
\forall x.\ P(x) &\implies P(a)\,, \quad \textnormal{où $a$ est une constante quelconque,}\\
&\implies P(y)\,, \quad \textnormal{où $y$ est une variable (parce que $P_I(y_I)$ est vrai).}
\end{align*}
La règle s'écrit alors $\begin{array}{c}
	\forall x.\ p \\
	\hline
	p[x/t]
\end{array}$. Il est important de penser au renommage,
s'il est nécessaire.

\subsubsection{Introduction du quantificateur universel}
La règle s'écrit $\begin{array}{c}
	p \\
	\hline
	\forall x.\ p
\end{array}$.
On distingue plusieurs cas:
\begin{itemize}
	\item Si $p$ n'a pas d'occurrence libre de $x$, on peut le faire.
	\item Si $p$ contient une occurrence libre de $x$,
	on doit s'assurer que la preuve jusqu'à cet endroit
	fonctionne pour toutes les valeurs de $x$.
	Il ne peut donc pas y avoir de contraintes sur $x$ jusque là.
	Cela impose deux conditions:
	\begin{itemize}
		\item $x$ n'était pas libre
		dans une formule contenant un quantificateur existentiel
		qu'on a éliminé ;
		\item $x$ n'est pas libre dans une prémisse
		(sinon, $x$ possèderait une valeur depuis le début).
	\end{itemize}
\end{itemize}

\subsubsection{Élimination du quantificateur existentiel}
$\exists x.\ P(x)$ signifie
qu'il existe un élément $x_I \in \mathcal{D}_I$
pour lequel $P_I(x_I)$ est vrai.
On ne connaît pas cet élément,
mais on peut introduire un symbole qui le représente.
\begin{align*}
	\exists x.\ P(x) &\implies P(a)\,, \quad \textnormal{où $a$ est une nouvelle constante dans la preuve ($\val{I}(a) = x_I$),}\\
	&\implies P(z)\,, \quad \textnormal{où $z$ est une nouvelle variable dans la preuve ($\val{I}(z) = x_I$),}\\
	&\nimplies P(y)\,, \quad \textnormal{où $y$ est une variable qui existe déjà dans la preuve.}
\end{align*}

\subsubsection{Introduction du quantificateur existentiel}
Si $p[t]$ est vrai, alors il existe $t_I = \VAL{I}(t) \in \mathcal{D}_I$
avec $\big(\val{I}(p)\big)(t_I)$.
On peut donc introduire $\exists x$
car un élément qui rend vrai $p[x]$ existe.
Il faut cependant faire attention
à pouvoir retrouver la formule originale à partir de $\exists x.\ p[x]$
en utilisant la substitution $p[x/t]$,
sinon l'introduction du quantificateur existentiel
a changé le sens de la formule.
La règle s'écrit alors $\begin{array}{c}
	p[t] \\
	\hline
	\exists x.\ p[x]
\end{array}$.

\section{Algorithme de preuve pour la logique des prédicats}
Il est possible d'adapter l'algorithme de preuve de la logique propositionnelle
pour qu'il fonctionne en logique des prédicats,
mais il perd alors certaines des ses propriétés.

\subsection{Propriétés}
\begin{myprop}[Propriétés de l'algorithme de preuve en logique des prédicats]
	Soit $\mathcal{B} = \{A_1, \dots, A_n\}$ l'ensemble des axiomes
	et $T$ le théorème que l'on veut prouver.
	L'algorithme de preuve est
	\begin{itemize}
		\item \textbf{Semi-décidable (\emph{semi-decidable}).}
		Si $\mathcal{B} \vDash T$, alors l'algorithme trouve une preuve.
		Sinon, il peut tourner indéfiniment.
		\item \textbf{Adéquat (\emph{sound}).}
		Si $\mathcal{B} \vdash T$,
		alors $\mathcal{B} \vDash T$.
		\item \textbf{Complet (\emph{complete}).}
		Si $\mathcal{B} \vDash T$,
		alors $\mathcal{B} \vdash T$.
	\end{itemize}
\end{myprop}

L'algorithme se base sur deux idées:
\begin{itemize}
	\item \textbf{Simplification des prémisses.}
	On transforme les prémisses de sorte à
	les avoir en forme clausale.
	\item \textbf{Simplification des règles d'inférence.}
	On garde un seule règle d'inférence, la résolution,
	qui fonctionne sur la forme clausale.
\end{itemize}

\subsection{Forme normale conjonctive (\textsc{fnc})}
La transformation des prémisses en forme clausale se fait en trois étapes.
\begin{enumerate}
	\item \emph{Mise sous forme prénexe.}
	\[
	(\cdots \forall \cdots \exists \cdots \forall) \logeq \forall \exists \forall (\cdots)\,.
	\]
	La forme prénexe est une formulation
	équivalente à la formulation initiale
	telle que tous les quantificateurs
	se retrouvent devant le reste de la formule.
	Cette transformation préserve donc les modèles.
	Elle se fait elle-même en quatre étapes:
	\begin{enumerate}
		\item Éliminer les implications et les équivalences.
		\item Renommer les variables et
		supprimer les quantificateurs si possible.
		\item Migrer les négations vers l'intérieur,
		jusqu'aux formules atomiques (prédicats).
		Cela peut toujours se faire
		grâce à la Loi~\ref{law:quant_swap}.
		\begin{mylaw}[Interchangeabilité des quantificateurs]
			\label{law:quant_swap}
			Pour passer d'un quantificateur universel
			à un quantificateur existentiel (et inversement),
			on peut utiliser la formule
			\[
			\forall x.\ \lnot p \logeq \lnot \exists x.\ p\,.
			\]
			Cette équivalence se justifie
			par un raisonnement informel sur les modèles :
			tout modèle de $\forall x.\ \lnot p$
			est aussi modèle de $\lnot \exists x.\ p$.
		\end{mylaw}
		\item Transférer les quantificateurs vers la tête de la formule.
	\end{enumerate}
	\item \emph{Mise sous forme de Skolem.}
	\[
	\forall \exists \forall (\cdots) \leadsto \forall \cdots \forall (\cdots)\,.
	\]
	La forme de Skolem transforme les quantificateurs existentiels
	en quantificateurs universels.
	Elle préserve l'existence des modèles (la satisfaisabilité),
	mais pas les modèles eux-mêmes,
	qui doivent être modifiés pour conserver la même signification.

	Notons ${x_i}_I$ les variables
	dans la portée d'un quantificateur universel
	et ${z_i}_I$ les variables
	dans la portée d'un quantificateur existentiel.
	Les quantificateurs existentiels expriment que
	pour toutes variables ${x_i}_I \in \mathcal{D}_I$,
	on peut trouver ${z_i}_I \in \mathcal{D}_I$.
	Cela implique qu'il y a une relation fondamentale
	entre les ${x_i}_I$ et les ${z_i}_I$.
	On le note par ${z_i}_I = {f_i}_I({x_i}_I)$,
	où les $f_i$ sont des \emph{fonctions de Skolem}.
	Si l'interprétation $I$ est un modèle,
	alors la nouvelle formule, sous forme de Skolem, a comme modèle
	l'interprétation $I' = I \cup \{f_i \leftarrow {f_i}_I\}$.
	La version skolémisée d'une formule
	ne lui est en général pas équivalente (le langage étant étendu).
	Néanmoins :
	\begin{itemize}
		\item tout modèle de la formule skolémisée
		est modèle de la formule initiale ;
		\item tout modèle de la formule initiale
		peut être étendu en un modèle de la formule skolémisée,
		obtenu en conservant les interprétations des symboles
		de la signature initiale,
		et en interprétant correctement
		les nouveaux symboles de fonction
		introduits par la skolémisation.
	\end{itemize}
	Une formule close et sa forme de Skolem
	sont dites \emph{équisatisfaisables} :
	l'une possède un modèle si, et seulement si l'autre en possède un.
	\item \emph{Mise sous forme normale conjonctive.}
	\[
	\forall \cdots \forall \bigwedge\limits_{i = 1}^{m} \bigvee\limits_{j = 1}^{n_i} L_{ij}
	\logeq \forall \cdots \forall \big(L_{11} \lor \dots \lor L_{1n_1}\big) \land \dots \land \big(L_{m1} \lor \dots \lor L_{mn_m}\big) \,.
	\]
	La forme normale conjonctive est une forme où la formule
	est représentée comme une conjonction de disjonctions.
	Elle est semblable à la transformation en logique propositionnelle
	et conserve les modèles.
\end{enumerate}

\subsection{Résolution}
L'algorithme de preuve n'utilise qu'une seule règle, la résolution.
Cependant, il faut l'adapter un peu
afin qu'elle fonctionne pour la logique des prédicats.
La règle va prendre la forme
\[
\frac{L_1 \lor C_1,\lnot L_2 \lor C_2}{(C_1 \lor C_2)\sigma}\,,
\]
où $L_1$ et $L_2$ représentent des ensembles d'objets généralement différents.
Afin de pouvoir utiliser la résolution telle qu'on la connaît,
on doit essayer de rendre les deux ensembles égaux.
Appelons $L$ la partie commune des deux ensembles.
Mathématiquement, il s'agit alors
de l'intersection des deux : $L = L_1 \cap L_2$.
Afin de trouver cet ensemble, on définit une \emph{substitution} $\sigma$,
de sorte à ce que $L = L_1 \sigma = L_2 \sigma$.
\begin{mytheo}[Existence d'une substitution la plus générale]
	Plusieurs choix sont possibles pour $\sigma$,
	mais on peut prouver qu'il existe une substitution unique
	qui soit plus générale (minimale) que toutes les autres.
	On appelle cette substitution
	l'\foreignquote{french}{\emph{unificateur
	le plus général}}, \textsc{upg}
	(\foreignquote{french}{\emph{most general unifier}}, \textsc{mgu}).
	On appelle le processus
	pour trouver le \textsc{mgu} l'\emph{unification}.
	S'il existe, le \textsc{mgu} est calculable\footnote{L'algorithme
	de Martelli et Montanari permet de le trouver, par exemple.}.
\end{mytheo}

\subsection{Algorithme de preuve}
L'Algorithme~\ref{algo:pred} est l'adaptation à la logique de prédicats
de l'Algorithme~\ref{algo:proof} pour la logique des propositions.

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{$\textnormal{Ax}_1,\dots,\textnormal{Ax}_n$, les axiomes,
et $\textnormal{Th}$ le théorème à prouver.
Chaque formule est en \textsc{fnc}.}
\KwResult{Si $\textnormal{Ax}_1, \dots, \textnormal{Ax}_n \vdash \textnormal{Th}$,
à condition que l'algorithme termine.
Sinon, on ne peut rien conclure.}
\Begin{
	$\mathcal{S} \gets \{\textnormal{Ax}_1, \dots, \textnormal{Ax}_n, \lnot \textnormal{Th}\}$\;
	\While{$\false \notin \mathcal{S}$ et
	$\exists p_i, p_j \in \mathcal{S}$ telles que $p_i, p_j$
	résolvables et non résolues}{
		choisir $p_i, p_j \in \mathcal{S}$
		telles que $\exists L^+ \in p_i, \lnot L^- \in p_j$
		et que $L^+$ et $L^-$ unifiables par \textsc{mgu} $\sigma$\;
		calculer $r \gets (p_i \setminus \{L^+\} \lor p_j \setminus \{ \lnot L^-\})\sigma$\;
		calculer $\mathcal{S} \gets \mathcal{S} \cup \{r\}$\;
	}
	\eIf{$\false \in \mathcal{S}$}{
		\Return \true \Comment*[r]{$\mathcal{S}$ inconsistant,
		$\textnormal{Th}$ prouvé}
	}{
		\Return \false \Comment*[r]{$\mathcal{S}$ consistant,
		$\textnormal{Th}$ non prouvé}
	}
}
\caption{Algorithme de preuve en logique des prédicats\label{algo:pred}}
\end{algorithm}

Cet algorithme utilise la preuve par contradiction.
On rajoute donc $\lnot \textnormal{Th}$ à l'ensemble des axiomes,
et essayer de déduire une contradiction.
Les déductions utilisent la règle de résolution,
bien adaptée à la forme normale conjonctive.

Il n'y a pas de garantie que l'algorithme terminera
car il est \emph{semi-décidable} ; s'il termine avec une contradiction,
cela démontre le théorème $\textnormal{Th}$.
Sinon, on ne peut pas décider.

L'algorithme a plusieurs sources de non-déterminisme :
\begin{itemize}
	\item le choix des clauses ;
	\item le choix des littéraux dans les clauses.
\end{itemize}
À chaque itération, ces choix sont faits,
et un mauvais choix peut faire en sorte que l'algorithme ne se termine pas
ou faire des déductions inutiles.

Pour que cet algorithme soit utilisable en pratique,
il est important de concrétiser les stratégies pour les choix
des paires $p_i, p_j$ et $L^+, L^-$.
Selon le choix qu'on fait,
cela donne lieu à deux types de programmes appelés
\emph{assistants de preuve} ou \emph{langages logiques}.

\subsubsection{Assistant de preuve}
Un \emph{assistant de preuve} est un outil qui aide un mathématicien humain
à faire des preuves formelles.
C'est au mathématicien de lui donner des \foreignquote{french}{coups de pouce},
sous forme de lemmes, hypothèses, chemins, stratégies,\ldots
Ensuite, l'assistant de preuve s'occupe de la manipulation des symboles.

\subsubsection{Langage logique}
Un \emph{langage logique} est un langage de programmation
basé sur la logique des prédicats.
Le plus célèbre de ces langages est Prolog.
Prolog utilise volontairement des stratégies naïves
qui permettent de rendre l'algorithme prévisible.
Les axiomes deviennent un \emph{programme}.
L'heuristique utilisée par Prolog s'appelle \textsc{lush},
pour \emph{Linear resolution with Unrestricted Selection for Horn clauses}.
Elle fonctionne comme suit :
\begin{itemize}
	\item choisir les $p_i, p_j$ dans l'ordre spécifié par l'utilisateur ;
	\item choisir de gauche à droite à l'intérieur de la clause $p_i$.
\end{itemize}
Le choix de l'ordre des clauses est donc une responsabilité de l'utilisateur.

\section{Théories logiques}
Lorsqu'on définit une structure mathématique avec la logique,
la définition contient deux types de formules:
\begin{itemize}
	\item des \emph{axiomes} ;
	\item des \emph{règles d'inférence}.
\end{itemize}
\begin{mydef}[Théorie logique]
	Une \emph{théorie logique} est un ensemble d'axiomes et de formules
	avec un vocabulaire propre et éventuellement quelques règles d'inférence
	spécifiques à la théorie.
\end{mydef}
Ces formalisations peuvent être utilisées en programmation
et dans les assistants de preuve,
mais il faut alors faire attention à la manière dont on définit
les axiomes et les règles,
car cela peut avoir une grande influence sur l'efficacité.

\subsection{Théorie du premier ordre}
\begin{mydef}[Théorie du premier ordre]
	Une théorie logique du premier ordre contient les parties suivantes :
	\begin{itemize}
		\item un sous langage de la logique du premier ordre :
		\begin{itemize}
			\item vocabulaire : constantes, fonctions, prédicats ;
			\item règles syntaxiques
			et sémantiques sur ce vocabulaire.
		\end{itemize}
		\item ensemble d'axiomes ;
		\item ensemble de règles d'inférence
	\end{itemize}
	Les axiomes et les règles d'inférence sont des formules \emph{fermées},
	ne contenant pas de variables libres.
\end{mydef}

Nous allons voir quatre exemples de théories du premier ordre :
\begin{itemize}
	\item \textsc{fam}, la théorie des liens familiaux ;
	\item \textsc{ops}, la théorie d'ordre partiel strict ;
	\item \textsc{eg}, la théorie d'égalité et
	\item \textsc{op}, la théorie d'ordre partiel.
\end{itemize}

\subsection{Théorie des liens familiaux (\textsc{fam})}
La théorie des liens familiaux possède le vocabulaire suivant :
\begin{itemize}
	\item deux symboles de fonctions à un paramètre : $p/1$ et $m/1$ ;
	\item trois symboles de prédicats à deux paramètres :
	$P/2$, $GM/2$ et $GP/2$.
\end{itemize}
Une signification possible de chaque symbole pourrait être
\begin{itemize}
	\item $p$ et $m$ signifient
	\foreignquote{french}{père de} et \foreignquote{french}{mère de} ;
	\item $P$, $GM$ et $GP$ dénotent \foreignquote{french}{parent de},
	\foreignquote{french}{grand-mère de} et
	\foreignquote{french}{grand-père de}.
\end{itemize}

Les axiomes de la théorie sont
\begin{align*}
	&(\forall x) (P(x, p(x))) \tag{\textnormal{Père}} \\
	&(\forall x) (P(x, m(x))) \tag{\textnormal{Mère}} \\
	&(\forall x) (\forall y) (P(x, y) \to GP(x, p(y))) \tag{\textnormal{Grand-père}} \\
	&(\forall x) (\forall y) (P(x, y) \to GM(x, m(y))) \tag{\textnormal{Grand-mère}} \\
\end{align*}

Les règles d'inférence sont uniquement celles de la logique des prédicats.
Une théorie logique peut avoir plusieurs modèles.

\subsection{Propriétés des théories}
Une théorie logique a plusieurs propriétés :
\begin{itemize}
	\item Une formule fermée $p$ est \emph{valide}
	dans la théorie \textsc{th} si elle est vraie
	dans chaque modèle de \textsc{th}.
	On l'écrit $\models_{\textnormal{\textsc{th}}} p$.
	Soit l'ensemble des axiomes
	$\textnormal{Ax} = \{\textnormal{Ax}_1, \ldots, \textnormal{Ax}_n\}$.
	On a bien que $\models_{\textnormal{\textsc{th}}} \textnormal{Ax}_i$.
	\item $q$ est une \emph{conséquence logique} de $p$
	dans la théorie \textsc{th} si $q$ est vraie
	dans tous les modèles de \textsc{th} qui rendent $p$ vraie.
	On écrit $p \vDash_{\textnormal{\textsc{th}}} q$.
	En anglais, cela se dit \foreignquote{french}{$p$ \emph{entails} $q$}.
	\item Une théorie est \emph{consistante} si elle a au moins un modèle.
	\item Une théorie est \emph{inconsistante} si elle n'a pas de modèle.
\end{itemize}

\subsubsection{Établir la validité}
Pour établir $\models_{\textnormal{\textsc{th}}} p$,
il y a deux approches :
\begin{itemize}
	\item L'approche \emph{sémantique} :
	on prend un modèle quelconque $I$
	et on évalue la théorie grâce à la fonction de valuation $\VAL{I}$.
	\item L'approche \emph{syntaxique} :
	on essaie de construire une preuve de $p$ à partir des axiomes,
	en appliquant les règles de la théorie.
	Cette approche s'appelle aussi \emph{théorie de la preuve}.
\end{itemize}

\subsubsection{Qualité d'une théorie}
Une \foreignquote{french}{bonne} théorie est
\begin{itemize}
	\item \emph{Consistante.}
	Il est impossible de déduire $p$ et $\lnot p$ de la même théorie.
	\item \emph{Minimale.}
	Les axiomes sont indépendants : $\{\textnormal{Ax}_1, \ldots, \textnormal{Ax}_n\} \nvDash \textnormal{Ax}_k$.
	\item \emph{Complète.}
	Les axiomes suffisent pour prouver les propriétés d'intérêt.
\end{itemize}

\subsection{Opérations sur les théories}
Définissons maintenant une opération, appellée l'\emph{extension},
qui permet de créer une nouvelle théorie par rapport à une théorie existante,
et deux opérations (l'inclusion et l'équivalence)
qui permettent de comparer deux théories.

\subsubsection{Extension d'une théorie}
Afin de rendre une théorie plus complète,
on souhaite parfois l'étendre.
\begin{mydef}[Extension d'une théorie]
	Une théorie $\textnormal{\textsc{th}}_2$ est
	une extension de $\textnormal{\textsc{th}}_1$ si
	\begin{itemize}
		\item le vocabulaire de $\textnormal{\textsc{th}}_1$ est inclus
		dans le vocabulaire de $\textnormal{\textsc{th}}_2$
		($V_1 \subseteq V_2$) ;
		\item tout axiome de $\textnormal{\textsc{th}}_1$ est axiome
		de $\textnormal{\textsc{th}}_2$.
	\end{itemize}
\end{mydef}
On peut donc étendre le vocabulaire et ajouter des axiomes.
\begin{myrem}
	Étendre un théorie peut réduire son nombre de modèles,
	et celui-ci peut même devenir nul.
	L'ensemble des modèles de la théorie étendue est
	un sous-ensemble de l'ensemble des modèles de la théorie initiale.
\end{myrem}

\subsubsection{Comparaison de théories}
Soit $V_i$ le vocabulaire,
$M_i$ l'ensemble des modèles et
$\{\textnormal{Ax}_i\}$ l'ensemble des axiomes de $\textnormal{\textsc{th}}_i$.

\begin{mydef}[Inclusion de théories]
	Si $V_1 \subseteq V_2$ et
	toute formule valide dans $\textnormal{\textsc{th}}_1$
	l'est aussi dans $\textnormal{\textsc{th}}_2$.
	On a alors $M_2 \subseteq M_1$,
	et on dit que $\textnormal{\textsc{th}}_1$
	est \emph{contenue} dans $\textnormal{\textsc{th}}_2$.
\end{mydef}

\begin{mydef}[Équivalence de théories]
	On dit que $\textnormal{\textsc{th}}_1$
	et $\textnormal{\textsc{th}}_2$ sont \emph{équivalentes}
	si $\textnormal{\textsc{th}}_1$ est contenue
	dans $\textnormal{\textsc{th}}_2$
	et $\textnormal{\textsc{th}}_2$ est
	contenue dans $\textnormal{\textsc{th}}_1$.
\end{mydef}

\begin{myrem}
	Il faut bien faire la différence entre l'inclusion d'une théorie
	et l'extension de celle-ci :
	l'extension est définie par rapport aux axiomes de la théorie,
	alors que l'inclusion est définie par rapport aux modèles.
	L'extension est donc une manipulation syntaxique,
	alors que l'inclusion est une manipulation sémantique.
\end{myrem}

\begin{mycorr}
	Si $V_1 \subseteq V_2$ et $M_2 \subseteq M_2$,
	alors $\textnormal{\textsc{th}}_1$ est contenue
	dans $\textnormal{\textsc{th}}_2$.
\end{mycorr}

\begin{mycorr}
	Si $V_1 \subseteq V_2$ et
	$\{\textnormal{Ax}_1\} \subseteq \{\textnormal{Ax}_2\}$,
	alors $\textnormal{\textsc{th}}_1$ est contenue
	dans $\textnormal{\textsc{th}}_2$.
	Il est donc vrai que $M_2 \subseteq M_1$ :
	une extension donne lieu à une extension (\emph{mais pas inversement}).
\end{mycorr}

\begin{mycorr}
	Si $V_1 = V_2$, et pour tout axiome $p \in \{\textnormal{Ax}_1\}$,
	nous avons $\models_{\textnormal{\textsc{th}}_2} p$
	et pour tout axiome $q \in \{\textnormal{Ax}_2\}$
	nous avons $\models_{\textnormal{\textsc{th}}_1} q$,
	alors $\textnormal{\textsc{th}}_1$ et $\textnormal{\textsc{th}}_2$
	sont équivalentes.
\end{mycorr}

\begin{mycorr}
	Si $p$ est une formule fermée
	telle que $\models_{\textnormal{\textsc{th}}_1} p$
	et $\textnormal{\textsc{th}}_2 = \textnormal{\textsc{th}}_1 \cup \{p\}$,
	alors $\textnormal{\textsc{th}}_1$ et $\textnormal{\textsc{th}}_2$
	sont équivalentes.
\end{mycorr}

\subsection{Théorie des ordres partiels stricts (\textsc{ops})}

La théorie des liens familiaux possède le vocabulaire suivant :
\begin{itemize}
	\item un symbole de prédicat à deux paramètres : $P/2$.
\end{itemize}

Les axiomes de la théorie sont
\begin{align*}
	&(\forall x) \lnot P(x, x) \tag{\textnormal{Irréflexivité, \textsc{ops}}1} \\
	&(\forall x)(\forall y)(\forall z) (P(x, y) \land P(y, z)) \to P(x, z) \tag{\textnormal{Transitivité, \textsc{ops}}2}
\end{align*}
Un modèle possible de cette théorie est $\mathcal{D}_I = \mathbb{Z}$,
$\val{I}(P) = $ \og$<$ \fg{}.

\subsection{Théorie de l'égalité (\textsc{eg})}
\begin{mynota}[Égalité]
	Plusieurs symboles pour représenter l'égalité existent.
	Dans le cours, le professeur Van Roy utilise \og $x == y$ \fg{},
	mais d'autres notations existent (\og $x = y$ \fg{}, \og $Exy$ \fg{}).
\end{mynota}

\subsubsection{Axiomes}
La théorie de l'égalité contient trois axiomes et deux schémas d'axiomes.
\begin{align*}
	&\forall x.\ (x == x) \tag{\textnormal{Réflexivité}} \\
	&\forall x.\ \forall y.\ (x == y) \to (y == x) \tag{\textnormal{Symétrie}} \\
	&\forall x.\ \forall y.\ \forall z.\ \Big((x == y) \land (y == z)\Big) \to (x == z) \tag{\textnormal{Transitivité}} \\
	&\forall x.\ \forall x_1. \cdots \forall x_n.\ (x == x_i) \to \Big(f(\ldots, x, \ldots) == f(\ldots, x_i, \ldots)\Big) \tag{\textnormal{Substituabilité dans les fonctions}} \\
	&\forall x.\ \forall x_1. \cdots \forall x_n.\ (x == x_i) \to \Big(P(\ldots, x, \ldots) == P(\ldots, x_i, \ldots)\Big) \tag{\textnormal{Substituabilité dans les prédicats}}
\end{align*}
CLes deux schémas d'axiomes exprimant
la substituabilité des fonctions et des prédicats
définissent plusieurs axiomes, où on remplace $f$ ou $P$
par tout symbole de fonction ou de prédicat, respectivement.

\subsubsection{Règles d'inférence}
Il faut également définir deux règles d'inférence.
\begin{align*}
	&\begin{array}{c}
	(s_1 == t_1) \land \cdots \land (s_n == t_n) \\
	\hline
	f(s_1, \ldots, s_n) == f(t_1, \ldots, t_n)
	\end{array} \tag{\textnormal{Substituabilité fonctionnelle}} \\
	&\\
	&\begin{array}{c}
	(s_1 == t_1) \land \cdots \land (s_n == t_n) \\
	\hline
	P(s_1, \ldots, s_n) == P(t_1, \ldots, t_n)
	\end{array} \tag{\textnormal{Substituabilité prédicative}}
\end{align*}

\subsubsection{Théorème sur la sémantique de l'égalité}
\begin{mytheo}
	Si $\VAL{I}(t_1) == \VAL{I}(t_2)$ alors $\VAL{I}(t_1 == t_2) = \true$.
	L'égalité comme définie ci-dessus est une propriété \emph{syntaxique}
	qui permet de faire des preuves.
	Grâce à ce théorème, nous pouvons également raisonner sur un modèle
	(faisant partie de la sémantique) pour déterminer l'égalité.
	Si deux termes donnent la même entité dans le domaine de discours,
	alors on peut dire que les deux termes sont égaux.
	\begin{proof}
		Soit $I$ un modèle de \textsc{eg} et
		$t_1, t_2$ deux termes quelconques.
		Par définition, $\VAL{I}(t_1 == t_2) =
		\VAL{I}(==)\big(\VAL{I}(t_1), \VAL{I}(t_2)\big)$.
		On pose alors $\VAL{I}(==) = E_I$ et
		$\VAL{I}(t_i) = e_i$ pour $i = 1, 2$.
		La prémisse du théorème nous dit que $e_1 = e_2 = e$.
		On trouve alors $\VAL{I}(t_1 == t_2) = E_I(e, e)$.
		Par l'axiome de réflexivité,
		$\VAL{I}(\forall x.\ (x == x) = \true$.
		La sémantique de $\forall$ nous dit que
		pour tout $d \in \mathcal{D}_I$,
		il est vrai que $\VAL{I}(d == d) = \true$.
		On prend $d = e$ et
		on peut alors conclure que $E_I(e, e) = \true$.
	\end{proof}
	\begin{myrem}
		Cette preuve n'est pas une preuve en logique formelle
		mais bien en métalangage ;
		on ne peut pas la formaliser comme un objet formel
		en logique des prédicats.
		Toute l'argumentation sur la sémantique
		de la logique des prédicats
		et la justification des règles de preuve
		sont des raisonnements en métalangage.
	\end{myrem}
\end{mytheo}

\subsection{Théorie de l'ordre partiel (\textsc{op})}
On peut dire en quelque sorte que
$\textnormal{\textsc{op}} = \textnormal{\textsc{ops}} + \textnormal{\textsc{eg}}$.
Pour définir la théorie de l'ordre partiel,
nous prenons \textsc{eg} et nous ajoutons un symbole en plus
du symbole d'égalité : \og $\le$ \fg{}.
Il faut donc définir des nouveaux axiomes pour ce nouveau symbole.
\begin{align*}
	&\forall x.\ (x \le x) \tag{\textnormal{Réflexivité}} \\
	&\forall x.\ \forall y.\ \Big((x \le y) \land (y \le x)\Big) \to (y == x) \tag{\textnormal{Anti-symétrie}} \\
	&\forall x.\ \forall y.\ \forall z.\ \Big((x \le y) \land (y \le z)\Big) \to (x \le z) \tag{\textnormal{Transitivité}} \\
	&\forall x.\ \forall x_1.\ \forall x_2.\ (x_1 == x) \to \Big((x_1 \le x_2) \leftrightarrow (x \le x_2)\Big) \tag{\textnormal{Substituabilité à gauche}} \\
	&\forall x.\ \forall x_1.\ \forall x_2.\ (x_2 == x) \to \Big((x_1 \le x_2) \leftrightarrow (x_1 \le x)\Big) \tag{\textnormal{Substituabilité à droite}}
\end{align*}
\begin{myrem}
	Pour l'axiome d'anti-symétrie,
	l'implication pourrait être une équivalence.
	Cependant, afin de garder une théorie minimale,
	et comme l'équivalence peut se démontrer avec les axiomes ci-dessus,
	elle n'est pas explicitement demandée.
\end{myrem}
Un exemple de modèle pour \textsc{op} est $\mathcal{D}_I = \mathbb{Z}$,
$\val{I}(==) =$ \og $=$ \fg{}, l'égalité d'entiers,
et $\val{I}(\le) =$ \og $\le$ \fg{},
le symbole plus petit ou égal pour les entiers.

\section{Introduction à la programmation logique}
Pour la programmation logique,
l'exécution d'un programme correspond à un raisonnement logique
dans une théorie logique.
Elle propose l'approche suivante :
\begin{itemize}
	\item le programme est un \emph{ensemble d'axiomes}
	en logique des prédicats ;
	\item l'exécution du programme
	est faite par un \emph{prouveur de théorèmes} ;
	\item l'exécution démarre avec une \emph{requête},
	une formule à prouver en logique des prédicats.
\end{itemize}
Cette approche permet
\begin{itemize}
	\item la vérification des programmes et
	\item l'intelligence artificielle (en partie, pour l'instant).
\end{itemize}

\subsection{La voie vers la programmation logique}
Pour tout langage de programmation logique,
il y a un compromis entre expressivité et efficacité.
Il faut résoudre trois problèmes :
\begin{enumerate}
	\item \emph{Limitations theóriques du prouveur.}
	L'algorithme de preuve en logique des prédicats est semi-décidable,
	et peut tourner à l'infini.
	\item \emph{Efficacité du prouveur.}
	Même lorsqu'il est possible de trouver une preuve,
	le prouveur prend un temps exponentiel.
	Il doit donc avoir une sémantique opérationnelle efficace et prévisible.
	\item \emph{Construction du résultat.}
	Le prouveur ne peut pas se contenter
	de démontrer l'existence de la réponse,
	il doit également la construire.
\end{enumerate}

\subsubsection{Limitations théoriques du prouveur}
L'algorithme de preuve est
\begin{itemize}
	\item \emph{Adéquat.}
	Si $p \vdash q$, alors $p \vDash q$.
	\item \emph{Complet.}
	Si $p \vDash q$, alors $p \vdash q$.
\end{itemize}
Cependant, l'algorithme n'est pas décidable (il est semi-décidable).
S'il n'arrive pas à prouver $p \vdash q$, on ne peut rien conclure.

\subsubsection{Efficacité du prouveur}
L'algorithme de preuve a une complexité exponentielle.
Pour réduire le temps d'exécution, les solutions utilisées sont :
\begin{itemize}
	\item \emph{Mettre des restrictions sur la forme des axiomes.}
	On ne permet que des axiomes pour lesquels
	l'algorithme sait raisonner efficacement.
	Typiquement, on ne permet dans une clause $C_i$
	seulement un littéral sans négation.
	Ce type de clause s'appelle une \emph{clause de Horn}.
	\item \emph{Permettre au programmeur de donner des heuristiques.}
	Le programmeur est souvent libre de spécifier l'ordre dans lequel
	le programme doit essayer de résoudre les clauses.
\end{itemize}
Le langage Prolog utilise ces deux techniques pour obtenir un langage efficace.

\subsubsection{Construction du résultat}
Notre algorithme de preuve construit de nouvelles clauses
à partir des anciennes.
La résolution utilise l’unification, ce qui instancie les variables.
Pour obtenir un résultat final d’une requête,
il suffit de garder la requête pendant le calcul
et de l’instancier avec les mêmes substitutions utilisées par la résolution.
À la fin du calcul, les variables de la requête
contiendront alors le résultat.

\subsection{Introduction au langage Prolog}
Un programme en Prolog est un ensemble de clauses de la forme
\[
A_1 \leftarrow B_{11}, \ldots, B_{1n}\,.
\]
L'intuition d'une clause est que l'on peut prouver $A_i$
en prouvant $B_{i1}, \ldots B_{in}$.
On voit facilement qu'il s'agit d'une clause de Horn.

\subsubsection{Exemple de programme en Prolog}
Écrire un programme en Prolog, c'est formuler les algorithmes
avec des axiomes en logique.
Voici un exemple de programme en Prolog.
\begin{minted}{prolog}
grandpere(X,Z) :- pere(X,Y), pere(Y,Z).
pere(terach, abraham).
pere(abraham, isaac).
pere(haram, lot).
\end{minted}

Les noms de variable sont en majuscules
et le symbole \og $\leftarrow$ \fg{}
est représenté par \og \mintinline{prolog}{:-} \fg{}.
Cette syntaxe représente la forme normale suivante:
\[
\forall x.\ \forall y.\ \forall z.\ \Big(GP(x, z) \lor \lnot P(x, y) \lor \lnot P(y, z)\Big) \land P(t, a) \land P(a, i) \land P(h, l)\,.
\]
On voit clairement qu'il y a une correspondance entre Prolog
et les bases de données relationnelles.
Un programme Prolog peut être vu comme une base de données relationnelle,
augmentée avec la déduction.

\subsection{Algorithme d'exécution de Prolog}
L'algorithme d'exécution de Prolog est basé
sur l'algorithme de preuve de la logique des prédicats,
modifié avec les techniques nécessaires pour être plus efficace
et pour construire les résultats.
Au lieu de travailler avec un ensemble $\mathcal{S}$ grandissant toujours,
on ne retient que la clause $r$ qui s'appelle la \emph{résolvante}.
On n'augmente donc jamais le nombre de clauses.
On ne garde que trois choses :
\begin{itemize}
	\item les axiomes de base ($P = \{\textnormal{Ax}_1, \ldots, \textnormal{Ax}_n\}$);
	\item la résolvante $r$ ;
	\item la requête originale $G$, c'est-à-dire le théorème à prouver.
\end{itemize}

\subsubsection{Explication de l'algorithme}
Voici les différentes étapes de l'exécution de l'algorithme :
\begin{enumerate}
	\item Mettre $G$ dans $r$, sans négation.
	\item Tant que $r$ n'est pas vide,
	prendre le premier littéral dans $r$, par exemple $A_1$.
	\item Parcourir les axiomes de $P$
	pour trouver une clause $\textnormal{Ax}_i$ unifiable avec $A_1$
	au moyen du \textsc{mgu}, $\sigma$.
	\begin{itemize}
		\item Si on trouve une clause unifiable,
		ajouter à $r$ les littéraux de cette clause
		après unification avec $A_1$, puis reprendre depuis le début.
		\item Si on ne trouve pas de clause unifiable,
		remettre $r$ dans son état initial
		et revenir sur le dernier choix.
		\item Si on épuise tous les choix sans que $r$ ne soit vide,
		$G$ ne peut pas être prouvé.
	\end{itemize}
	\item Si $r$ est vide ou qu'il n'y a plus de choix à faire,
	arrêter l'exécution. Si $r$ est vide, $G$ contient le résultat.
\end{enumerate}
Il est possible que l'algorithme boucle à l'infini ;
c'est au programmeur de veiller à définir les axiomes de telle sorte
que ce scénario ne se passe pas.

\subsubsection{Définition de l'algorithme}

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{$\textnormal{Ax}_1,\dots,\textnormal{Ax}_n$, les axiomes,
et $G$ le théorème à prouver.
Chaque formule est une clause de Horn.}
\KwResult{Si $\textnormal{Ax}_1 \land \cdots \land \textnormal{Ax}_n \vdash G$,
à condition que l'algorithme termine.
Sinon, on ne peut rien conclure.}
\Begin{
	$r \gets \langle G \rangle$ \Comment*[r]{Résolvante initiale ;
	pendant l'exécution, $r = \langle A_1, \ldots, A_m \rangle$}
	\While{$r$ est non vide et qu'il reste des choix}{
		Choisir le premier littéral, $A_1$, dans $r$\;
		Choisir une clause $\textnormal{Ax}_i = (A \leftarrow B_1, \ldots, B_k)$ dans $P$.
		On commence par prendre la première, puis la suivante,
		et ainsi de suite, jusqu'à trouver
		une clause unifiable avec $A_1$.
		Si aucune clause n'est unifiable,
		on revient sur le dernier choix (\emph{backtrack})\;
		$r \gets \langle B_1, \ldots, B_k, A_2, \ldots, A_m \rangle \sigma$.\;
		$G \gets G \sigma$\;
	}
	\If{$r$ est vide}{
		\Return $G$ \Comment*[r]{Le théorème est prouvé ;
		si on le veut, on peut faire un retour en arrière pour trouver d'autres solutions}
	}
	\If{$r$ non vide mais on a épuisé les choix}{
		\Return $\false$ \Comment*[r]{On n'a pas prouvé $G$,
		mais il peut être vrai
		(les heuristiques sont insuffisantes pour le prouver)}
	}
}
\caption{Algorithme de Prolog \label{algo:prolog}}
\end{algorithm}

\subsubsection{Gestion des choix}
L'algorithme fait parfois un retour en arrière (\emph{backtrack})
pour changer un choix.
Chaque fois que l'on choisit une clause
à unifier avec le début de la résolvante
cela marque un endroit où l'on peut faire un retour en arrière.
Pour faire le retour en arrière,
on remet les structures de données internes $r$ et $G$ à l'état avant le choix,
et on recommence l'exécution à l'endroit du dernier choix.
S'il n'y a plus de clauses possibles,
on revient au choix d'avant, et ainsi de suite.
La gestion des choix est \emph{récursive}.
Il y a une pile de choix gérée par l'algorithme.
Pour simplifier la présentation,
la gestion de la pile n'est pas montrée dans la définition de l'algorithme.
Si on arrive au tout premier choix qui a été fait,
et qu'on ne peut plus unifier de clauses,
l'algorithme termine sans solution.
C'est une faiblesse de l'algorithme.
Si au contraire, on arrive à rendre $r$ vide,
on a une solution dans $G$.

Une conséquence intéressante de ceci est que
l'algorithme peut trouver plusieurs solutions différentes
pour une requête donnée (si elles existent).

\subsection{Exemples de programmes Prolog}
Comme dit plus haut,
l'idée de base de la programmation logique est qu'un programme
est un ensemble d'axiomes qui énoncent des propriétés vraies
des algorithmes que l'on veut programmer.
\begin{myexem}[Factorielle en Prolog]
	Ce programme permet de calculer la factorielle d'un nombre.
	On commence par établir un fait : $0! = 1$,
	ou bien en Prolog, \mintinline{prolog}{fact(0, 1).}.
	Ensuite, on définit une clause pour les factorielles en général :
	$n! = n (n-1)!$ si $n > 0$.
	Le programme est le suivant :
\begin{minted}{prolog}
fact(0, 1).
fact(N,F) :- N>0, N1 is N-1, fact(N1,F1), F is N*F1.
\end{minted}
	Les virgules sont les séparateurs entre littéraux négatifs
	alors que le point marque la fermeture de la clause.

	Ce programme n'est pas récursif terminal ;
	l'exécution dépend également d'une pile
	et il serait donc intéressant de le mettre en forme récursive terminale.
	Pour un compilateur Prolog, la différence est importante.

	Étudions maintenant l'exécution du programme.
	Le prompt Prolog est représenté par \og \mintinline{prolog}{|?-} \fg{}
	et la sortie standard par \og \mintinline{prolog}{->} \fg{}.
	\catcode`\%=11
	Les commentaires sont écrits après des \og \mintinline{prolog}{%} \fg{}
	où entre \og \mintinline{prolog}{/* */} \fg{}.
	Le point à la fin de la requête est important également.

\begin{minted}{prolog}
|?- fact(5,F). % Requête de l'utilisateur
-> F=120       % Réponse du système
\end{minted}

	Sous forme clausale, le programme s'écrit
	\[
	\mathrm{fact}(0, 1) \land \forall n.\ \forall f.\ \forall n_1.\ \forall f_1.\ \mathrm{fact}(n, f) \lor \lnot (n > 0) \lor \lnot \mathrm{minus}(n_1, n, 1) \lor \lnot \mathrm{fact}(n_1, f_1) \lor \lnot \mathrm{times}(f, n, f_1)\,.
	\]
	Les prédicats mathématiques font partie du système et sont prédéfinis :
	\begin{itemize}
		\item $a > b$ est vrai si
		$a$ est strictement plus grand que $b$ ;
		\item $\mathrm{minus}(a, b, c)$ est vrai si $a = b - c$ ;
		\item $\mathrm{times}(a, b, c)$ est vrai si $a = bc$.
	\end{itemize}

	Afin de prouver $G = \mathrm{fact}(5, r)$,
	Prolog construira au fur et à mesure de l'exécution
	une substitution finale $\sigma_{\textnormal{res}}$
	en faisaant les résolutions qui feront évoluer $r$.
	Appliquée à $G$, cette substitution donne le résultat final.
	\begin{table}[H]
		\centering
		\begin{tabular}{lll}
			\hline
			\\
			Requête initiale & $\sigma_0 = \emptyset$ & $r = \langle \mathrm{fact}(5, n) \rangle$ \\
			Résolution $1$ & $\sigma_1 = \sigma_0 \cup \{(n, 5), (f, r)\}$ & $r = \langle (5 > 0), \mathrm{minus}(n_1, 5, 1), \mathrm{fact}(n_1, f_1), \mathrm{times}(r, 5, f_1) \rangle$ \\
			Résolution $2$ & $\sigma_2 = \sigma_1$ & $r = \langle \mathrm{minus}(n_1, 5, 1), \mathrm{fact}(n_1, f_1), \mathrm{times}(r, 5, f_1) \rangle$ \\
			Résolution $3$ & $\sigma_3 = \sigma_2 \cup \{(n_1, 4)\}$ & $r = \langle \mathrm{fact}(4, f_1), \mathrm{times}(r, 5, f_1) \rangle$ \\
			\\
			$\vdots$ & $\vdots$ & $\vdots$ \\
			\\
			Fin & $\sigma_{\textnormal{res}} = \{(r, 120), \ldots\}$ & $r = \langle \rangle$ \\
			\\
			\hline
		\end{tabular}
	\end{table}
	Le résultat final est donc $G \sigma = \mathrm{fact}(5, 120)$,
	ou bien en Prolog : \mintinline{prolog}{fact(5,120)}.
\end{myexem}

\begin{myexem}[Concaténation de listes]
	Ce programme fait la concaténation de deux listes.
\begin{minted}{prolog}
append([],L,L).
append([X|L1],L2,[X|L3]) :- append(L1,L2,L3).
\end{minted}

	La requête initiale est $G = \textnormal{append}(\mathrm{cons}(1, \mathrm{nil}), \mathrm{cons}(2, \mathrm{nil}), \ell)$.
	En syntaxe Prolog, on veut concaténer
	les listes \mintinline{prolog}{[1]} et \mintinline{prolog}{[2]}.
	La paire de listes écrite \mintinline{prolog}{[X|L1]} en Prolog
	s'écrit $\mathrm{cons}(x, \ell_1)$.
	Notre requête Prolog s'écrit
\begin{minted}{prolog}
|?- append([1],[2],L).
-> L=[1,2]
\end{minted}

	Sous forme clausale, le programme devient
	\[
	\textnormal{append}(\mathrm{nil}, \ell', \ell') \land \Big(\forall \ell_1.\ \forall \ell_2.\ \forall \ell_3.\ \forall x.\ \textnormal{append}\big(\mathrm{cons}(x, \ell_1), \ell_2, \mathrm{cons}(x, \ell_3)\big) \lor \lnot \textnormal{append}(\ell_1, \ell_2, \ell_3) \Big)\,.
	\]

	L'exécution est la suivante :
	\begin{table}[H]
		\centering
		\begin{tabular}{ll}
			\hline
			\\
			\multirow{2}{*}{Requête initiale} & $\sigma_0 = \emptyset$ \\
			& $r = \langle \textnormal{append}\big(\mathrm{cons}(1, \mathrm{nil}), \mathrm{cons}(2, \mathrm{nil}), \ell \big)\rangle$ \\
			\\
			\multirow{2}{*}{Résolution $1$} & $\sigma_1 = \sigma_0 \cup \{(x, 1), (\ell_1, \mathrm{nil}), (\ell_2, \mathrm{cons}(2, \mathrm{nil})), (\ell, \mathrm{cons}(x, \ell_3))\}$ \\
			& $r = \langle \textnormal{append}\big(\mathrm{nil}, \mathrm{cons}(2, \mathrm{nil}), \ell_3 \big)\rangle$ \\
			\\
			\multirow{2}{*}{Résolution $2$} & $\sigma_{\textnormal{res}} = \sigma_1 \cup \{(\ell', \mathrm{cons}(2, \mathrm{nil})), (\ell_3, \ell')\}$ \\
			& $r = \langle \rangle$ \\
			\\
			\hline
		\end{tabular}
	\end{table}

	Lors de la première résolution,
	il y a une unification entre le premier élément de $r$
	et la tête d'une clause dans le programme.
	Cette unification donne lieu à une substitution.
	La nouvelle résolvante après la résolution
	est le littéral négatif de la clause.
	Après la deuxième résolution, l'exécution s'arrête avec réussite,
	parce que $r$ est vide.
	Le résultat final est
	$G = \textnormal{append}\big(\mathrm{cons}(1, \mathrm{nil}), \mathrm{cons}(2, \mathrm{nil}), \mathrm{cons}(1, \mathrm{cons}(2, \mathrm{nil}))\big)$.

	\begin{myrem}
		Chaque fois qu'on fait une résolution
		avec une clause du programme,
		il faut renommer les variables de la clause.
	\end{myrem}

	Le programme précédent permet de calculer plusieurs solutions,
	à condition qu'elles existent.
	Prenons la requête $G = \textnormal{append}(\ell_1, \ell_2, \mathrm{cons}(1, \mathrm{nil}))$.
	Il y a deux solutions possibles :
	\begin{itemize}
		\item $\ell_1 = \mathrm{nil}$, $\ell_2 = \mathrm{cons}(1, \mathrm{nil})$ ;
		\item $\ell_1 = \mathrm{cons}(1, \mathrm{nil})$ et $\ell_2 = \mathrm{nil}$.
	\end{itemize}

	En syntaxe Prolog,
	la requête est \mintinline{prolog}{append(L1,L2,[1]).}.
	Pour demander une solution supplémentaire,
	l'utilisateur peut utiliser le caractère \mintinline{prolog}{;}.
\begin{minted}{prolog}
|?- append(L1,L2,[1]).
-> L1=[], L2=[1] ;
-> L1=[1], L2=[]
\end{minted}

	L'exécution est la suivante :
	\begin{table}[H]
		\centering
		\begin{tabular}{ll}
			\hline
			\\
			\multirow{2}{*}{Requête initiale} & $\sigma_0 = \emptyset$ \\
			& $r = \langle \textnormal{append}\big(\ell_1, \ell_2, \mathrm{cons}(1, \mathrm{nil}) \big)\rangle$ \\
			\\
			\multirow{2}{*}{Résolution $1$} & $\sigma_{\textnormal{res}} = \sigma_0 \cup \{(\ell_1, \mathrm{nil}), (\ell_2, \ell'), (\ell', \mathrm{cons}(1, \mathrm{nil}))$ \\
			& $r = \langle \rangle$ \\
			\\
			\hline
		\end{tabular}
	\end{table}
	L'algorithme commence par choisir la première clause.
	Cela donne le résultat
	$G = \textnormal{append}\big(\mathrm{nil}, \mathrm{cons}(1, \mathrm{nil}), \mathrm{cons}(1, \mathrm{nil})\big)$.
	En syntaxe Prolog,
	ce résultat s'écrit \mintinline{prolog}{append([],[1],[1])}.
	Si on demande un autre résultat,
	l'algorithme fera un retour en arrière
	pour faire un autre choix de clause.
	Dans ce cas-ci, ce choix est fait au moment de la première résolution.
	Une autre possibilité est
	\begin{table}[H]
		\centering
		\begin{tabular}{ll}
			\hline
			\\
			\multirow{2}{*}{Résolution $1'$} & $\sigma_{1'} = \sigma_0 \cup \{(\ell_1, \mathrm{cons}(x', \ell'_1)), (\ell_2, \ell'_2), (x', 1), (\ell'_3, \mathrm{nil})\}$ \\
			& $r = \langle \textnormal{append}\big(\mathrm{cons}(1, \ell'_1), \ell'_2, \mathrm{nil} \big)\rangle$ \\
			\\
			\multirow{2}{*}{Résolution $2'$} & $\sigma_{2'} = \sigma_1 \cup \{(\ell'', \mathrm{cons}(1, \ell'_1)), (\ell'', \ell'_2)\}$ \\
			& $r = \langle \textnormal{append}\big(\ell'_1, \ell'_2, \mathrm{nil} \big)\rangle$ \\
			\\
			\multirow{2}{*}{Résolution $3'$} & $\sigma_{\textnormal{res}'} = \sigma_2 \cup \{(\ell'_1, \mathrm{nil}), (\ell'_2, \ell'''), (\ell''', \mathrm{nil})\}$ \\
			& $r = \langle \rangle$ \\
			\\
			\hline
		\end{tabular}
	\end{table}
	Cette fois-ci, le résultat est
	$G = \textnormal{append}\big(\mathrm{cons}(1, \mathrm{nil}), \mathrm{nil}, \mathrm{cons}(1, \mathrm{nil})\big)$.
	En syntaxe Prolog,
	ce résultat s'écrit \mintinline{prolog}{append([1],[],[1])}.
	On peut essayer de trouver une troisième résultat
	en faisant un nouveau retour en arrière
	(la résolution $3'$ choisit la première clause),
	mais l'unification échouera ;
	il n'y a que deux résultats pour notre requête.
\end{myexem}

\subsubsection{Remarques finales}
Il y a deux façons de percevoir l'exécution d'un programme Prolog :
\begin{itemize}
	\item \emph{La vue opérationnelle.}
	L'exécution est vue comme une suite de calculs.
	On se concentre sur l'exécution de l'algorithme de preuve.
	Le résultat d'un calcul est une séquence d'opérations,
	avec ses problèmes de temps d'exécution et d'espace mémoire utilisé.
	\item \emph{La vue logique.}
	L'exécution est vue comme une preuve en logique des prédicats.
	On oublie l'algorithme et on se concentre sur la logique.
	Le résultat d'un calcul est un théorème,
	donc une conséquence logique des axiomes.
\end{itemize}
On peut donc raisonner (presque) indépendamment
sur l'efficacité et l'exactitude du programme.

\begin{myprop}[Équation de Kowalski]
	La relation entre la vue logique et la vue opérationnelle
	est résumée par l'équation de Kowalski :
	\[
	\textnormal{Algorithm} = \textnormal{Logic} + \textnormal{Control}\,.
	\]
	La logique et le contrôle peuvent être vus de façon séparée ;
	pour une même définition logique,
	on peut changer l'efficacité (l'algorithme)
	sans changer le contrôle (ce que calcule l'algorithme).
\end{myprop}

\part{Structures discrètes}
\section{Théorie des graphes et réseaux sociaux}
Un graphe modélise un réseau dans le monde réel.
Il modélise les relations entre une collection d'objets.
En modélisant ce réseau comme un graphe,
il devient possible d'utiliser les raisonnements de la théorie des graphes
pour déduire certaines propriétés des réseaux dans le monde réel.

\subsection{Graphes}
\subsubsection{Définitions de base}
\begin{mydef}[Graphe]
	Un \emph{graphe} $G$ est une structure mathématique constituée
	d'un ensemble de n\oe{}uds, $V$
	et d'un ensemble de liens $E$.
	On le note $G(V, E)$.
\end{mydef}
\begin{mydef}[N\oe{}uds voisins]
	Deux n\oe{}uds dans $V$ sont dits \emph{voisins}
	dans un graphe $G(V, E)$
	lorsqu'ils sont reliés par un lien appartenant à $E$.
\end{mydef}
\begin{mydef}[Graphe dirigé]
	Un graphe peut être \og \emph{dirigé} \fg{}
	ou \og \emph{non dirigé} \fg{}.
	Dans le premier cas, les arêtes ont un sens,
	alors que dans le second elles vont dans les deux sens
	(et n'en ont donc pas).
	Dans le cas des graphes dirigés, les \og liens \fg{} entre les n\oe{}uds
	sont appelés des \og arcs \fg{} alors que pour les graphes non dirigés
	on parle d'\og arêtes \fg{}.
\end{mydef}
\subsubsection{Chemins et connexité}
\begin{mydef}[Chemin]
	Un \emph{chemin} dans un graphe dirigé est une séquence de n\oe{}uds
	telle que chaque paire de n\oe{}uds consécutifs dans la séquence
	est liée par un arc.
	Dans un graphe non dirigé,
	le concept équivalent s'appelle une \emph{chaîne}.
\end{mydef}
\begin{mydef}[Chemin élémentaire]
	Un \emph{chemin élémentaire} est un chemin dont la séquence de n\oe{}uds
	ne contient pas de n\oe{}uds répétés.
	La définition pour les graphes non dirigés est semblable.
\end{mydef}
\begin{mydef}[Cycle]
	Un \emph{cycle élémentaire} est une chaîne
	contenant au moins trois arêtes,
	dont les n\oe{}uds de départ et de fin sont égaux,
	et dont tous les autres n\oe{}uds sont distincts.
\end{mydef}
\begin{mydef}[Graphe connexe]
	Un graphe est dit \emph{connexe} si pour chaque paire de n\oe{}uds
	il existe un chemin liant les deux n\oe{}uds de la paire.
\end{mydef}
\begin{mydef}[Clique]
	Une \emph{clique maximale} dans un graphe est un sous-ensemble
	connexe de n\oe{}uds possédant deux propriétés :
	\begin{enumerate}
		\item chaque n\oe{}ud dans la clique
		est lié à chaque autre n\oe{}ud de celle-ci ;
		\item le sous-ensemble est maximal, c'est-à-dire
		ayant la propriété qu'il est impossible
		de rajouter un n\oe{}ud du graphe à la clique
		en gardant la première propriété.
	\end{enumerate}
	On parle dans le domaine de la modélisation de réseaux
	de \emph{composante}.
\end{mydef}
\begin{mydef}[Composante géante]
	Une \emph{composante géante} est une composante connexe qui contient
	un pourcentage significatif des n\oe{}uds d'un graphe.
	\begin{myprop}[Unicité de la composante géante]
		Pour un réseau venant du monde réel,
		il y a \og toujours \fg{}
		exactement \emph{une} composante géante.
		\begin{proof}
			Supposons qu'il y en ait plus qu'une.
			Il suffirait qu'un seule paire de n\oe{}uds
			devienne liée pour que les deux composantes
			se combinent.
			Il s'agirait donc d'une situation instable.
		\end{proof}
	\end{myprop}
\end{mydef}
\subsubsection{Distance et parcours en largeur}
\begin{mydef}[Longueur d'un chemin]
	La \emph{longueur d'un chemin} est définie comme le nombre de liens
	dans ce chemin.
\end{mydef}
\begin{mydef}[Distance dans un graphe]
	La \emph{distance} entre deux n\oe{}uds d'un graphe est la longueur
	du chemin minimal les joignant.
	Lorsque les n\oe{}uds ne sont pas liés par un chemin,
	la distance est infinie.
\end{mydef}

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{Un graphe $G(V, E)$ et un n\oe{}ud source $s$.}
\KwResult{Les distances de $s$ à tous les autres n\oe{}uds du graphe.}
\Begin{
	On déclare d'abord $s$ à une distance de $0$,
	ainsi que tous les autres n\oe{}uds à une distance infinie\;
	Ensuite, on dit que tous les voisins de $s$ sont à une distance de $1$\;
	Les voisins des voisins (sauf $s$) sont à une distance de $2$\;
	Et ainsi de suite, les voisins de leurs voisins,
	sauf ceux ayant déjà été choisis,
	sont à une distance de $3$\ldots\;
}
\caption{Parcours en largeur (\textsc{bfs})\label{algo:bfs}}
\end{algorithm}

\begin{myprop}[Phénomène du petit monde]
	Dans un réseau venant du monde réel,
	la plupart des distances sont \emph{petites}.
	C'est l'origine du dicton \og \emph{six degrees of separation} \fg{}.
	Il y a deux façons de voir cette propriété :
	\begin{itemize}
		\item les distances entre n\oe{}uds sont petites ;
		\item le réseau est divisé
		en un petit nombre de \og mondes \fg{}.
	\end{itemize}
\end{myprop}

\subsection{Liens forts et faibles}
\subsubsection{Fermeture triadique}
\begin{myprop}
	Lorsque deux n\oe{}uds dans un réseau réel ont un voisin en commun,
	il est probable que ces deux n\oe{}uds deviennent liés également.
	Ce principe s'appelle la \emph{fermeture triadique}.
\end{myprop}
\begin{mydef}[Coefficient de regrouppement]
	Le \emph{coefficient de regrouppement} d'un n\oe{}ud
	est la probabilité que deux n\oe{}uds voisins de celui-ci
	soient voisins entre eux.
	Le coefficient de regrouppement est donc un rationnel entre $0$ et $1$.
\end{mydef}

\subsubsection{Force des liens faibles}
\begin{mydef}[Pont]
	Un \emph{pont} dans un graphe est un lien qui,
	si on l'enlève,
	ferait en sorte que les n\oe{}uds liés par ce lien
	se retrouvent dans des composantes différentes.
	Un \emph{pont local} est un lien qui,
	si on l'enlève,
	amènerait la distance entre les n\oe{}uds
	initialement liés par le lien à un valeur strictement supérieure à deux.
\end{mydef}

Afin de donner un poids aux liens du graphe,
on introduit la notion de \og force \fg{} d'un lien.
On distingue deux types de liens :
les liens forts et les liens faibles.
Dans un réseau de personnes,
un lien fort représenterait par exemple deux personnes amies,
alors qu'un lien faible représenterait deux connaissances.

\begin{myprop}[Fermeture triadique forte]
	Un n\oe{}ud viole la \emph{propriété de fermeture triadique forte}
	lorsqu'il a un lien fort vers deux voisins
	qui ne sont eux-mêmes pas liés, que ce soit par un lien faible ou fort.
	Un n\oe{}ud satisfait la propriété lorsqu'il ne la viole pas.
\end{myprop}
Cette propriété est en général trop forte,
mais elle permet de facilement passer de la réalité
au monde plus abstrait du raisonnement.

\begin{myprop}[Ponts locaux et liens faibles]
	Si un n\oe{}ud $a$ dans un réseau satisfait
	la propriété de fermeture triadique forte,
	alors tout pont local incident à $a$ doit être un lien faible.
	\begin{proof}
		Par contradiction.
		Supposons que $a$ soit incident à au moins deux liens forts,
		et à un pont local également incident au n\oe{}ud $b$,
		qui soit également un lien fort.
		Comme $a$ est incident à un deuxième lien fort,
		il doit y avoir un troisième n\oe{}ud $c$ lié à $a$
		par un lien fort.
		Par la propriété de fermeture triadique forte,
		$b$ et $c$ doivent alors être liés également.
		Or l'arête $ab$ était un pont local,
		ce qui donne lieu à une contradiction.
		Tout pont local incident à un n\oe{}ud
		satisfaisant la propriété de fermeture triadique forte
		doit donc être un lien faible.
	\end{proof}
\end{myprop}

\subsubsection{Force d'un lien
et structure de réseau dans les données à grande échelle}
En pratique, ces définitions de \og force \fg{} et de \og pont local \fg{}
sont trop restrictives ;
au lieu de proposer une définition binaire,
on va définir des nouveaux concepts permettant de quantifier plus facilement
ces quantités :
\begin{itemize}
	\item L'idée de pont local peut être généralisée
	par le concept de \emph{chevauchement des voisinages}
	(\og \emph{neighborhood overlap} \fg{} en anglais).
	On le définit par la formule suivante\footnote{En appelant $N(v)$
	le voisinage ouvert du n\oe{}ud $v$,
	c'est-à-dire l'ensemble des n\oe{}uds adjacents à $v$,
	sans compter $v$ lui-même.} :
	\[
	w(ab) = \frac{\textnormal{nombre de n\oe{}uds adjacents à $a$ et $b$}}{\textnormal{nombre de n\oe{}uds adjacents à $a$ ou $b$}} = \frac{\abs{N(a) \cap N(b)}}{\abs{N(a) \cup N(b)}}\,.
	\]
	On remarque qu'un pont local est un arête $ab$ telle que $w(ab) = 0$.
	\item La force d'un lien est plus difficile à définir précisément,
	mais l'idée générale est qu'il faut trouver une valeur numérique,
	que l'on peut quantifier et
	qui soit pertinente pour l'analyse qu'on veut faire.
\end{itemize}

\end{document}
