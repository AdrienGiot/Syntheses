\documentclass[en]{../../../eplsummary}
\usepackage{listings}

\usetikzlibrary{calc, trees, positioning, arrows, shapes, shapes.multipart, shadows, matrix, decorations.pathreplacing, decorations.pathmorphing}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}

\hypertitle{cloud-INGI2145}{7}{INGI}{2145}
{Houtain Nicolas, Thibault Gérondal,Kabasele Ndonda Nicolas}
{Canini Marco}

\section{Cloud computing}

\subsection{Introduction}

\begin{center}
    \textit{Cloud computing is a model for enabling convenient, 
        on-­demand network access to a shared pool of configurable 
        computing resources (e.g., networks, servers, storage, 
        applications, and services) that can be rapidly provisioned 
        and released with minimal management effort or 
    service provider interaction.}
\end{center}

\begin{tabular}{cm{10cm}}
    \textbf{Cloud commandments} &
    \begin{enumerate}
        \item Partition everything
        \item Use asynchony everywhere
        \item Automate everything
        \item Remember that everything fails
        \item Embrace inconsistency
    \end{enumerate}
\end{tabular}

\begin{tabular}{cm{10cm}}
    \textbf{Cloud benefit} &
    \begin{itemize}
        \item Elastic, just-­in-­time infrastructure
        \item More efficient resource utilization
        \item Pay for what you use
        \item Potential to reduce processing time (parallelization)
        \item Leverage multiple data centers (high availability)
    \end{itemize}
\end{tabular}


\subsubsection{Hardware scalability}

Cloud need for \textbf{scalability} because modern application require
huge amounts of processing and data. Cluster (\textit{room-sized}) and datacenter 
(\textit{building-sized}) can provide the resources needed.
They are composed of \textbf{rack} which is a aggregation of storage devices, many
nodes and switch to connect nodes together. Unfortunately, they are not perfect.
\begin{enumerate}
    \item Difficult to dimension because they must be provisioning for the peak load
    \item Expensive in hardware invest, expertise (ex: special software) and maintenance
    \item Difficult to scale because adding new machines is not easy
\end{enumerate}


\subsubsection{Model}
Cloud computing is a business models where everything is a service:
\begin{itemize}
    \item \textbf{SaaS}: Software as a service, soft. run on distant server 
        instead on running on the user machine.
    \item \textbf{PaaS}: Platform as a service.  Customers handle the application.
    \item \textbf{IaaS}: Infrastructure as a service. Customers handle the 
        middleware and application. Provider handle hardware.
\end{itemize}

\subsubsection{Types}
There also have three types of cloud : 
\begin{itemize}
    \item \textbf{Public}: commercial commercial service open to almost anyone.
    \item \textbf{Community}: shared by several similar organization
    \item \textbf{Private}: shared within a organization
\end{itemize}
In this course we focus on public cloud.

\subsubsection{Applications}

Typically, applications that involve large amounts of computation, storage,
bandwidth Especially when lots of resources are needed quickly or load varies
rapidly.

\begin{itemize}
    \item \textbf{Web app}: Near the edge of the application focus is on vast 
        numbers of clients and rapid response
    \item \textbf{Processing pipelines}: Inside we find data-­intensive services that 
        operate in a pipelined manner, asynchronously
    \item \textbf{Batch processing}: Deep inside the application we see a world of 
        virtual computer clusters that are scheduled to 
        share resources and on which applications like 
        MapReduce (Hadoop) are very popular
\end{itemize}


\subsubsection{Virtualization}
It is used to simulate multiple physical machine for the consumer with different
capabilities. It's powerful for security and isolation because VM cannot influence
other and it provides high flexibility. In the other hand, performance is hard 
to predict because other VM run on the same physical machine.

\subsubsection{Challenge}
\begin{tabular}{m{0.5\linewidth}m{0.5\linewidth}}
    \begin{itemize}
        \item Availability
        \item Data lock-in (moving data)
        \item Data confidentiality and auditability 
        \item Data transfer bottlenecks
        \item Performance unpredictability for VM
    \end{itemize}
    &
    \begin{itemize}
        \item Scalable storage
        \item Bugs in large distributed systems
        \item Scaling quickly
        \item Reputation fate sharing
        \item Software licensing
    \end{itemize}
\end{tabular}

\section{Summary of Eventually Consistent}

Data inconsistency in large-scale reliable distributed systems has to be tolerated for two reasons: improving read and write performance under highly concurrent conditions; and handling partition cases where a majority model would render part of the system unavailable even though the nodes are up and running.

Whether or not inconsistencies are acceptable depends on the client application. In all cases the developer needs to be aware that consistency guarantees are provided by the storage systems and need to be taken into account when developing applications.

\begin{description}
    \item[Strong consistency] After the update completes, any subsequent access (by A, B, or C) will return the updated value.

    \item[Weak consistency] The system does not guarantee that subsequent accesses will return the updated value. A number of conditions need to be met before the value will be returned. The period between the update and the moment when it is guaranteed that any observer will always see the updated value is dubbed the inconsistency window.

    \item[Eventual consistency] This is a specific form of weak consistency; the storage system guarantees that if no new updates are made to the object, eventually all accesses will return the last updated value. If no failures occur, the maximum size of the inconsistency window can be determined based on factors such as communication delays, the load on the system, and the number of replicas involved in the replication scheme. The most popular system that implements eventual consistency is DNS (Domain Name System). Updates to a name are distributed according to a configured pattern and in combination with time-controlled caches; eventually, all clients will see the update.

    \item[The eventual consistency has a number of variations that are important to consider:]

    \item[Causal consistency] If process A has communicated to process B that it has updated a data item, a subsequent access by process B will return the updated value, and a write is guaranteed to supersede the earlier write. Access by process C that has no causal relationship to process A is subject to the normal eventual consistency rules.

    \item[Read-your-writes consistency] This is an important model where process A, after it has updated a data item, always accesses the updated value and will never see an older value. This is a special case of the causal consistency model.

    \item[Session consistency] This is a practical version of the previous model, where a process accesses the storage system in the context of a session. As long as the session exists, the system guarantees read-your-writes consistency. If the session terminates because of a certain failure scenario, a new session needs to be created and the guarantees do not overlap the sessions.

    \item[Monotonic read consistency] If a process has seen a particular value for the object, any subsequent accesses will never return any previous values.

    \item[Monotonic write consistency] In this case the system guarantees to serialize the writes by the same process. Systems that do not guarantee this level of consistency are notoriously hard to program.

\end{description}

A number of these properties can be combined. For example, one can get monotonic reads combined with session-level consistency. From a practical point of view these two properties (monotonic reads and read-your-writes) are most desirable in an eventual consistency system, but not always required. These two properties make it simpler for developers to build applications, while allowing the storage system to relax consistency and provide high availability.


\section{Design for scale}

A system is scalable if it can easily adapt to 
increased (or reduced) demand. Scalablity
is usually limited by some sort of bottleneck.

\subsection{Parallelism programming}

\subsubsection{Vocabulary}
\begin{description}
    \item[Parallelism]: refers to techniques to make programs faster by
        performing several computations in parallel.

    \item[Concurrency]: is the composition of independently executing
        computations.

\end{description}

\subsubsection{Parallelization}
\begin{itemize}
    \item \textbf{Amdahl's law}:

        \begin{tabular}{m{0.5\linewidth}m{0.5\linewidth}}
            $$\quad S = \frac{1}{\alpha +
            \frac{1-\alpha}{N}}$$ where $\alpha$ is the sequential part and $N$ the
            number of cores.
            &
            \includegraphics[width=6cm]{img/amdahl.png}
        \end{tabular}

        The coarse-grain (low rate communication) opposed to fine-grain (high rate) 
        parallelism is more efficient because it limits the communication and 
        coordination overheads by allow bigger task.

    \item \textbf{Dependencies}: Some task need to be after other which limits the degree of 
        parallelism. $\rightarrow$ Scheduling problem
\end{itemize}

\subsubsection{Synchronization}
Synchronization is needed when state is updated by different entities
(threads,process,servers,...)
\begin{description}
    \item[Race condition]: Result of the computation depends on the exact
        timing of the two threads of execution,i.e, the order in which instruction 
        are executed.

\end{description}

\subsubsection{Architecture}
\begin{itemize}
    \item Symmetric multiprocessing (SMP): all processors share same memory.

        \begin{tabular}{m{0.7\linewidth}m{0.3\linewidth}}
            \begin{itemize} 
                \item[+] Simplicity and easy to load balance 
                \item[-] Scalability limited and expensive
            \end{itemize}
            &
            \begin{tiny}
                \begin{tikzpicture}
                    \node[draw, rectangle] (1) {CPU};
                    \node[draw, rectangle] (2) [right=of 1] {CPU};
                    \node[draw, rectangle] (3) [right=of 2] {CPU};

                    \node[draw, rectangle] (11) [below=0cm of 1] {Cache};
                    \node[draw, rectangle] (21) [below=0cm of 2] {Cache};
                    \node[draw, rectangle] (31) [below=0cm of 3] {Cache};

                    \node (12) [below=0.3cm of 11] {};
                    \node (22) [below=0.3cm of 21] {};
                    \node (32) [below=0.3cm of 31] {};

                    \node (tmp) [below left= -0.2cm and 0.7cm of 12] {};
                    \node (tmp2) [below right=-0.2cm and 0.7cm of 32] {};

                    \draw (tmp) edge[double, <->] node[below](p) {Memory bus} (tmp2);
                    \draw (12) edge[<->] (11);
                    \draw (22) edge[<->] (21);
                    \draw (32) edge[<->] (31);

                    \node[draw, rectangle] (mem) [below=1.0cm of 21] {Memory};
                    \draw (mem) edge[<->] (p);
                \end{tikzpicture}
            \end{tiny}
        \end{tabular}

    \item Non Uniform memory architecture (NUMA)

        \begin{tabular}{m{0.7\linewidth}m{0.3\linewidth}}
            \begin{itemize} 
                \item[+] Better scalability and faster memory
                \item[-] Complicates programming and scalability limited
            \end{itemize}
            &
            \begin{tiny}
                \begin{tikzpicture}
                    \node[draw, rectangle] (1) {CPU};
                    \node[draw, rectangle] (2) [right=of 1] {CPU};
                    \node[draw, rectangle] (3) [right=of 2] {CPU};

                    \node[draw, rectangle] (11) [below=0cm of 1] {Cache};
                    \node[draw, rectangle] (21) [below=0cm of 2] {Cache};
                    \node[draw, rectangle] (31) [below=0cm of 3] {Cache};

                    \node[draw, rectangle] (12) [below=0.3cm of 11] {Memory};
                    \node[draw, rectangle] (22) [below=0.3cm of 21] {Memory};
                    \node[draw, rectangle] (32) [below=0.3cm of 31] {Memory};

                    \node (13) [below=0.3cm of 12] {};
                    \node (23) [below=0.3cm of 22] {};
                    \node (33) [below=0.3cm of 32] {};

                    \node (tmp) [below left= -0.2cm and 0.7cm of 13] {};
                    \node (tmp2) [below right=-0.2cm and 0.7cm of 33] {};

                    \draw (tmp) edge[double, <->] node[below](p) {Memory bus} (tmp2);
                    \draw (12) edge[<->] (11);
                    \draw (22) edge[<->] (21);
                    \draw (32) edge[<->] (31);

                    \draw (12) edge[<->] (13);
                    \draw (22) edge[<->] (23);
                    \draw (32) edge[<->] (33);
                \end{tikzpicture}
            \end{tiny}
        \end{tabular}

    \item Shared Nothing

        \begin{tabular}{m{0.7\linewidth}m{0.3\linewidth}}
            \begin{itemize} 
                \item[+] Nice scalability
                \item[-] Requires different programming model
            \end{itemize}
            &

            \begin{tiny}
                \begin{tikzpicture}
                    \node[draw, rectangle] (1) {CPU};
                    \node[draw, rectangle] (2) [right=of 1] {CPU};
                    \node[draw, rectangle] (3) [right=of 2] {CPU};

                    \node[draw, rectangle] (11) [below=0cm of 1] {Cache};
                    \node[draw, rectangle] (21) [below=0cm of 2] {Cache};
                    \node[draw, rectangle] (31) [below=0cm of 3] {Cache};

                    \node[draw, rectangle] (12) [below=0.3cm of 11] {Memory};
                    \node[draw, rectangle] (22) [below=0.3cm of 21] {Memory};
                    \node[draw, rectangle] (32) [below=0.3cm of 31] {Memory};

                    \node[draw, rectangle] (14) [below=0cm of 12] {Net card};
                    \node[draw, rectangle] (24) [below=0cm of 22] {Net card};
                    \node[draw, rectangle] (34) [below=0cm of 32] {Net card};

                    \node (13) [below=0.3cm of 14] {};
                    \node (23) [below=0.3cm of 24] {};
                    \node (33) [below=0.3cm of 34] {};

                    \node (tmp) [below left= -0.2cm and 0.7cm of 13] {};
                    \node (tmp2) [below right=-0.2cm and 0.7cm of 33] {};

                    \draw (tmp) edge[double, <->] node[below](p) {Network} (tmp2);
                    \draw (12) edge[<->] (11);
                    \draw (22) edge[<->] (21);
                    \draw (32) edge[<->] (31);

                    \draw (13) edge[<->] (14);
                    \draw (23) edge[<->] (24);
                    \draw (33) edge[<->] (34);
                \end{tikzpicture}
            \end{tiny}
        \end{tabular}

\end{itemize}


\subsection{Distributed programming}

\subsubsection{Vocabulary}
\begin{description}
    \item[Faults]: Some component is not working correctly
    \item[Failure]: System as a whole is not working correctly
\end{description}

\subsubsection{Wide-area network}
The transfert speed for some data are defined by some attributs:
\begin{enumerate}
    \item Propagation delay
    \item Bottlenecks capacity on the path
    \item Queueing delay, loss, reordering, congestion, rtt 
        (take in account by TCP)
\end{enumerate}

$\rightarrow$ wide-area networks complicates the communication and faults are
more common

\subsubsection{Faults}

Fault are a common event in distributed system and some faults
are correlated.

\begin{itemize}
    \item \textbf{Crash faults}: node simply stop
    \item \textbf{Rational behavior}: owner manipulates node to increase profit
        (ex: lies on the routes to have traffic through it's own AS)
    \item \textbf{Byzantine faults}: faulty node could do anything (ex: stop, send spam,
        attack other, tell lies,...)
\end{itemize}

To prevent fault, we can \textit{prevent} them by using verification,
\textit{detect} them or \textit{mask} them by using replicas. The problem of
using \textbf{replicas} is to be able to maintain consistency between them!

\subsubsection{Consistency}

\begin{itemize}
    \item \textbf{Strong consistency}: After update completes, all subsequent
        accesses will return the updated value

    \item \textbf{Weak consistency}: After update completes, accesses do not
        necessarily return the updated value;; some condition must be satisfied
        first (such as update needs to reach all the replicas)

    \item \textbf{Eventual consistency}: Specific form of weak consistency: If no
        more updates are made to an object, then eventually all reads will return
        the latest value

    \item \textbf{Causal consistency}: If client A has communicated to client B that it
        has updated a data item, a subsequent access by B will return the updated
        value, and a write is guaranteed to supersede the earlier write. Client C
        that has no causal relationship to client A is subject to the normal
        eventual consistency rules

    \item \textbf{Read-your-writes consistency}: Client A, after it has updated a data
        item, always accesses the updated value and will never see an older
        value.

    \item \textbf{Session consistency}: Like previous case but in the context of a
        session, for as long as the sessions remains alive.

    \item \textbf{Monotonic read consistency}: If client A has has seen a particular value
        for the object, any subsequent accesses will never return any previous
        values

    \item \textbf{Monotonic write consistency}: In this case the system guarantees to
        serialize the writes by the same process
\end{itemize}

We can combine them, and monotonic reads + read-your-write are most desirable
than eventual consistency.

\paragraph{Storage system consistency: example}

We have N nodes that can store data.
To write a value: Pick W replicas and write the value to each, using a fresh timestamp
To read a value:
\begin{itemize}
    \item Pick R replicas and read the value from each
    \item Return the value with the highest timestamp
    \item If any replicas had a lower timestamp, send them the newer value
\end{itemize}

Strong consistency :
\begin{description}
    \item[Majority quorum] Always write to and read from a majority of nodes. At least one node knows the most recent value. tolerate up to ⌈N/2⌉ - 1 crashes. But have to read/write ⌊N/2⌋ + 1 values.
    \item[Read/write quorums] Read R nodes, write W nodes, s.t. R + W > N. Adjust performance of reads/writes. But availability can suffer.

        \begin{tabular}{m{5cm}m{12cm}}
            \includegraphics[width=3cm]{img/replicas} &
            \includegraphics[width=7cm]{img/replicas2}
            \begin{itemize}
                \item[$\rightarrow$] Strong consistency
            \end{itemize} \\
        \end{tabular}
    \item[Consensus solutions] Paxos (for crash faults), PBFT (for Byzantine faults). Idea : Correct replicas ``outvote'' faulty ones.
\end{description}

The cap theorem : We can get at most two out of the three
\begin{itemize}
    \item Consistency : All clients single up-to-data copy of the data, even in the presence of concurrent updates
    \item Availability: Every request (including updates) received by a non-failing node in the system must result in a response, even when faults occur
    \item Partition-tolerance: Consistency and availability hold even when the network partitions
\end{itemize}

Dealing with network partitions when a partition is cut : Enter an explicit partition mode that can limit some operations.


\includegraphics[width=\linewidth]{img/part}




\paragraph{Consensus}
\begin{enumerate}
    \item Clients send requests to each of the replicas
    \item Replicas coordinate and each return a result
    \item Client chooses one of the results, e.g., the one that is 
        returned by the largest number of replicas
    \item If a small fraction of the replicas returns the wrong result, or 
        no result at all, they are 'outvoted' by the other replicas
\end{enumerate}


\subsubsection{CAP theorem}
\begin{center}
    \begin{tikzpicture}
        \node (A) {\textcolor{red}{A}};
        \node (C) [below left= 2.5cm and 1.5cm of A] {\textcolor{red}{C}};
        \node (P) [below right= 2.5cm and 1.5cm of A] {\textcolor{red}{P}};

        \node[right =1cm of A, text width=4cm] {\scriptsize \textbf{Availability}: Each client can always read
        and write despite node failures};

        \node[left =1cm of C, text width=4cm] {\scriptsize \textbf{Consistency}: all clients always have the
        same view of the data at the same time};

        \node[right =1cm of P, text width=4cm] {\scriptsize \textbf{Partition-tolerance}: the
        system continues to operate despite arbitrary message loss};

        \draw (A) edge[-] node[left] {C\&A} (C);
        \draw (A) edge[-] node[right] {A\&P \tiny (Many replicas + relaxed consistency)} (P);
    \draw (C) edge[-] node[below] {\begin{tabular}{c}C\&P\\ \tiny (Many replicas + consensus) \end{tabular}} (P);
    \end{tikzpicture}
\end{center}


\paragraph{Actually}, we have a lot of partition and we have a trade-off
between C and A. \textbf{ACID} (atomicity, consistency, isolation, durability)
become \textbf{BASE} (basically available, soft-state, eventually consistent).


\subsection{Structure}
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}

        \node[draw, rectangle, minimum height= 2cm] (L) {\rotatebox{90}{Load balancer}};
        \node[draw, rectangle, fill=black!40, minimum height= 2cm, text width=1cm, right= of L] (W) {};
        \node[draw, rectangle, fill= white, minimum height= 0.5cm, text width=1cm, below=-1cm of W] (W1) {};
        \node[above=0cm of W] (1) {Web server};

        \node[draw, rectangle, fill=black!40, minimum height= 2cm, text width=1cm, right= of W] (A) {};
        \node[draw, rectangle, fill=white, minimum height= 0.5cm, text width=1cm, below=-1cm of A] (A1) {};
        \node[above=0cm of A] (1) {App server};

        \node[draw, rectangle, minimum height= 2cm, right= of A] (C) {\rotatebox{90}{Caching}};
        \node[draw, rectangle, minimum height= 2cm, right= of C] (D) {Data store};

        \draw[->] (-2, 0) -- (L);
        \draw[->] (-2, -1) -- (L);
        \draw[->] (-2, 1) -- (L);

        \node[left=2cm of L] (R) {\rotatebox{90}{Request}};

    \end{tikzpicture}
    \caption{Cloud structure with 
        caching which is central to responsiveness. Cached data are used by the inner
    services to shield them form online load}
\end{figure}

\begin{itemize}
    \item \textbf{Stateless server}: Views a client request as an independent 
        transaction and responds to it


        Easy to scale and more robust because instance failure does not require
        overheas restoring state

    \item \textbf{Statefull server}: Scaling is challenging because we need to maintains 
        the state.

        \paragraph{Traditionnal approach is replication}:

        \begin{tabular}{m{12cm}m{5cm}}
            \begin{itemize}
                \item Data is written to a \textit{master server} and then
                    replicated to one or more \textit{slave servers}
                    (synchronously or asynchronously)
                \item Read operations can be handled by the slaves
            \end{itemize}
            But in this case, master becomes the write bottleneck and single point of failure.
            &
            \centering
            \begin{tikzpicture}
                \node[draw, rectangle] (M) {Master};
                \node[draw, rectangle, below=1cm of M] (S) {Slave};
                \node (I) [right= of M] {};
                \node (O) [right= of S] {};

                \draw[->, double] (I) edge node[above] {in/out} (M);
                \draw[<-, double] (I) edge (M);
                \draw[<-, double] (O) edge node[below] {out}(S);
                \draw[->, double] (M) edge (S);
            \end{tikzpicture}
        \end{tabular}


        \paragraph{Sharding with partitionning}
        The idea is to split data between multiple 
        machines and have a way to make sure that
        the data is available on the right place.
        Typically, define a \textbf{sharding key} and create a \textbf{shard mapping}.

        \begin{tabular}{cm{10cm}}
            This allow to have &
            \begin{itemize}
                \item a really high availability
                \item increase read and write throughput
                \item the possibility 
                    of doing more work in parallel within the application server.
                \item[But] the challenge is to find a good partitionning scheme.
            \end{itemize}
        \end{tabular}

        \textbf{Sharding} is not only for partitioning data 
        within a database, but can also be use to partition data across caching 
        servers (memcached, redis).
\end{itemize}

\paragraph{First-tier parallelism}
Parallelism is vital for fast interactive services, and 
parallel actions must focus on the \textbf{critical path}. This is the
contributor delay for the response delay and do
not care about asynchronous that are shortly.

\begin{center}
    \includegraphics[width=9cm]{img/critical}
\end{center}


Note that update for replicas are typically done in asynchronous way, so we
might not experience much delay on the critical path.

$\rightarrow$ Asynchronous operations decouple systems and 
enable quicker responses at the expense strong 
consistency.
Indeed, this can rise some issues which result in inconsistency:
\begin{itemize}
    \item We don't know in which order replicas applying the update
    \item The leader can crash before replicas change and lose some
        update
\end{itemize}


\section{Cloud storage}

\begin{center}
    \includegraphics[width=12cm]{img/storage}
\end{center}

Many cloud services have a similar structure, 
But the actual storage service is very simple (Read/write 'blocks',
similar to a giant hard disk) and the translation done by the web service.

\paragraph{Ideal store}
\begin{itemize}
    \item Perfect durability – nothing would ever disappear in a crash
    \item 100\% availability – we could always get to the service
    \item Zero latency from anywhere on earth – no delays!
    \item Minimal bandwidth utilization – we only send across the network what we absolutely need
    \item Isolation under concurrent updates – make sure data stays consistent
\end{itemize}

BUT the “cloud” exists over a physical network (communication take time + limited bandwith)
and The “cloud” has imperfect hardware (failures)

\paragraph{Observation}
\begin{itemize}
    \item Read-­only (or read-­mostly) data is easiest to support
    \item Granularity matters: “Few large-­object” tasks generally 
        tolerate longer latencies than “many small-­object” tasks


        But it’s much more expensive to replicate or to update!

    \item[$\rightarrow$] Maybe it makes sense to develop separate solutions for large 
        read-­mostly objects vs. small read-­write objects!
        Different requirements $\rightarrow$ different technical solutions
\end{itemize}


\subsection{Key-value stores (KVS)}
The key-­value store (KVS) is a simple 
abstraction for managing persistent state where data is 
organized as (key,value) pairs with three basic operation:
\begin{enumerate}
    \item \texttt{PUT(key, value)}
    \item \texttt{value = GET(key)}
    \item \texttt{DEL(key)}
\end{enumerate}

\paragraph{Concurrency control}
Most systems use locks on individual items.



\subsubsection{Amazon dynamo}
\begin{figure}[!h]
    \centering
    \includegraphics[width=5cm]{img/AWSplat}
    \caption{Amazon dynamo. 
        AWS is \textbf{Availability\&Partition-tolerance} and follow 
    \textbf{BASE} philosophy.}
\end{figure}

\begin{tabular}{cm{10cm}}
    Dynamo is :&
    \begin{itemize}
        \item High performance with low latency
        \item High scalable
        \item High available key storage, especially for writes
        \item Partition/fault-tolerant
        \item Eventually consistent (sacrify for high availability)
    \end{itemize}
\end{tabular}
Very low latency writes, reconciliation in reads (last write wins). Object stored are 
relatively small (typically <1MB)

\paragraph{Key Techniques}
\begin{itemize}
    \item \textbf{Consistent hashing}: dynamically partitions a set of keys
        over a set of storage nodes (hash keys and give 
        a key of m-bits). One key range per virtual node.

        \paragraph{Data partitioning and load balancing}
        A key is assigned to the closest successor node id 
        (key $k$ is assigned to the first node with $id \geq k$)

        \includegraphics[width=5cm]{img/consistent}

        \begin{center}
            Each node has at most $(1+\epsilon) \frac{K}{N}$ keys with $N$ nodes, $K$ keys
            and $\epsilon=O(log(N))$ 
            %TODO: why epsilon=0 with virtual nodes?
        \end{center}

        \paragraph{Replication}
        Replication is done on the successors node because if the node fail, it's the successor
        which will take failed node keys.

        \begin{center}
            \includegraphics[width=5cm]{img/dynamoReplica}
        \end{center}

        Replication is may in asynchronous way (eventual consistency), and
        key data versioning technique is done with vector clocks.


        If the replica A in the preference list is down then
        another replica is created on a new node.
        In this case, coordinator will involve D that substitutes
        A until A comes back again. When D gets info A is
        back up it hands back the data to A


    \item \textbf{Vector clocks}: Each write to a key k is associated with a 
        vector clock VC(k) which is an array (map) of integers (one entry VC(k)[i] for each node i).

        This is use for tracking causal dependencies among different versions 
        of the same key (data)

        \begin{center}
            \includegraphics[width=5cm]{img/vectorClock}
        \end{center}

    \item \textbf{Sloppy quorums}: In order to enforce consistency, to response
        to a read or write operation the node must asl to a \textit{quorum}
        which is the minimum number of votes that a distributed transaction has
        to obtain.

        \begin{itemize}
            \item $R$ number of nodes that participate in a get, $W$ number
                of node that participate in a write and $R+W > N$

            \item \textbf{Put}: Generate new $VC$, write locally, send value $VC$
                to N selected nodes from preference list and wait for $W-1$

            \item \textbf{Get}: Send read to $N$ selected nodes from preference list, 
                wait for $R$, select highest versions per $VC$ and return
                all such versions. 

                Writeback merge versions.

            \item[Note]
                \begin{enumerate}
                    \item Note that each node has routing info to all other node
                        to reduce latency (but this is less scalable)
                \end{enumerate}
                To handle puts and gets, 3 differents ways:
                \begin{itemize}
				\item \textbf{Extended preference list} N nodes from preference list +
				some additional nodes (following the circle) to account for failure.
				\item \textbf{Failure-free case} Node from preference list are involved
				in get/put
				\item \textbf{Failures} First N alive nodes from extended preference list
				are involved.
			\end{itemize}
        \end{itemize}

        \textbf{Sloppy} allow availability under a much wider range of 
        partitions (failures) but sacrifice consistency. The
        N selected nodes are the first N healthy 
        nodes.

    \item \textbf{Anti-entropy protocol using hash/merkle tree}:
        Each Dynamo node keeps a Merkle tree for 
        each of its key ranges. Hash trees can be used to verify any kind of data stored.

        \begin{center}
            \includegraphics[width=5cm]{img/merkle}
        \end{center}

        Compares the root of the tree with replicas
        \begin{itemize}
            \item If equal, all keys in a range are equal (replicas in sync)
            \item If not equal 
                Traverse the branches of the tree to pinpoint the children that differ
                The process continues to all leaves
                Synchronize on those keys that differ
        \end{itemize}

    \item \textbf{Gossip-based group membership protocol}: 


        Membership info are also eventually 
        consistent — propagated by background 
        gossip protocol.
        \begin{itemize}
            \item Node contacts a random node every 1s
            \item 2 nodes reconcile the membership info and partitioning/placement
                metadata
        \end{itemize}


        \paragraph{Unreliable failure detection (FD)}
        Used to refresh the healthy node info in the extended 
        preference list. In the presence of steady rate of client request
        \begin{enumerate}
        	\item Node A may consider node B failed if node B
        		  does not respond to A's message
		\item Node A then uses an alternates nodes to services 
		requests that maps to B partitions
		\item A periodically checks on B
        \end{enumerate}
        If there s no client requests to drive traffic between two nodes,
        nodes are unaware whether the other has crashed or note
        \footnote{http://www.allthingsdistributed.com/2007/10/amazons\_dynamo.html}.
\end{itemize}


\section{MapReduce}

\begin{figure}[!h]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node[draw, rectangle] (I0) {Input split 0};
        \node[draw, rectangle, below=0cm of I0] (I1) {Input split 1};
        \node[draw, rectangle, below=0cm of I1] (I2) {Input Split 2};

        \node[draw, circle, right=of I0 ] (W2) {Worker};
        \node[draw, circle, below=of W2 ] (W1) {Worker};
        \node[draw, circle, above=of W2 ] (W3) {Worker};

        \node[draw, minimum height=1cm,rectangle, right=of W1 ] (F1) {};
        \node[draw, minimum height=1cm, rectangle, right=of W2 ] (F2) {};
        \node[draw, minimum height=1cm, rectangle, right=of W3 ] (F3) {};

        \node[draw, rectangle, minimum height=1cm, right=0cm of F1 ] (FF1) {};
        \node[draw, rectangle, minimum height=1cm,right=0cm of F2 ] (FF2) {};
        \node[draw, rectangle, minimum height=1cm, right=0cm of F3 ] (FF3) {};

        \node[draw, circle, above right=1cm and 2.5cm of FF1 ] (R1) {Reducer};
        \node[draw, circle, below right=1cm and 2.5cm of FF3 ] (R2) {Reducer};

        \node[draw, rectangle, right=of R1 ] (O1) {Output file 0};
        \node[draw, rectangle, right=of R2 ] (O2) {Ouput file 1};

        \draw (I0.0) edge[->](W1);
        \draw (I1.0) edge[ ->](W3);
        \draw (I2.0) edge[ ->](W2);

        \draw (W1.0) edge[->] (F1.180);
        \draw (W2.0) edge[->] node 
        {\rotatebox{90}{ \textcolor{red}{\textbf{map}: (key, val) $\rightarrow$ (red\_key, val)}}} (F2.180);
        \draw (W3.0) edge[->] (F3.180);

        \draw (FF1.0) edge[->] (R1.180);
        \draw (FF1.0) edge[->] (R2.180);
        \draw (FF2.0) edge[->] node[below=-2.5cm]
        {\rotatebox{90}{ \textcolor{red}{\textbf{shuffled}: redistribution by output's key}}} (R2.180);
        \draw (FF2.0) edge[->] (R1.180);
        \draw (FF3.0) edge[->] (R1.180);
        \draw (FF3.0) edge[->] (R2.180);

        \draw (R1.0) edge[->] node[above=-2cm]
        {\rotatebox{90}{ \textcolor{red}{\textbf{reduce}: (red\_key, \{set of values\}) $\rightarrow$ result}}} (O1.180); 
        \draw (R2.0) edge[->] (O2.180);

        \node[below=1cm of W1] (2) {Map phase};
        \node[left=of 2] (1) {Input files};
        \node[right=0.8cm of 2] (3){Intermediate files};
        \node[right=0.7cm of 3] (4){Reduce phase};
        \node[right=1cm of 4] (5){Output files};


    \end{tikzpicture}

    \centering
    \begin{itemize}
        \item \textbf{File system} distributed across all nodes with replication
        \item \textbf{Driver program} on the master to keeping all node busy
        \item \textbf{Runtime system} which control nodes
    \end{itemize}
    \caption{MapReduce where mapper and reducer should be \textbf{stateless}
    }
\end{figure}


A variety of different tasks can be expressed 
as a single-­pass MapReduce program which are specifically designed
for \textbf{batch operation} over large amounts of data:
\begin{itemize}
    \item filter, collect, aggregate, join on shared element
\end{itemize}

\paragraph{Not for MapReduce}
\begin{itemize}
    \item sorting don't work 
    \item algorithms that depend on shared global state during 
        processing are difficult to implement.
    \item Process live data at high throughput and low latency

\end{itemize}


\subsection{Failure}
On worker crash we rely on the file system being shared 
across all the nodes. 
\begin{itemize}
    \item If the node wrote its output and then crashed, 
        the file system is likely to have a copy of the complete output.
    \item If the node crashed before finishing its output, the master 
        see that the job isn’t making progress, and restarts the 
        job elsewhere on the system
\end{itemize}

\subsection{Optimization}

\begin{enumerate}
    \item \textit{locality}: Master tries to do work on nodes that 
        have replicas of the data
    \item \textit{Stragglers}: re-execute slow machines task somewhere else.

    \item \textit{Combiner}: use between mapper and reducer in order to be more efficient.
        Typically, it's job is to pass $(xyz, k)$ instead of $k$ copies of $(xyz, 1)$.
\end{enumerate}

\subsection{Single-Pass algorithm}

\begin{itemize}
	%% Should not the Intersection check the size of the values?
	\item \textbf{Aggreation} Compute 
	\item \textbf{Filtering} Remove item that does not sastifies a property
	\item \textbf{Intersection} Return the intersection bewteen two set (
	remove duplicated value that appears in the two set).
	\item \textbf{Join} Combine two set according to a common properties
\end{itemize}

\subsection{Shuffle}

\begin{itemize}
    \item \textbf{Sorting on key}
        Runtime guarantees that reduce keys will be 
        presented to reduce in sorted order. Shuffle really
        consists of two parts: (1) Partition and (2) Sort.

    \item \textbf{Sorting on value} To sort by value, a composite key containing 
    the value to sort must be used. In that case, the key comparator must take
    into account that the key is composed. 
\end{itemize}

\subsection{Iterative}
\begin{tikzpicture}
    \node[rectangle, draw] (IM) {Map};
    \node[rectangle, draw, right= of IM] (IR) {Reduce};

    \node[rectangle, draw, above right= of IR] (CM) {Map};
    \node[rectangle, draw, right= of CM] (MR) {Reduce};

    \node[rectangle, draw, right= of MR] (TM) {Map};
    \node[rectangle, draw, right= of TM] (TR) {Reduce};

    \node[rectangle, draw, below right= of TR] (OM) {Map};
    \node[rectangle, draw, right= of OM] (OR) {Reduce};

    \draw (IM) edge[->] node[below=1cm, text width=2cm] {Init state} (IR);
    \draw (CM) edge[->] node[above=0.5cm, text width=2cm] {Iterative} (MR);
    \draw (TM) edge[->] node[above=0.5cm, text width=2cm] {Test} (TR);
    \draw (OM) edge[->] node[below=1cm, text width=2cm] {Output result} (OR);

    \draw (IR) edge[->, bend left] (CM);
    \draw (MR) edge[->, ] (TM);
    \draw (TR) edge[->, bend left] (OM);
\end{tikzpicture}

Iterative MapReduce can be done if the reduce
output is compatible with map input but this require to passing the
entire state and doing a lot of network and disk I/O.


\subsection{Graphs algorithm}
$G = (V, E)$ where $V$ is vertices, $E$ edges of the form $(v_1, v_2, cost, attr)$
where $v_1, v_2 \in V$.

\begin{itemize}
    \item \textbf{Single Source Shortst Path (SSSP)}: based on dijkstra algorithm idea but parallelized.
        \begin{description}
            \item[Init]: for each node $id$, $<\infty, -, \{<succId, cost>\}>$
            \item[Map]: for each node $id$, $<dst, next, \{<succId, cost>\}>$:
                \begin{itemize}
                    \item emit $id$, $<dst, next, \{<succId, cost>\}>$
                    \item for each successor:
                        \begin{itemize}
                            \item emit $succId, <dst+cost, id>$ 
                        \end{itemize}
                \end{itemize}
            \item[Reduce]: emit $id$, $<minDst, nextWithMinDst, \{<succId, cost>\}>$
        \end{description}

        This algorithm is based on a \textit{wave} which go on at each iteration.

    \item \textbf{k-clustering}: the idea is to assign $k$ random centroid and move them 
        until be stable.
        \begin{description}
            \item[Init]: choose random point
            \item[Map]: Assign each point to the closest centroid
                $$S_i^{(t)} = \{ x_j : x_j − m_i^{(t)} ≤ x_j − m_i^{(* t )} , i* = 1,..., k \}$$
            \item[Reduce]: Recenter with $m_i$ the new centroid for its points
                $$m_i^{(t+1)} = \frac{1}{|S_i^{(t)}|} \sum_{x_j \in S_i^{(t)}} x_j$$
        \end{description}

    \item \textbf{Classification with naïve Bayes}: where it's \textit{naïve} because probability
        of events are independent.
        $$\textrm{Probability messages "XYZ" is SPAM?} = \frac{p(spam) p(containsXYZ | spam)}
        {p(containsXYZ)}$$ 

        \begin{itemize}
            \item \texttt{p(spam)} : Nbr spam email / Nbr email
            \item \texttt{p(containsXYZ)} : Nbr emails with XYZ / Nbr emails
            \item \texttt{p(containsXYZ|spam)} : Nbr emails with XYZ / Nbr emails with XYZ
        \end{itemize}

        \begin{tabular}{m{2cm}cm{13cm}}
            Nbr spam with XYZ &:
            & 
            \begin{description}
                \item[map]: for each message $m <class, \{words\}>$, emit $<word, class> \rightarrow 1$
                \item[reduce]: emit $<word, class> \rightarrow count$
            \end{description}
        \end{tabular}

        \begin{tabular}{m{2cm}cm{12cm}}
            Nbr email with XYZ&:
            & 
            \begin{description}
                \item[map]: for each message $m <class, \{words\}>$, emit $<word> \rightarrow 1$
                \item[reduce]: emit $<word> \rightarrow count$
            \end{description}
        \end{tabular}

    \item \textbf{PageRank}: The idea is to allow $\frac{1}{N}$ vote par page
        at the initialization, and each page vote for all the page it has a
        link to. To ensure fairness, pages voting for more than one page must
        split their vote equally between them. Voting proceeds in rounds: in
        each round, each page has the number of votes it received in the
        previous round.


        \begin{itemize}
            \item \textsc{Random surfer model}: Imagine a random surfer, who starts on a 
                random page and, in each step
                \begin{enumerate}
                    \item Click on a random link on the page with probability $d$ 
                    \item Jump to a random page with probability $1-d$ 
                \end{enumerate}
                The PageRank of a page can be interpreted 
                as the fraction of steps the surfer spends on 
                the corresponding page


            \item \textsc{Naïve PageRank}
                $$rank_i = \sum_{j \in B_i} \frac{1}{N_j} rank_j \quad \textrm{where }N_i \textrm{ is
                the outgoing link of i and} B_i \textrm{ the ingoing link of i}$$ 

                This can't be able to manage vertex which have no outgoing edge.

                \begin{tabular}{m{7cm}m{7cm}}
                    \textbf{Sinks} & \textbf{Hogs}\\
                    \begin{tikzpicture}
                        \node[rectangle, draw] (G) {Google};
                        \node[rectangle, draw, below= of G] (A) {Amazon};
                        \node[rectangle, draw, right= of A] (Y) {Yahoo};

                        \draw[->] (G) -| (Y);
                        \draw[->] (G.300) -- (A.60);
                        \draw[<-] (G.240) -- (A.120);
                        \draw[->] (A) -- (Y.180);
                        \draw[->] (A) -- (Y.180);
                    \end{tikzpicture}
                    &
                    \begin{tikzpicture}
                        \node[rectangle, draw] (G) {Google};
                        \node[rectangle, draw, below= of G] (A) {Amazon};
                        \node[rectangle, draw, right= of A] (Y) {Yahoo};

                        \draw[->] (G) -| (Y);
                        \draw[->] (G.300) -- (A.60);
                        \draw[<-] (G.240) -- (A.120);
                        \draw[->] (A) -- (Y.180);
                        \draw[->] (A) -- (Y.180);
                        \draw[->, loop right] (Y) edge[loop right] (Y);
                    \end{tikzpicture}\\
                    PageRank is lost after each round and $\forall_i rank_i = 0$ & 
                    PageRank is accumulates on Yahoo and $\forall_i rank_i = 0$ behalf 
                    $rank_{yahoo} = 1$
                \end{tabular}

            \item \textsc{Improved PageRank}
                $$rank_i = 1-d + d \sum_{j \in B_i} \frac{1}{N_j} rank_j \quad \textrm{where }N_i \textrm{ is
                the outgoing link of i and} B_i \textrm{ the ingoing link of i}$$ 
        \end{itemize}

        \begin{description}
            \item[Init]: page $p <1/Nn, \{outgoingLink\}>$
            \item[Map]: page $p$ propagate $\frac{1}{N_p} * d * weigth_p$
            \item[Reduce]: page $p$ = $1-d + \sum_{incomingWeight}$
        \end{description}

\end{itemize}


\section{Hadoop}

\subsection{HDFS}
Hadoop use a distributed file system (HDFS) specialized for particular
types of applications. Files are stored as sets of blocks (64MB) which
are replicated for durability and availability.

\begin{center}
    \includegraphics[width=11cm]{img/hdfs}
\end{center}

\begin{itemize}
    \item Namespace is managed by a single name node and transfert
        is done directly between client and data node
    \item State stored in two files:

        \begin{tabular}{m{7cm}m{9cm}}
            \begin{itemize}
                \item fsimages: snapshot of file system metadata
                \item edits: change since last snapshot
            \end{itemize} 
            &
            \includegraphics[width=9cm]{img/namenode}
        \end{tabular}
\end{itemize}
If the state of the namenode is lost, there are two possible solutions:
\begin{itemize}
	\item \textbf{Metadata backups} Namenode can write its metadata to a local disk
	and/or to a remote NFS
	\item \textbf{Secondary Namenode} (1) Periodically merge the edit log with fsimage 
	to prevent from growing to large, (2) Copy of the metadata
\end{itemize}
\paragraph{Note} Secondary namenode is more of a checkpoint
\footnote{http://blog.madhukaraphatak.com/secondary-namenode---what-it-really-do/} 
than a backup node, if it fails
$\to$ data loss.



\begin{table}[!h]
    \begin{tabular}{m{8cm}m{8cm}}
        \textbf{Does well} & \textbf{Does not well} \\
        \begin{itemize}
            \item very large read-only or append-only file (Terabytes EASY)
            \item Sequential access parten
            \item High throughput and high capacity
        \end{itemize} 
        &
        \begin{itemize}
            \item Low-latency access
            \item Multiple writers
            \item Not append-only file
            \item Storing small files
        \end{itemize} \\
        & $\rightarrow $ This is well done by Network File System (NFS)
    \end{tabular}
    \caption{HDFS advantage}
\end{table}


\subsection{Hive Query Language (HQL)}
A data warehouse infrastructure built on top 
of Hadoop for providing data summarization, 
query and analysis.

\subsubsection{SQL recap}
\begin{itemize}
    \item \texttt{SELECT}: Projection and remapping/renaming 
    \item \texttt{JOIN}: Cartesian product 
    \item \texttt{WHERE}: Filtering 
    \item \texttt{UNON, INTERSECT}: Set operations 
    \item \texttt{GROUP BY, MIN, MAX, AVG}: Aggregation 
    \item \texttt{ORDER BY}: Sorting 
    \item \texttt{SELECT .. FROM (SELECT .. FROM ..)}: Composition 
\end{itemize}

\subsubsection{HQL}

\begin{itemize}
    \item Suitable for processing structured data
    \item Create a table structure on top of HDFS
    \item Queries are compiled in to MapReduce jobs
\end{itemize}
\paragraph{Note} Hive is not designed fot OnLine Transaction Processing which 
consist mostly of updates. Indeed updating record or transaction are not 
supported

\subsection{Pig and Pig latin}
Relational data model does no allow nested table (attribute which is collection).
That's a part of what Pig Latin brings.

Somewhere between a programming language and a DBMSa which allows
distributed  programming with explicit parallel dataflow operators.

\begin{description}
    \item[Pig]: runtime system
    \item[Pig latin]:
        A dataflow language that compiles to MapReduce.
        Collection-­valued   expressions  whose  results  get  
        assigned  to  variables.
        \begin{itemize}
            \item A  program  does  a  series  of  assignments  in  a  dataflow
            \item It  gets  compiled  down  to  a  sequence  of  MapReduces. 

                Note that Pig Latin has its own query langage (not SQL)
        \end{itemize}
\end{description}

\subsubsection{Basic SQL-like operation}
Operations are \textbf{explicitly} specified.
\begin{itemize}
    \item \texttt{load\ldots as}: Projection and renaming 
    \item \texttt{foreach \ldots generate}: remapping 
    \item \texttt{filter by}: Filtering 
    \item \texttt{join}: intersection 
    \item \texttt{group by}: Aggregation 
    \item \texttt{order}: Sorting 
    \item \texttt{store}: save result
\end{itemize}

\subsubsection{Implementation}
\begin{center}
    \includegraphics[width=10cm]{img/pig}
\end{center}

\subsubsection{Work-sharing techniques}

\begin{center}
    \includegraphics[width=10cm]{img/work}
\end{center}


\section{Batch processing}
\subsection{Pregel: Bulk Synchonous Parallel}
Much  of  the  mismatch  stems  from  the  lack  of  shared  global  state.
Complex  applications  and  interactive  queries  both  need  one  thing  that
MapReduce lacks: efficient  primitives  for  data  sharing.


With \textbf{Pregel}, we consider the  nodes  with a  state  (memory)  
that  carries  from  superstep to  superstep.

\subsubsection{Model}

It's a sequence of \textbf{superstep} where at superstep $S$:
\begin{enumerate}
    \item Compute in parallel at each $V$
        \begin{itemize}
            \item Read message sent to $V$ in $S-1$
            \item Update value/state
            \item Optionnaly change topology
        \end{itemize}
    \item Send messages to neighbors
    \item Synchronization
\end{enumerate}

\begin{tabular}{m{9cm}m{7cm}}
    $\rightarrow $ Algorithm terminates when all vertices are simultaneously inactive.
    & \includegraphics[width=7cm]{img/halt}
\end{tabular}

\subsubsection{Algorithm}
\begin{itemize}
    \item \textbf{Find Maximum Value}

        \begin{tabular}{m{11cm}m{5cm}}
            \begin{enumerate}
                \item At superstep 0, propagating value to neighbors
                \item In each step, if he learn a larger number, then propagate
                    it to neighbors else vote to halt
                \item Finish when all vote halt
            \end{enumerate}
            & \includegraphics[width=5cm]{img/FMV}
        \end{tabular}

    \item \textbf{PageRank}
        $$rank_i = 1-d + d \sum_{j \in B_i} \frac{1}{N_j} rank_j \quad \textrm{where }N_i \textrm{ is
        the outgoing link of i and} B_i \textrm{ the ingoing link of i}$$ 

        \begin{itemize}
            \item Each  page  $j$  distributes  its  importance  to  all  of  the  pages  it  
                points  to  (so  we  scale  by  $\frac{1}{N_j}$)
            \item Page  p’s  importance  is  increased  by  the  importance  of  its  
                back  set
        \end{itemize}
        %TODO: example pregel code ?

\end{itemize}


\subsection{Spark: Resilient Distributed Dataset}

Basically, Spark is a distributed  memory  abstraction  that  is  both  fault-­tolerant  
and  efficient. Note that there is some restriction form of distributed shared memory
in order to be called \textit{Resilient Distributed Datasets (RDDs)}
\begin{itemize}
    \item Immutable, partitioned collections of records
    \item Can  only  be  built  through  coarse-­grained deterministic  
        transformations  (map,  filter,  join,  ...)
\end{itemize}

Despite their restrictions,  RDDs can express many parallel algorithms (These
naturally  apply the same operation to many items). RDDs also unify many
programming models.

\begin{tabular}{m{7cm}cm{7cm}}
    \begin{itemize}
        \item MapReduce,  DryadLINQ
        \item Pregel graph  processing
        \item Iterative  MapReduce
        \item SQL:  Hive  on  Spark  (Shark)
    \end{itemize}
    & $\Rightarrow$  &
    All are based on coarse-­grained  
    operations
\end{tabular}

\begin{center}
    \includegraphics[width=11cm]{img/databaseComp}
\end{center}

\subsubsection{Efficient fault recovery}
Spark use \textbf{lineage}:

\begin{tabular}{m{9cm}m{7cm}}
    \begin{itemize}
        \item Log  one  operation  to  apply  to  many  elements
        \item Recompute lost  partitions  on  failure
        \item No  cost  if  nothing  fails
    \end{itemize}
    &
    \includegraphics[width=7cm]{img/sparkFault}
\end{tabular}

\subsubsection{Spark operation}
\begin{itemize}
    \item Transformations (define a new RDD) : \begin{tabular}{ccc}
            map & flatMap & union \\ join & cogroup & cross \\ mapValues & filter & sample\\
            groupByKey & reduceByKey& sortByKey\\
        \end{tabular}

    \item Action (return a result to driver program) : \begin{tabular}{ccc}
            collect& reduce& count\\ save& lookupKey& take\\
        \end{tabular}
\end{itemize}

\subsubsection{Spark scheduler}

\begin{tabular}{m{7cm}m{8cm}}
    \begin{itemize}
        \item Pipelines  functions within  a  stage
        \item Locality  \&  data  reuse aware
        \item Partitioning-­aware to  avoid  shuffles
    \end{itemize}
    &
    \includegraphics[width=8cm]{img/sparkScheduler}
\end{tabular}

\subsubsection{Algorithm}
\begin{itemize}
    \item \textbf{PageRank}:
        $$rank_i = 1-d + d \sum_{j \in B_i} \frac{1}{N_j} rank_j \quad \textrm{where }N_i \textrm{ is
        the outgoing link of i and} B_i \textrm{ the ingoing link of i}$$ 

        \begin{lstlisting}[mathescape]
        var links   =  //  RDD  of  (url,   neighbors)   pairs
        var ranks   =  //  RDD  of  (url,   rank)   pairs
        for  (i $\leftarrow$ 1 to ITERATIONS)   {
            contribs =  links.join(ranks).flatMap {
                (url,   (links,   rank))   =>
                links.map(dest =>  (dest,   rank/links.size))
            }
            ranks   =  contribs.reduceByKey((x,   y)  =>  x  +  y)
            .mapValues(sum   =>  0.85*sum   +  0.15/N)
        }
        \end{lstlisting}
        Note that as RDDs are  immutable, contribs and  ranks  are  new  RDDs!

        %TODO optimizing placement slide 20 for 07
\end{itemize}


\section{Stream processing}
Many important applications must process large streams of live data
and provide results in near-­real-­time.
Distributed  stream  processing  framework  is  required  to  scale  large
clusters  (100s  of  machines) and achieve  low  latency  (few  seconds).


\begin{tabular}{m{10cm}m{6cm}}
    Streaming  systems  have  a  record-at-a-time processing  model
    \begin{itemize}
        \item Each node has mutable state
        \item For  each  record,  update  state   and  
            send  new  records
    \end{itemize}
    &
    \includegraphics[width=5cm]{img/stream}
\end{tabular}

Note that state  is  lost  if  node  dies, so make a stateful stream
processing  be  fault-­tolerant  is  challenging.

\subsection{Storm}

Framework  for  distributed  stream  processing which provides:  
(1) stream  Partitioning, (2) fault  tolerance  and (3)  parallel  execution.
\begin{center}
    \includegraphics[width=10cm]{img/storm}
\end{center}

\begin{itemize}
    \item \textbf{Spout} is the source of stream where \textbf{bolt} process the stream
        and output new streams
    \item Nimbus  node  (master  node): 
        \begin{enumerate}
            \item Distributes  code,  launches  workers  across  the  cluster
            \item Monitors  computation  and  reallocates  workers  as  needed
        \end{enumerate}
    \item ZooKeeper nodes (coordinate  the  cluster)
    \item Supervisor  nodes: Start  and  stop  workers  according  to  signals
        from  Nimbus
\end{itemize}

\subsubsection{Fault tolerance}
Storm  can  guarantee  that  every  tuple  will  be  process  at  least  once or
at  most  once,  but  not  exactly  once. (Exactly  once  guarantee  requires
a  durable  data  source  that  can  replay  any  message  or  set  of
messages  given  the  necessary  selection  criteria)
%Suppose you want to update a counter in a database, if a worker fails, you 
% don't know if it has failed before updating the value or after 
%TODO really see slide 29 for 07 why this


If  a  supervisor  node  fails,  Nimbus  reassigns  that  node’s  task  to
other  nodes  in  the  cluster. Any  tuples  sent  to  a  failed  node  will
time  out  and  be  replayed. (Delivery  guarantee  dependent  on  a  reliable
data  source)

\subsection{Spark streaming}
Run  a  streaming  computation  as  a  series  of  
very  small,  deterministic  batch  jobs. This 
combine  the  efficiency  of  in-­memory  
distributed   processing  of  Spark  with  stream  
processing  mode.


\subsubsection{Work}

\begin{tabular}{m{10cm}m{6cm}}
    \begin{itemize}
        \item Chop  up  the  live  stream  into  batches  of  X  seconds  
        \item Spark  treats  each  batch  of  data  as  RDDs  and  processes  them
            using  RDD  operations
        \item Finally,  the  processed  results  of  the  RDD  operations  are
            returned  in  batches
    \end{itemize}
    &
    \includegraphics[width=4cm]{img/sparkStream}
\end{tabular}

At the end, a new RDDs are created for every batch

\subsubsection{Fault tolerance}

\begin{tabular}{m{10cm}m{6cm}}
    \begin{itemize}
        \item RDDs  remember  the  operations  that  created  them.
        \item Batches  of  input  data  are  replicated  in  memory  for
            fault-­tolerance.
        \item Data  lost  due  to  worker  failure,  can  be  recomputed  from
            replicated  input  data.
    \end{itemize}
    &
    \includegraphics[width=6cm]{img/faultSpark}
\end{tabular}


\section{Data Center Galaxy}

\subsection{Infrastructure}

\begin{tabular}{m{8cm}m{7cm}}
    Streaming  systems  have  a  record-at-a-time processing  model
    \begin{itemize}
        \item Servers organized in racks with a Top Of Rack (ToR) switch
        \item Aggregation switch interconnect ToR switch
    \end{itemize}
    &
    \includegraphics[width=7cm]{img/dcStruct}
\end{tabular}

\subsection{Traffic}

\paragraph{Internal traffic}

Internal traffic (East-west) is clearly a critical component compared
to external traffic (North-south).
\begin{center}
    \begin{tabular}{m{7cm}m{7cm}}

        \includegraphics[width=7cm]{img/internalCrit}
        &
        \includegraphics[width=7cm]{img/internalTraffic}
    \end{tabular}
\end{center}

\subsubsection{Solutions}

\begin{tabular}{m{12cm}m{4cm}}
    The goal is to allow each server to talk to any other server at its full access
    link rate.
    We can see the DC as a giant switch between server.
    &
    \includegraphics[width=3cm]{img/giant}
\end{tabular}

\begin{itemize}
    \item \textbf{Full Bisection Bandwith }: it's the \textit{scale up } approach.

        \begin{tabular}{m{11cm}m{7cm}}
            A traditional tree topology is really expensive
            because it requires a lot of switch with high bandwidth.
            &
            \includegraphics[width=3cm]{img/FBBW}
        \end{tabular}

        \paragraph{Oversubscription}
        A solution is to provision less
        than full Bisection Bandwidth because we can reasonably say
        that all server will know talk with all server simultaneously. 


    \item \textbf{Fat Tree}: it's the \textit{scale out} approach.

        \begin{tabular}{m{11cm}m{7cm}}
            Fat tree offers high bisection bandwidth but
            the system must be to exploit this available capacity:
            \begin{itemize}
                \item Routing must use all path
                \item Transport protocol must fill into all pipe
            \end{itemize}

            Note that this topology only require 10Gb/s links.
            &
            \includegraphics[width=3cm]{img/fattree}
        \end{tabular}
\end{itemize}

\subsection{Network stack}


\paragraph{Models}
Traditional \textbf{TCP} enforce \textit{per-flow min/max fairness}, but
Data center operators want to enforce other models such
as tenant-based or deadline-based.

\paragraph{Low latency}
\begin{itemize}
    \item 55\% of flow $<100KB$ $\Rightarrow$ Delay-sensitive
        So low latency is \textbf{critical}!
    \item 5\% of flow $>10MB$ $\Rightarrow$ Throughput-sensitive
\end{itemize}

\begin{center}
    \begin{tabular}{cc}
        High throughput & Low latency\\
        \hline
        Deep queue at switches& Shallow queues at switches \\
        which increase delays & which
        is bad for burst and throughput\\
    \end{tabular}
\end{center}

The goal is to have in DC low queue occupancy (to achieve
low RRTs within DC approache $1µs$) and high throughput.


\subsubsection{Solutions}
\begin{itemize}
    \item \textbf{DC-TCP} which use \textbf{Explicit Congestion Notification (ECN)} 
        \begin{enumerate}
            \item React early and quickly by using ECN which avoid large
                buildup in queue and allow low latency!

                \paragraph{Switch} If queue length > $k$ set ECN bit

            \item React in proportion to the extend of congestion, not
                in presence.

                \paragraph{Sender} Maintain running average of \textbf{fraction
                of packets marked} ($\alpha$)
                $$ alpha = (1-g) \alpha + g \frac{\# marked ACK}{\# ACK} $$
                and adapts the window based on $\alpha$
                $$ W = (1-\frac{\alpha}{2}) W$$
        \end{enumerate}

    \item \textbf{pFabric}: use priorities
        \begin{enumerate}
            \item Packets carry a single priority number.

                priority = remaining flow size (e.g., \#bytes un-acked)

            \item Switches have small queues (10 packets), send highest
                priority and drop lowest priority

            \item Server (re)transmit aggressively it means at full link rate
                and drop transmission rate only under extreme loss (timeout).
        \end{enumerate}
\end{itemize}

\paragraph{Flow Completion time (FCT)}: is
the time from when flow started at the sender, to when all
packets in the flow were received at the receiver.
\paragraph{Pfabric} With DCTCP, small flow are delayed by big ones. To
handel that:
\begin{itemize}
	\item Packets carry a single priority number representing the remaining flow size.
	\item Switches send the packet with highest priority and drop the lowest
	priority packet
	\item Servers transmit/retransmit at full link rate and decrease transmission
	rate only under extreme loss (typically timeouts)
\end{itemize}
assigning priority number to packet. 
\begin{center}
    \includegraphics[width=8cm]{img/fct}
\end{center}

\subsection{Management}

\begin{description}
    \item[Data plane]: 
        \begin{tabular}{m{8cm}m{3cm}}
            process packet by using local forwarding state and packet header.
            &
            \includegraphics[width=2cm]{img/app}
        \end{tabular}
    \item[Control plane]: Compute forwarding state.

        Actually, we have a protocol which solve
        \begin{itemize}
            \item Consistent with particular low-level hardware/software
            \item  Based on entire network topology
            \item  For all routers/switches (i.e., must configure each one)
        \end{itemize}

        But it's too much, and \textbf{SDN} allow to perform some abstraction.
\end{description}

\subsubsection{Software Defined Network (SDN)}
\begin{center}
    \includegraphics[width=8cm]{img/SDN}
\end{center}

\subsubsection{Open Flow}
is a protocol for remotely controlling the forwarding table of a switch or
router which is one element of SDN.

\begin{tabular}{m{8cm}m{8cm}}
    \begin{itemize}
        \item Message between controller and switches 
            (Synchronous: Stats, Flow-mods, Asynchronous: Packet-in)
        \item Abstract hardware details
        \item  Allows direct control over forwarding table
    \end{itemize}
    &
    \includegraphics[width=9cm]{img/openflow}
\end{tabular}

\begin{table}[ht!]
	\centering
	\begin{tabular}{|c|l|}
	\hline
	Match field & To match against packet. These consist of the ingress port 
	and packet headers\\
	\hline
	Priority & Matching precedence of the flow entry\\
	\hline
	Counters & e.g. packet and byte counters\\
	\hline
	Instructions & Determine action set or pipeline processing\\
	\hline
	Timeouts & Maximum amount of time or idle time before flow is expired by
	the switch\\
	\hline
	Cookies & Opaque data value chosen by the controller. Not used when
	processing packets.\\
	\hline
	\end{tabular}
	\caption{Flow table entries}
\end{table}

\begin{table}[ht!]
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
	\hline
	Switch&VLAN&VLAN&MAC&MAC&Eth&IP&IP&IP&IP&L4&L4\\
	Port&ID&pcp&src&dst&type&src&dst&Tos&Prot&sport&dport\\
	\hline
\end{tabular}
\caption{match fields}
\end{table}
\begin{table}[ht!]
	\centering
	\begin{tabular}{|c|c|l|}
	\hline
	Message&Direction&Description\\
	\hline
	Packet-In & Switch$\to$Controller & Transfer the control of the packet to the 
	controller.\\
	&& Packet-in events can be configured to buffer packets.\\
	\hline
	Packet-Out & Controller $\to$ Switch & Instruct switch to send a packet out 
	of a specified port.\\
	&& Send in response to Packet-in messages.\\
	\hline
	Modify-State & Controller$\to$ Switch & Add,delete and modify flow/group entries
	in the flow tables\\
	&& and to set switch port properties.\\
	\hline
	Flow-Removed & Switch $\to$ Controller & Inform the controller about the removal
	of a flow entry \\
	&&from a flow table.\\
	\hline
	\end{tabular}
	\caption{messages exchanged in openFlow}
\end{table}

%TODO improvement from 89 for 08


\section{Web programming}

\begin{itemize}
    \item \textbf{HTML}: Separate content and style by using CSS.
    \item \textbf{Browser}: Show information and not the part of the file 
        which say how to show information.

    \item \textbf{Delivering document} 
        \begin{enumerate}
            \item Peer to peer model
            \item Client/server model where a 
                server is a machine that offer services to other machine.

                Note that state must be maintain when request are related. 
                \begin{itemize}
                    \item Client side: It's better to keep state on client side to avoid DoS.
                    \item Server: If state is on the server and multiple server are used,
                    they must be able to exchange state information.
                \end{itemize}

                \paragraph{Concurrency}
                \begin{itemize}
                    \item Thread pools: keep a thread pool with a fixed number of 
                        worker threads
                    \item Event-driven: server's work is driven by events
                        which need a single thread.
                \end{itemize}

                Event-driven servers typically have better performance
                and do not need as much synchronization but
                thread-based are easier to write and maintain.

                \paragraph{Multiple machine}
                Need a proxy to load balance request across machines.

        \end{enumerate}

    \item \textbf{Uniform  Resource  Identifier (URI)} which comme in two form:
        \begin{enumerate}
            \item Uniform  Resource  Name (URNs): \textit{What} to find
            \item Uniform  Resource  Locator (URLs): \textit{Where} to find
        \end{enumerate}

    \item \textbf{DNS}: Hierarchical namespace

        \begin{tabular}{m{5cm}m{11cm}}
            Authoritative  server  knows,  for  each  name  in  its  zone,  which
            machine  corresponds  to  a  given  name,  or  which  other  name  server
            is  responsible.
            &
            \includegraphics[width=11cm]{img/dns}
        \end{tabular}

    \item \textbf{HTTP}: is a simple stateless protocol
        which use request/response that contain headers used to
        exchange additional information.

        \begin{enumerate}
            \item GET: retreive document
            \item HEAD: Get metadata for a specific document
            \item POST: Add information to a document (as form)
            \item PUT, DELETE
        \end{enumerate}

    \item \textbf{Dynamic content}:

        \paragraph{Document Object Model} (DOM)
        \includegraphics[width=11cm]{img/dom}

        \paragraph{XMLHttpRequest}
        It's a JavaScript  object  that  enables  web  pages  
        to  dynamically and asynchronously  load  more  content. 

        \paragraph{Event handlers}
        Events  can  be  requested  from  the  web  page
        or  directly  from  JavaScript.

        \begin{itemize}
            \item Server-side
                \begin{itemize}
                    \item Web app
                        \begin{enumerate}
                            \item Common Gateway Interface (CGI). When
                                dynamic content is requested:
                                (1) the web server prepare metadata and user-submitted
                                data, (2) runs an external program that produce the web page and
                                (3) return the web page produce by program to the client
                            \item Java servlets: servlets implement a specific
                                method that is given request. They also run one
                                \textbf{servlet container} and each request is its own
                                thread.
                        \end{enumerate}

                        \begin{center}
                            \begin{tabular}{l|cc}
                                & CGI & Servlets \\
                                \hline
                                Request handled by & Processes & Threads \\
                                Copies of code & Potentially many & One \\
                                Session state stored & File System & Servelt container \\
                                Security & Problematic & Handled by Java sandbox \\
                                Portability & Varies & Java \\
                            \end{tabular}
                        \end{center}

                    \item Node.js which is a event-driven programming model, so there
                        is a single thread which must never block. 

                        Note that Express are minimal and flexible framework for writing
                        web app in node.js and EJS allow to write \textit{page templates}.

                \end{itemize}


            \item Client-side
                \begin{itemize}
                    \item JavaScript: Can be embed entirely in HTML or attach in
                        a separate file.

                    \item Ajax: Asynchronous JS, XML,...
                        \begin{description}
                            \item[+] Much more responsive than plain HTML
                            \item[-] Difficult to integrate navigation element (back button),
                                to accommodate search engine, some compatibility issues
                        \end{description}
                        \begin{center}
                            \includegraphics[width=9cm]{img/ajax}
                        \end{center}
                \end{itemize}
        \end{itemize}

    \item \textbf{Session management}
        \begin{itemize}
            \item Use session ID in part of every URL:
                \begin{enumerate}
                    \item URL rewriting by append session ID
                        $$<a  href="foo.html">  \rightarrow <a  href="foo.html?sid=012345">$$
                    \item hidden session ID variables in the HTML
                        $$<input  type="hidden"  name="sid"  value="012345">$$
                \end{enumerate}

            \item Use cookies which is a set of key-­value pairs that a web
                site can store in your browser which is send with \texttt{set-cookie header}
                in HTTP response.

                \paragraph{Note}
                Each cookies as an expiration date, a domain and a
                path. Browser  only  sends  the  cookies  whose  path  
                and  domain  match  the  requested  page.

                \paragraph{Issue}
                Cookie persist and can have some useful information such as
                password.
        \end{itemize}
\end{itemize}

\section{Behavioural tracking}

The idea is to make the client load a blank pixel so that it reportes its activity to the server. Motivations are :
\begin{description}
\item[Operational] logging, payment reports
\item[Product] create analytics, recommendation
\item[Insight] analytics, research
\end{description}

\subsection{Production}
Client side implementation. Javascripts that will schedule and plan for HTTP Calls. Will have to handle failure. Send status of the client to the server. The platform matters as mobile can have battery drain issue. So, need for keep alive, scheduling, batching. All of these can be achieve via the device ID and the local storage of the browser.

\subsection{Collection}
The collection is the front door of the pipeline. It is servers that needs high availability (HA). This can be done via round-robin DNS and HA proxy that will redirect the HTTP flow to collectors.

\subsection{Transmission}
All the collectors need to send the collected data to a big queue that will process them. For this task, we need guarantee on delivery, high availibility and queuing behavior.

For the guarantee on delivery, this problem is complex. In an ideal case, you send the message, get an acknowledgment. But as the queue service is distributed, it gets more complicated. If the ack is lost, you're going to send the message a second time. So you will have a de-duplication. But it ensures that you have ``at least once'' the message. To tackle this problem and get ``exactly Once'' the message, you can put a filter behind the queue. But this will cost in performance.

Two solutions proposed by the paper : kafka and RabbitMQ.
\begin{tabular}{l|r}
RabbitMQ & Kafka \\
\hline
\hline
Master/slave & Masterless\\ 
Complex topology & Zookeeper\\
Short lived queues & long lived logs\\
At least once & At least once\\
\end{tabular}

\subsection{Augmentation}
The augmentation is the transformation of the data to produce your analytics of whatever you want to compute. The fan-in is going to split a message to multiple message. The fan-out is going to aggregate multiple message to one.

Results can be then store.

\subsection{Storage}
Storage have to be scalable, replicated, available.

Two solutions proposed by the paper :
\begin{tabular}{l|r}
HDFS & S3 \\
\hline
\hline
self hosted & Managed \\
Cost effective at scale & Cost prohibitive at scale \\
Can run multi tenant & Network to Map Reduce \\
Non trivial operational cost & EMR cost \\
\end{tabular}

\subsection{Query}
Database, low vs high latency, common vs custom operations.

\begin{tabular}{l|c|r}
Hadoop based & Columnar Store & Key Value \\
\hline
\hline
Pig/hive/Spark & Redshift/Vertica & Cassandra/Riak \\
Shared tenancy & SQL & simple queries \\
HA & Expensive & complex \\
\end{tabular}

\subsection{Example at SoundCloud : Stitch}
Stitch provides counts and time-series of counts in real-time.
It uses webclient (js) -> RoR (ruby on rails) -> RabbitMQ -> Aggregation -> HDFS -> Cassandra

\subsection{Event sourcing}
Share data, not state.
Decouple producers/consumers.
Materialize State with data given.
=> Scale with volume / complexity.

\includegraphics[width=\linewidth]{img/matstate.png}

\subsection{Lambda architecture}
Nathan Marz came up with the term Lambda Architecture (LA) for a generic, scalable and fault-tolerant data processing architecture, based on his experience working on distributed data processing systems at Backtype and Twitter.

The LA aims to satisfy the needs for a robust system that is fault-tolerant, both against hardware failures and human mistakes, being able to serve a wide range of workloads and use cases, and in which low-latency reads and updates are required. The resulting system should be linearly scalable, and it should scale out rather than up.

Here’s how it looks like, from a high-level perspective:

\includegraphics[width=\linewidth]{img/la-arch.png}

\begin{itemize}
\item 1. All data entering the system is dispatched to both the batch layer and the speed layer for processing.
\item 2. The batch layer has two functions: (i) managing the master dataset (an immutable, append-only set of raw data), and (ii) to pre-compute the batch views.
\item 3. The serving layer indexes the batch views so that they can be queried in low-latency, ad-hoc way.
\item 4. The speed layer compensates for the high latency of updates to the serving layer and deals with recent data only.
\item 5. Any incoming query can be answered by merging results from batch views and real-time views.
\end{itemize}

\includegraphics[width=\linewidth]{img/la-2.png}

\subsection{open questions} 
Ad blocker. ID of devices.

\subsection{Cloud Computing}
Commodity Computation, outsourced infrastructure and ability of scaling down and up on demand.


\section{Distributed Systems Coordination}
$$Agreement \Leftrightarrow Consensus \Leftrightarrow Consistency$$

\paragraph{Atomic broadcast}
\begin{itemize}
    \item Total Order property:

        $p, q$ two correct node and $m, n$ two message $\Rightarrow$ if $p$
        delivers $m$ before $n$ then $q$ delivers $m$ before $n$.

        $\rightarrow $ It's a strong consistency.

    \item No creation of message
    \item No duplication of message (delivered once)
\end{itemize}

This could be useful to have replication because all
database replicas apply updates/queries  in  
the  same  order.

\subsection{Zookeeper}
Zookeeper is a set of data node (\textbf{znodes}) which are organized in a
hierarchical namespace that resembles customary file systems. 
This file system is designed to store \textbf{metadata} and not data.
Zookeeper is a \textsc{CP} for the CAP point of view.

\begin{itemize}
    \item There is \textbf{regular} znodes and \textbf{ephemeral} znodes wich
        can be removed  by the  system  when  the  session  that  creates  them
        terminates.
\end{itemize}

\paragraph{Session}
Client connects  to  Zookeeper  and  initiates  a  session
which allow client to move from one server to another. 
Any server can serve client's requests.

If the server fail, the client library tries to  contact  another  server
before  session  expires.

\paragraph{Consensus}
Zookeeper  can  solve  consensus  (agreement)   for  arbitrary  number  
of  clients.

\subsubsection{Global structure}
\begin{tabular}{m{8cm}m{8cm}}
    \begin{itemize}
        \item Fully replicated (no partitionning)
        \item Use commit log and periodic snapshot to recovery
            on crash
        \end{itemize}
    &
    \includegraphics[width=8cm]{img/zookCompo}
\end{tabular}

\subsubsection{Operations}
Only full reads and write operations which can 
be asynchronous (concurrent calls allowed and so multiple requests) or synchronous
(no concurrent calls by a single client).

\begin{tabular}{m{9cm}m{6cm}}
    \begin{itemize}
        \item \textbf{Write}:
            \begin{tabular}{l}
                \texttt{create(znode, data, flags)}\\
                \texttt{setData(znode, data, version)}\\
                \texttt{delete(znode, version)}\\
                \texttt{sync()}
            \end{tabular}

            \paragraph{Linearizable} Write are linearizable.

            \begin{enumerate}
                \item Write request is forwarded by a follower
                    to the leader
                \item Leader use atomic (total-order) broadcast to
                    disseminate messages

                    $\rightarrow$ use ZAB protocol which support  FIFO/causal  
                    consistency  of  asynchronous  calls. 
                    ZAB tolerate $\frac{n-1}{2}$ failures.
            \end{enumerate}

            \paragraph{ZAB} use a internally elects leader for replica 
            (which is adopted by zookeeper)
            
            \paragraph{Request Processor}
            Upon receiving write request, the leader calculate in what state 
            system will after write is applied and transform the operation
            in the transactional update. These transactional update are then 
            processed by the ZAB and the DB.
            %TODO slide 64 for 11

        \item \textbf{Read}:
            \begin{tabular}{l}
                \texttt{exist(znode, watch)}\\
                \texttt{getData(znode, watch)}\\
                \texttt{getChildren(znode, watch)}
            \end{tabular}

            \paragraph{Linearizable}
            Read has \textbf{local read} (A  server  serving  a  read  request  might  not  have  been  a  part  of  a  write  
            quorum  of  some  previous  operation). 
            But if \texttt{sync()} is use before each read, we have a \textbf{linearizable read}.

            Indeed, the linearizable read is show as a \textit{slow read}.

            \paragraph{Caching reads}
    \end{itemize}
    &
    \includegraphics[width=7cm]{img/zook}
\end{tabular}

Leader atomically broadcast updates but read operations is processed
locally.


\subsubsection{Examples}
%TODO slides 51 to 59 for 11

\section{Raft}
Raft is a consensus algorithm for replicated logs which correspond to
replicated the state machine (all servers execute same commands in same order).

\begin{center}
    \includegraphics[width=9cm]{img/raft}
\end{center}

\subsection{Client protocol}
Client send command to leader (or other server that will redirect
to leader) and the leader does not respond until command as been
(1) logged, (2) commited and (3) executed by leader's state machine.

\paragraph{Unique ID} has use to identify a client command and has store in
log. This allow to not execute a command twice if leader crashes after
executing command, but before responding.

\subsection{Overview}

\subsubsection{Consensus}
Consensus is leader-based which is more efficient thant leader-less approach
and allow to simplifies normal operation because there is no conflict.

\subsubsection{Server states}

\begin{itemize}
    \item At any given time, each servier is EITHER :
        \begin{enumerate}
            \item Follower : completely passive 
            \item Candidate : used to elect a new leader
            \item Leader: handles all client interactions, log replication
        \end{enumerate}
    \item Time divided into term (maintains current term value)
\end{itemize}

\begin{center}
    \includegraphics[width=9cm]{img/rleader}
\end{center}

\subsubsection{Terms}
\begin{tabular}{m{8cm}m{6cm}}
    Time divided into term
    \begin{itemize}
        \item at most one leader per term
        \item If no leader on term then failed election
    \end{itemize}
    Term is used to identify obsolete information
    &
    \includegraphics[width=9cm]{img/term}
\end{tabular}

As every RPC contain term of send, \textbf{terms} are used to detect stale
leaders (and candidates) because each election increment the term counter.
\begin{itemize}
    \item If sender’s term is older, RPC is rejected, sender reverts to
        follower and updates its term
    \item if receiver’s term is older, it reverts to follower, updates its term,
        then processes RPC normally
\end{itemize}

\subsubsection{Heartbeats}
\begin{itemize}
    \item Servers start up as followers
    \item Followers expect to receive RPCs from leader or candidates
    \item Leader must send heartbeats (empty
        AppendEntries RPCs) to maintain authority.

        \texttt{ElectionTimeout} without receive RPCs $\rightarrow$ assume that
        leader has crashed.
\end{itemize}


\subsubsection{Leader election}

\begin{enumerate}
    \item Increment current term
    \item Change to candidate state
    \item Vote for self
    \item Send \texttt{Request Vote RPC} (which include
        index and term of last log entry) to all other server
        \begin{itemize}
            \item Receive vote from majority $\Rightarrow$ leader
            \item Receive RPC from valid leader $\Rightarrow$ follower state
            \item Election timeout elapse $\Rightarrow$ new election
        \end{itemize}
\end{enumerate}

Note that the candidate is choose in order to have the leader
with the most complete log.

\begin{itemize}
    \item \textbf{Safety}: allow at most one winner per term because
        each server gives only one vote per term and it's impossible two
        have two different candidates with majorities of vote.

    \item \textbf{Liveness}: some candidate must eventually win.

        $T \leq$ Election timeouts $\leq 2T$, so usually one server
        timeout and wins elections before other wake up.
        (Work well if $T >>$ broadcast time)
\end{itemize}

\subsubsection{Normal operation}
\begin{enumerate}
    \item Client sends command to leader
    \item Leader appends command to its log
    \item Leader sends AppendEntries RPCs to followers
    \item Once new entry committed:
        \begin{enumerate}
            \item  Leader passes command to its state machine, returns result to
                client
            \item  Leader notifies followers of committed entries in subsequent
                AppendEntries RPCs
            \item  Followers pass committed commands to their state machines
        \end{enumerate}
\end{enumerate}

If there is crashed/slow followers,
Leader retries RPCs until they succeed.

$\rightarrow$  Performance is optimal if one successful RPC to any majority of
servers.

\subsection{Log}

\subsubsection{Structure}

\begin{tabular}{m{6cm}m{8cm}}
    Leader take commands from clients and replicates its log to other server.
    \begin{itemize}
        \item Entry \textbf{commited} if know to be stored on majority of servers
    \end{itemize}
    Term is used to identify obsolete information
    &
    \includegraphics[width=11cm]{img/raftLog}
\end{tabular}

\subsubsection{Consistency}
\begin{itemize}
    \item If log entries on different servers have same index
        and term then (1) they store the same command and (2) the logs are identical
        in all preceding entries
    \item If a given entry is committed, all preceding entries are also committed
\end{itemize}

\subsubsection{Safety}
Raft safety property say that if a leader has decided that a log entry is committed, that entry
will be present in the logs of all future leaders.

To guarantee the safety requirement:
\begin{itemize}
    \item Leaders never overwrite entries in their logs
    \item Only entries in the leader’s log can be committed
    \item Entries must be committed before applying to state machine
\end{itemize}

\subsubsection{Append entries}
Each \texttt{AppendEntries RPC} contains index and term of entry preceding new
ones. This entry must match before append the new entry.

\subsubsection{Commitment rules}
If a leader decide to commit an entry:
\begin{enumerate}
    \item this entry must be stored on a majority of server
    \item \textbf{AND} at least one new entry from leader's term must 
        also be stored on majority of servers
\end{enumerate}

\begin{center}
\begin{tabular}{m{7cm}cm{7cm}}
    \includegraphics[width=7cm]{img/commitFault}
    &
    $\Rightarrow$
    &
    \includegraphics[width=6cm]{img/commitRight}
    \\
    Not safe commit for previous term because $s_5$ can
    be the next leader. (Without 2. commitment rules)
    & &
    Safe commit because $s_5$ cannot be the next leader
    (With 2. commitment rules) \\
\end{tabular}
\end{center}


\subsubsection{Repairing follower logs}
Leader must make follower logs consistent with its own
by (1) delete extraneous entries and (2) fill in missing entries.

\begin{tabular}{m{6cm}m{8cm}}
    \begin{itemize}
        \item Leader keep \textbf{nextIndex} for each follower (initialized
            to $1+leaderIndex$)
        \item When AppendEntries consistency check fails, decrement
            nextIndex and try again
        \item Not that when follower overwrites inconsistent entry, it
            deletes all next entries
    \end{itemize}
    &
    \includegraphics[width=11cm]{img/repairing}
\end{tabular}

\subsection{Configuration}
%TODO slide 27-28-29-30 for 12



\section{Paper}


\section{Summary Pregel: A System for Large-Scale Graph Processing}

\subsection{State of art}

Process a large graph can be done via :
\begin{description}
    \item[New custom algorithm] implementation effort that mustbe repeated for each new algorithm
    \item[Distributed computing platform]  ill-suited for graph processing (map reduce)
    \item[Single-computer graph algorithm] doesn't scale
    \item[Parallel graph system] Not fault tolerant
\end{description}

\subsection{Pregel}
Pregel computations consist of a sequence of iterations, called supersteps.


During a superstep the framework invokes a userdefined function for each vertex, conceptually in parallel. 
The function specifies behavior at a single vertex V and a single superstep S.
Messages are typically sent along outgoing edges, but a message may be sent to any vertex whose identifier is known.

\subsection{Pregel model}

A typical Pregel computation consists of input, when the graph is initialized, followed by a sequence of supersteps separated by global synchronization points until the algorithm terminates, and finishing with output.

Algorithm termination is based on every vertex voting to halt. In superstep 0, every vertex is in the active state; all active vertices participate in the computation of any given superstep. A vertex deactivates itself by voting to halt. This means that the vertex has no further work to do unless triggered externally, and the Pregel framework will not execute that vertex in subsequent supersteps unless it receives a message. If reactivated by a message, a vertex must explicitly deactivate itself again. The algorithm as a whole terminates when all vertices are simultaneously inactive and there are no messages in transit.

% TODO : Improve this graph (need edges : message received, vote to halt)
\begin{figure}[!h]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node[draw, circle] (Active) {Active node};
        \node[draw, circle, right=of Active ] (Inactive) {Inactive node};

        \draw (Active) edge[->](Active);
        \draw (Active) edge[ ->](Inactive);
        \draw (Inactive) edge[ ->](Inactive);
        \draw (Inactive) edge[ ->](Active);
    \end{tikzpicture}
    \\
    \includegraphics[width=0.5\linewidth]{pregelprop.png}
\end{figure}

\subsection{Pregel API}
\subsubsection{Message passing}
A vertex can send any number of messages in a superstep. All messages sent to vertex V in superstep S are available. There is no guaranteed order of messages in the iterator, but it is guaranteed that messages will be delivered and that they will not be duplicated.

A vertex could learn the identifier of a non-neighbor from a message received earlier.

\subsubsection{Combiners}
Sending a message, especially to a vertex on another machine, incurs some overhead. This can be reduced in some cases with help from the user.  For example, suppose that Compute() receives integer messages and that only the sum
matters, as opposed to the individual values. In that case the system can combine several messages intended for a vertex V into a single message containing their sum, reducing the number of messages that must be transmitted and buffered.


\subsubsection{Aggregators}
Pregel aggregators are a mechanism for global communication,
monitoring, and data. Each vertex can provide a value
to an aggregator in superstep S, the system combines those
values using a reduction operator, and the resulting value
is made available to all vertices in superstep S + 1.

For instance, a sum
aggregator applied to the out-degree of each vertex yields the
total number of edges in the graph.

\subsubsection{Topology Mutations}
Just as a user’s Compute() function can send messages, it can also
issue requests to add or remove vertices or edges.

Additions follow removals, with vertex
addition before edge addition, and all mutations precede
calls to Compute(). This partial ordering yields deterministic
results for most conflicts. (compute -> remove -> addition)

\subsubsection{Input and output}
Users with unusual needs can write their own
by subclassing the abstract base classes Reader and Writer.

\subsection{Architecture}
\subsubsection{Partition}
The default partitioning function is just hash(ID)
mod N, where N is the number of partitions, but users can
replace it.

\subsubsection{Processing}
\begin{itemize}
    \item Many copies of the user program begin executing on
        a cluster of machines. One of these copies acts as the
        master. It is not assigned any portion of the graph, but
        is responsible for coordinating worker activity. The
        workers use the cluster management system’s name
        service to discover the master’s location, and send registration
        messages to the master.
    \item The master determines how many partitions the graph
        will have, and assigns one or more partitions to each
        worker machine. The number may be controlled by
        the user. Having more than one partition per worker
        allows parallelism among the partitions and better load
        balancing, and will usually improve performance. Each
        worker is responsible for maintaining the state of its
        section of the graph, executing the user’s Compute()
        method on its vertices, and managing messages to and
        from other workers. Each worker is given the complete
        set of assignments for all workers.
    \item The master assigns a portion of the user’s input to
        each worker. The input is treated as a set of records,
        each of which contains an arbitrary number of vertices
        and edges. The division of inputs is orthogonal to the
        partitioning of the graph itself, and is typically based
        on file boundaries. If a worker loads a vertex that belongs
        to that worker’s section of the graph, the appropriate
        data structures (Section 4.3) are immediately
        updated. Otherwise the worker enqueues a message to
        the remote peer that owns the vertex. After the input
        has finished loading, all vertices are marked as active.
    \item The master instructs each worker to perform a superstep.
        The worker loops through its active vertices, using
        one thread for each partition. The worker calls
        Compute() for each active vertex, delivering messages
        that were sent in the previous superstep. Messages are
        sent asynchronously, to enable overlapping of computation
        and communication and batching, but are delivered
        before the end of the superstep. When the worker
        is finished it responds to the master, telling the master
        how many vertices will be active in the next superstep.
        This step is repeated as long as any vertices are active,
        or any messages are in transit
    \item After the computation halts, the master may instruct
        each worker to save its portion of the graph.
\end{itemize}

\subsubsection{Fault tolerance}
Fault tolerance is achieved through checkpointing. At the
beginning of a superstep, the master instructs the workers
to save the state of their partitions to persistent storage,
including vertex values, edge values, and incoming messages;
the master separately saves the aggregator values.

If a worker does not receive a ping message after a specified interval, the worker
process terminates. If the master does not hear back from
a worker, the master marks that worker process as failed.
When one or more workers fail, the current state of the
partitions assigned to these workers is lost. The master reassigns
graph partitions to the currently available set of workers,
and they all reload their partition state from the most
recent available checkpoint at the beginning of a superstep S. 

\section{Summary : Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing}

The paper proposes a new abstraction called resilient
distributed datasets (RDDs) that enables efficient
data reuse in a broad range of applications. RDDs are
fault-tolerant, parallel data structures that let users explicitly
persist intermediate results in memory, control
their partitioning to optimize data placement, and manipulate
them using a rich set of operators.
The main challenge in designing RDDs is defining a
programming interface that can provide fault tolerance
efficiently.

\subsection{RDDs}

Formally, an RDD is a read-only, partitioned collection
of records. RDDs can only be created through deterministic
operations on either (1) data in stable storage or (2)
other RDDs. We call these operations transformations to
differentiate them from other operations on RDDs. Examples
of transformations include map, filter, and join.

RDDs do not need to be materialized at all times. Instead,
an RDD has enough information about how it was
derived from other datasets (its lineage) to compute its
partitions from data in stable storage. This is a powerful
property: in essence, a program cannot reference an
RDD that it cannot reconstruct after a failure.

users can control two other aspects of RDDs:
persistence and partitioning. Users can indicate which
RDDs they will reuse and choose a storage strategy for
them (e.g., in-memory storage). They can also ask that
an RDD’s elements be partitioned across machines based
on a key in each record. This is useful for placement optimizations,
such as ensuring that two datasets that will
be joined together are hash-partitioned in the same way.

\subsection{Spark Programming Interface}

Spark is an implementation of RDDs.

\subsection{Advantages of the RDD Model}

\begin{tabular}{l|p{5cm}|p{5cm}}
\textbf{Aspect} & \textbf{RDDs} & \textbf{Distributed Memory (DSM)} \\
\hline
\hline
Reads & Coarse- or fine-grained & Fine-grained \\ \hline
Writes & Coarse-grained & Fine-grained  \\ \hline
Consistency & Trivial (immutable)  & Up to app / runtime \\ \hline
Fault recovery & Fine-grained and lowoverhead using lineage & Requires checkpoints and program rollback \\ \hline
Straggler mitigation & Possible using backup tasks & Difficult \\ \hline
Work placement & Automatic based on data locality & Up to app (runtimes aim for transparency) \\ \hline
Behavior if not enough RAM & Similar to existing data flow systems & Poor performance (swapping?) \\ \hline
\end{tabular}

\subsection{Applications Not Suitable for RDDs}
RDDs are best suited for batch applications that apply the same operation to
all elements of a dataset. In these cases, RDDs can ef-
ficiently remember each transformation as one step in a
lineage graph and can recover lost partitions without having
to log large amounts of data. RDDs would be less
suitable for applications that make asynchronous finegrained
updates to shared state, such as a storage system
for a web application or an incremental web crawler.

\subsection{Application suitable for RDDs}
Iterative Machine Learning Applications. Page rank with optimized partitioning of the RDDs. In-Memory Analytics. Traffic Modeling. Twitter Spam Classification.


\subsection{Checkpointing}

Although lineage can always be used to recover RDDs
after a failure, such recovery may be time-consuming for
RDDs with long lineage chains. Thus, it can be helpful
to checkpoint some RDDs to stable storage.

\subsection{Memory Management}
Three options : Memory, serealized in memory, hard-drive.

\subsection{Expressing MapReduce with Spark}
MapReduce can be expressed using the
flatMap and groupByKey operations in Spark, or reduceByKey
if there is a combiner.


\end{document}
