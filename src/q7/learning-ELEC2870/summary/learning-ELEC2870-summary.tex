\documentclass[en]{../../../eplsummary}

\hypertitle{Machine Learning : regression dimensionality reduction and data visualization}{7}{ELEC}{2870}
{Beno√Æt Legat}
{Michel Verleysen}

\paragraph{Note} I have started it in 2014 but I have not followed the course finally so I'll finish it in 2015, tell me if you are motivated to help.

\section{Introduction}
\subsection{Blabla}
Preprocessing, feature generation, feature selection, model generation, validation
We only do the 3 last

2 types of algo
regression then classification.
classification is regression with discrete ``height'' value.
Clustering, ...

2 problems
\subsection{Overfitting}
If we have a function about our examples that we want to optimize, we will overfit !

\subsubsection{2 different models}
A solution is to take different parametric models.
We take 2 sets of example,
one to build the model and one to evaluate the model.
We process the model (value of the parameters) for each model and evaluate
them with the other set.

For example, this will enable us to see that we need to have polynomial with not too high degree.

The problem is that since we split our data we have even less data to find the value of the parameters of our model.

\subsubsection{Influence in overfitting}
Overfitting increases with the complexity of the model and decreases with the size of the learning set.

\subsubsection{Large parameters}
Problem: large model parameters that compensate themselves.
Solution: Add $\|w\|^2$ in the function, $\min E(w) + \lambda \|w\|^2$.
This is called regularization.
New question: What is the value of $\lambda$ ?

\subsection{Curse of dimensionality}
We need at least as much examples (data) than dimension.
If we cut each dimension by $x$, you have $x^n$ data samples.
We need at least some data in each region.
It increase expenentially.
So we need feature selection !

All our algorithms are sensitive to this problem, just differently.


\end{document}
