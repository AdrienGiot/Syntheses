\documentclass[en]{../../../../../../eplexam}

\hypertitle{Information theory and coding}{8}{INGI}{2348}{2010}{Juin}
{Mélanie Sedda}
{Benoît Macq, Jérôme Louveaux Jérôme and Pereira Olivier}

\paragraph{Discussion link}
\url{http://www.forum-epl.be/viewtopic.php?t=10737}

\section{BM-1}
\begin{solution}
  \begin{enumerate}
    \item Voir cours
    \item
      \[
        p_X(x) =
        \begin{cases}
          \frac{3}{4D} & \text{ pour } \frac{-D}{3} \leq x \leq \frac{D}{3}\\
          \frac{9}{8D}(1-\frac{x}{D}) & \text{ pour }  \frac{D}{3} \leq x \leq D \\
          \frac{9}{8D} (1+\frac{x}{D}) & \text{ pour }  -D \leq x \leq \frac{-D}{3} \\
          0 & \text{ sinon }
        \end{cases}
      \]

      Pour le signal quantifié, on a
      \begin{align*}
        \tilde{p}(0) & = \frac{3}{8}\\
        \tilde{p}(-q) & = \tilde{p}(q) = \frac{71}{256} \\
        \tilde{p}(-2q) & =\tilde{p}(-2q) = \frac{9}{256}.
      \end{align*}

      $H$ vaut donc \(-\sum_{i=-2}^2 \tilde{p}(iq) \log \tilde{p}(iq)  = 0.872\).

      On peut dire que la moyenne de l'erreur est nulle car les parties gauches et droites s'annulent. On sépare l'intégrale en 5 termes, disons  \( I_{-2}, I_{-1}, I_0, I_1, I_2 \). A nouveau par symétrie, \( I_{-2} = I_2 \) et \( I_{-1}  = I_1 \). Je trouve (j'estime la probabilité que je ne me sois pas trompée quelque part à 0...)
      \(I_0 = \frac{D^2}{2^7}\\
      I_1 = \frac{2749D^2}{2^{14}3^3}\\
      I_2 = \frac{511D^2}{2^{14}3^2}
      \)
      Donc \(\sigma_{\epsilon}^2 = \frac{4963D^2}{2^{13}3^3}\).

      Pour \(\sigma_{X}^2\), je trouve \(\frac{37D^2}{54}\).

    \item
      Pour obtenir plus de points, il suffit juste de considérer d'autres valeurs de $q$ et de recommencer tous les calculs.
  \end{enumerate}
\end{solution}

\section{BM-2}
\begin{solution}
  \begin{enumerate}
    \item  c $\to$ 0 a $\to$ 10 b $\to$ 11
      (on aurait aussi pu échanger les codes de a et b ces lettres ont la même proba)
      On trouve que \(R = 1.1\), \(H = 0.569\) et donc \(\eta = 0.517\) ce qui est très loin de 1 et donc pas très optimal. C'est à cause du fait qu'on a une lettre qui est beaucoup plus probable que les autres.

    \item  Pour résoudre le problème on pourrait encoder par bloc et plus une seule lettre à la fois. Avec deux lettres par exemple on attend déjà \(\eta = 0.782\). Plus on choisit un long bloc, plus l'efficacité se rapprochera de 1 (mais ca à ses inconvénients aussi en terme de delay).
  \end{enumerate}
\end{solution}

\section{JL-1}
\begin{solution}
  \begin{enumerate}
    \item
      \[
        G =
        \begin{pmatrix}
          1 & 1 & 1 & 1 & 1\\
          1  & \alpha  & \alpha ^2 & \alpha^3  & \alpha^4\\
          1  & \alpha^2 & \alpha^4 & \alpha^6  & \alpha^8
        \end{pmatrix}
      \]

    \item \(d(\mathcal{C}) = n-k+1 = 3 \rightarrow t(\mathcal{C}) = 1\) donc on est capables de corriger un élément dans le Galois field
    \item Je dirais que ces éléments-là ne peuvent faire partie d'un même coset. Si ils l'étaient, ca voudrait dire que leur somme est un codeword. Or leur somme c'est un vecteur avec seulement 2 éléments non nuls. On peut alors utiliser la même idée que dans la preuve de la singleton bound : on prend \(c_\text{left}\) c'est la partie avec tous les 0, de taille 3, et \(c_\text{right}\) la partie avec les 2 éléments non nuls. On sait que les \(c_\text{left}\) de tous les codewords doivent être différents, or ici c'est le même que le \(c_\text{left}\) du codeword nul. Donc on est sûrs qu'ils sont pas un même coset. Ils sont pas dans \(\mathcal{C}\) non plus, par le même argument qu'avant (la partie de droite a un élément non nul et un élément nul). Du coup ils sont tous dans un coset différent et ils sont tous uniques leaders car les éléments qui ont un poids plus petit ou égal au leur ne sont pas dans le même coset.

    \item Oui il y en a d'autres. Il y a \(8^5\) vecteurs de taille \(5\) avec des éléments dans \(F_8\) possibles et il y a \(8^{n-d(\mathcal{C})+1} = 8^3 \) éléments dans chaque coset. Il y a donc 64 cosets différents et avec \(\mathcal{C}\) lui-même et ceux avec exactement un élément non nul ca nous en fait que \(1+7 \cdot 5 = 36\).

    \item \([\alpha^5 \quad 1]
      [1 \quad \alpha]
      [\alpha^6 \quad \alpha^2]\)

    \item On trouve \([\alpha^6 \quad \alpha^2]\) aussi donc il est dans le coset dont le leader est \(a = [0 \quad 0 \quad 0 \quad \alpha^4 \quad 0]\). Il est décodé en \(x = y - a = [0 \quad 1 \quad \alpha^3 \quad \alpha^5 \quad \alpha^2]\).
  \end{enumerate}

\end{solution}

\section{JL-2}
\begin{solution}
  \begin{enumerate}
    \item $C = 4/5$
    \item $R < \frac{C}{H(U)} = 4/5$
    \item Non parce que c'est un fixed length code
    \item $n \geq \frac{\log(1/p_\mathrm{max})}{C-R} = 499$
      Par contre c'est plutôt lower than et pas larger than non? Si $R$ est plus grand, ca fait diminuer le dénominateur donc augmenter la fraction et augmenter $n$...
  \end{enumerate}
\end{solution}

\section{OP-1}
\begin{solution}
  \begin{description}
    \item[Stratégie 1] $n$ copies of the bit $m$
      \begin{itemize}
        \item Bob reçoit bien $n$ fois $m$ donc il y a pas d'erreur. Il peut juste lire le premier bit par exemple. Il a une proba de succès de 1.
        \item Eve n'a qu'à lire un bit qu'elle a bien reçu.
          La proba qu'elle en ait au moins un qui ne soit pas $\epsilon$ est de $1-(\frac{1}{2})^n$. Si elle n'a que des \(\epsilon\), elle peut quand même essayer de deviner mais il n'y a qu'une chance sur 2 que ca marche. La probabilité totale est donc  \(1-(\frac{1}{2})^n + (\frac{1}{2})^n \frac{1}{2} = 1-(\frac{1}{2})^n + (\frac{1}{2})^{n+1} = 1-\frac{1}{2^{n+1}}\).
      \end{itemize}
    \item[Stratégie 2] $n$ bits $m_1, \ldots, m_{n-1}, m_n$
      \begin{itemize}
        \item Bob fait le XOR de tous les bits qu'il y a reçu. Il a toujours une proba 1 de succès comme il y a pas d'erreur sur le channel.
        \item Eve fait la même chose si elle n'a reçu aucun \(\epsilon\). Si elle a reçu un \(\epsilon\), elle ne peut rien faire de mieux que deviner avec \(1/2\) quel bit c'était comme un \(\epsilon\) a la même probabilité de provenir d'un 0 que d'un 1. Sa proba de succès c'est donc \(1\cdot (\frac{1}{2})^n + \frac{1}{2} (1 - (\frac{1}{2})^n)= \frac{1}{2}+\frac{1}{2^{n+1}} \).
      \end{itemize}
  \end{description}
  La deuxième stratégie est la meilleure car Bob trouve $m$ avec une proba dans les 2 case mais Eve a moins de chance de le trouver avec la Stratégie 2.
\end{solution}

\end{document}
